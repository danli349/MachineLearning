{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 – Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the logistic activation function (see Figure 11-1), you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 11-1. Logistic activation function saturation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization  \n",
    "\n",
    "In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable gradients problem. They point out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction. It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the *fan-in* and *fan-out* of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as described in Equation 11-1, where \n",
    "$\\text{fan}_{\\text{avg}} = (\\text{fan}_{\\text{in}} + \\text{fan}_{\\text{out}})/2$. This initialization strategy is called *Xavier initialization* or *Glorot initialization*, after the paper’s first author. \n",
    "\n",
    "*Method of Glorot initialization* \n",
    "\n",
    "- Step 1: Initialize weights from Gaussian or uniform distribution\n",
    "- Step 2: Scale the weights proportional to the number of inputs of the layer\n",
    "(For the first hidden layer, that is the number of features in the dataset; for the second hidden layer, that is the number of units in the 1st hidden layer, etc.)\n",
    "\n",
    "In particular, scale as follows:\n",
    "$$\\mathbf W^{(l)}:=\\mathbf W^{(l)}\\sqrt{\\frac{1}{m^{(l-1)}}}$$ where $m$ is the number of features of input units to the next layer and $l$ is the layer index.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb V(z_j^{(l)})&=\\mathbb V\\left(\\sum_{k=1}^{m^{l-1}}W_{jk}^{(l)}a_k^{(l-1)}\\right)\\\\\n",
    "&=\\sum_{k=1}^{m^{l-1}}\\mathbb V\\left[W_{jk}^{(l)}a_k^{(l-1)}\\right]\\\\\n",
    "&=\\sum_{k=1}^{m^{l-1}}\\mathbb V\\left[W_{jk}^{(l)}\\right]\\mathbb V \\left[a_k^{(l-1)}\\right]\\\\\n",
    "&=m^{(l-1)}\\mathbb V\\left[W^{(l)}\\right]\\mathbb V \\left[a^{(l-1)}\\right]\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "*Equation 11-1. Glorot initialization (when using the logistic activation function)*\n",
    "\n",
    "$$\\text{Normal distribution with mean 0 and variance } \\sigma^2=\\frac{1}{\\text{fan}_{\\text{avg}}}$$\n",
    "$$\\text{Or a uniform distribution between -r and +r, with } r=\\sqrt{\\frac{3}{\\text{fan}_{\\text{avg}}}}$$\n",
    "\n",
    "If you replace $\\text{fan}_{\\text{avg}}$ with $\\text{fan}_{\\text{in}}$ in Equation 11-1, you get an initialization strategy that Yann LeCun proposed in the 1990s. He called it *LeCun initialization*. Genevieve Orr and Klaus-Robert Müller even recommended it in their 1998 book *Neural Networks: Tricks of the Trade* (Springer). LeCun initialization is equivalent to Glorot initialization when $\\text{fan}_{\\text{in}}=\\text{fan}_{\\text{out}}$. It took over a decade for researchers to realize how important this trick is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning. \n",
    "\n",
    "\n",
    "\n",
    "*Table 11-1. Initialization parameters for each type of activation function*\n",
    "\n",
    "| Initialization |Activation functions | $\\sigma^2$ (Normal) |\n",
    "| ----------- |----------- |----------- |\n",
    "| Glorot |None, tanh, logistic, softmax | $1/\\text{fan}_{\\text{avg}}$ |\n",
    "| He |ReLU and variants | $2/\\text{fan}_{\\text{in}}$ |\n",
    "| LeCun |SELU | $1/\\text{fan}_{\\text{in}}$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, you can change this to He initialization by setting `kernel_initializer=\"he_uniform\"` or `kernel_initializer=\"he_normal\"` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f849e7bc940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want He initialization with a uniform distribution but based on $\\text{fan}_{\\text{avg}}$ rather than $\\text{fan}_{\\text{in}}$, you can use the `VarianceScaling` initializer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f83f505a4c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                          distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky ReLU, randomized leaky ReLU (RReLU), and parametric leaky ReLU (PReLU)\n",
    "\n",
    "One of the insights in the 2010 paper by Glorot and Bengio was that the problems with unstable gradients were in part due to a poor choice of activation function. Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural networks—in particular, the ReLU activation function, mostly because it does not saturate for positive values (and because it is fast to compute).\n",
    "\n",
    "Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.\n",
    "\n",
    "To solve this problem, you may want to use a variant of the ReLU function, such as the *leaky ReLU*. This function is defined as $LeakyReLU_{\\alpha}(z) = \\max(\\alpha z, z)$ (see Figure 11-2). The hyperparameter $\\alpha$ defines how much the function “leaks”: it is the slope of the function for z < 0 and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. A 2015 paper compared several variants of the ReLU activation function, and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting $\\alpha=0.2$ (a huge leak) seemed to result in better performance than $\\alpha=0.01$ (a small leak). The paper also evaluated the *randomized leaky ReLU (RReLU)*, where $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing. *RReLU* also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set). Finally, the paper evaluated the *parametric leaky ReLU (PReLU)*, where $\\alpha$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 11-2. Leaky ReLU: like ReLU, but with a small slope for negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on Fashion MNIST using the Leaky ReLU: \n",
    "\n",
    "To use the leaky ReLU activation function, create a `LeakyReLU` layer and add it to your model just after the layer you want to apply it to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 1.6627 - accuracy: 0.4992 - val_loss: 0.9047 - val_accuracy: 0.7158\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8542 - accuracy: 0.7231 - val_loss: 0.7203 - val_accuracy: 0.7636\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7114 - accuracy: 0.7625 - val_loss: 0.6475 - val_accuracy: 0.7886\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6361 - accuracy: 0.7900 - val_loss: 0.5924 - val_accuracy: 0.8072\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6011 - accuracy: 0.8008 - val_loss: 0.5597 - val_accuracy: 0.8196\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5634 - accuracy: 0.8143 - val_loss: 0.5361 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5386 - accuracy: 0.8212 - val_loss: 0.5166 - val_accuracy: 0.8308\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5159 - accuracy: 0.8287 - val_loss: 0.5094 - val_accuracy: 0.8286\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5103 - accuracy: 0.8274 - val_loss: 0.4901 - val_accuracy: 0.8398\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4920 - accuracy: 0.8337 - val_loss: 0.4819 - val_accuracy: 0.8394\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try PReLU:\n",
    "For PReLU, replace `LeakyRelu(alpha=0.2)` with `PReLU()`. There is currently no official implementation of RReLU in Keras, but you can fairly easily implement your own (to learn how to do that, see the exercises at the end of Chapter 12).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 1.6969 - accuracy: 0.4974 - val_loss: 0.9255 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8706 - accuracy: 0.7247 - val_loss: 0.7305 - val_accuracy: 0.7630\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7211 - accuracy: 0.7621 - val_loss: 0.6564 - val_accuracy: 0.7882\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6447 - accuracy: 0.7879 - val_loss: 0.6003 - val_accuracy: 0.8048\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6077 - accuracy: 0.8004 - val_loss: 0.5656 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5692 - accuracy: 0.8118 - val_loss: 0.5406 - val_accuracy: 0.8236\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5428 - accuracy: 0.8194 - val_loss: 0.5196 - val_accuracy: 0.8314\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5193 - accuracy: 0.8283 - val_loss: 0.5113 - val_accuracy: 0.8318\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5128 - accuracy: 0.8273 - val_loss: 0.4916 - val_accuracy: 0.8382\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4940 - accuracy: 0.8314 - val_loss: 0.4826 - val_accuracy: 0.8394\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exponential linear unit (ELU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, a 2015 paper by Djork-Arné Clevert et al. proposed a new activation function called the *exponential linear unit (ELU)* that outperformed all the ReLU variants in the authors’ experiments: training time was reduced, and the neural network performed better on the test set. Figure 11-3 graphs the function, and Equation 11-2 shows its definition.\n",
    "\n",
    "*Equation 11-2. ELU activation function*\n",
    "\n",
    "$$\\text{ELU}_{\\alpha}(z)=\\begin{cases}\n",
    "\\alpha(\\exp(z)-1) & \\text{if } z<0\\\\\n",
    "z & \\text{if } z\\ge 0\\\\\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure elu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 11-3. ELU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f83f4fc4610>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"elu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled ELU (SELU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will *self-normalize*: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ<sub>1</sub> or ℓ<sub>2</sub> regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions.  \n",
    "\n",
    "a few conditions for self-normalization to happen (see the paper for the mathematical justification):\n",
    "\n",
    "- The input features must be standardized (mean 0 and standard deviation 1).\n",
    "\n",
    "- Every hidden layer’s weights must be initialized with LeCun normal initialization. In Keras, this means setting `kernel_initializer=\"lecun_normal\"`.\n",
    "\n",
    "- The network’s architecture must be sequential. Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks (see Chapter 15) or networks with *skip connections* (i.e., connections that skip layers, such as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.\n",
    "\n",
    "- The paper only guarantees self-normalization if all layers are dense, but some researchers have noted that the SELU activation function can improve performance in convolutional neural nets as well (see Chapter 14). \n",
    "\n",
    "$$\\alpha = \\sqrt{2 / \\pi} / \\left(erfc(1/\\sqrt{2}) \\exp(1/2) - 1\\right)$$\n",
    "$$\\text{scale}=\\bigg[1 - erfc(1 / \\sqrt{2}) \\sqrt{e}\\bigg] \\sqrt{2 \\pi}\\bigg[2 erfc(\\sqrt{2})e^2 + \n",
    "                                                                               \\pi \\cdot erfc(1/\\sqrt{2})^2e - \n",
    "                                                                               2(2+\\pi)erfc(1/\\sqrt{2})\\sqrt{e}+\\pi+2\\bigg]^{-1/2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi)*(2 * erfc(np.sqrt(2))*np.e**2 + \n",
    "                                                                               np.pi*erfc(1/np.sqrt(2))**2*np.e - \n",
    "                                                                               2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure selu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization, scale is Standard deviation, then variance is 1/100 \n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SELU is easy: \n",
    "\n",
    "For SELU activation, set `activation=\"selu\"` and `kernel_initializer=\"lecun_normal\"` when creating a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f83e664a9d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 1.3846 - accuracy: 0.4696 - val_loss: 0.8607 - val_accuracy: 0.6844\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.8137 - accuracy: 0.6976 - val_loss: 0.6269 - val_accuracy: 0.7816\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.6570 - accuracy: 0.7609 - val_loss: 0.5947 - val_accuracy: 0.7866\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.5791 - accuracy: 0.7917 - val_loss: 0.5250 - val_accuracy: 0.8190\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.5197 - accuracy: 0.8174 - val_loss: 0.4826 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at what happens if we try to use the ReLU activation function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 2.0610 - accuracy: 0.1872 - val_loss: 1.6005 - val_accuracy: 0.3170\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 1.2244 - accuracy: 0.4732 - val_loss: 1.2055 - val_accuracy: 0.5082\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 1.0743 - accuracy: 0.5505 - val_loss: 0.8802 - val_accuracy: 0.6350\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.8585 - accuracy: 0.6582 - val_loss: 0.7745 - val_accuracy: 0.7008\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.7509 - accuracy: 0.6960 - val_loss: 0.8195 - val_accuracy: 0.6486\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great at all, we suffered from the vanishing/exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.\n",
    "\n",
    "In a 2015 paper, Sergey Ioffe and Christian Szegedy proposed a technique called *Batch Normalization (BN)* that addresses these problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a `StandardScaler`); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).\n",
    "\n",
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”). The whole operation is summarized step by step in Equation 11-3.\n",
    "\n",
    "*Equation 11-3. Batch Normalization algorithm*  \n",
    "\n",
    "- 1. $$\\boldsymbol\\mu_{B}=\\frac{1}{m_{B}}\\sum_{1}^{m_{B}}\\mathbf x^{(i)}$$\n",
    "- 2. $$\\boldsymbol\\sigma^2_{B}=\\frac{1}{m_{B}}\\sum_{1}^{m_{B}}\\left(\\mathbf x^{(i)}-\\boldsymbol\\mu_{B}\\right)^2$$\n",
    "- 3. $$\\hat{\\mathbf x}^{(i)}=\\frac{\\mathbf x^{(i)}-\\boldsymbol\\mu_{B}}{\\sqrt{\\boldsymbol\\sigma^2_{B}+\\epsilon}}$$\n",
    "- 4. $$\\mathbf z^{(i)}=\\boldsymbol\\gamma\\otimes \\hat{\\mathbf x}^{(i)}+\\boldsymbol\\beta$$\n",
    "\n",
    "In this algorithm:\n",
    "\n",
    "- $\\boldsymbol\\mu_{B}$ is the vector of input means, evaluated over the whole mini-batch $B$ (it contains one mean per input).\n",
    "- $\\boldsymbol\\sigma_{B}$ is the vector of input standard deviations, also evaluated over the whole mini-batch (it contains one standard deviation per input).\n",
    "\n",
    "- $m_{B}$ is the number of instances in the mini-batch.\n",
    "\n",
    "- $\\mathbf x^{(i)}$ is the vector of zero-centered and normalized inputs for instance $i$.\n",
    "\n",
    "- $\\boldsymbol\\gamma$ is the output scale parameter vector for the layer (it contains one scale parameter per input).\n",
    "\n",
    "- $\\otimes$ represents element-wise multiplication (each input is multiplied by its corresponding output scale parameter).\n",
    "\n",
    "- $\\boldsymbol\\beta$ is the output shift (offset) parameter vector for the layer (it contains one offset parameter per input). Each input is offset by its corresponding shift parameter.\n",
    "\n",
    "- $\\epsilon$ is a tiny number that avoids division by zero (typically $10^{-5}$). This is called a smoothing term.\n",
    "\n",
    "- $\\mathbf z^{(i)}$ is the output of the BN operation. It is a rescaled and shifted version of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So during training, BN standardizes its inputs, then rescales and offsets them. Good! What about at test time? Well, it’s not that simple. Indeed, we may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable. One solution could be to wait until the end of training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations could then be used instead of the batch input means and standard deviations when making predictions. However, most implementations of Batch Normalization estimate these final statistics during training by using a moving average of the layer’s input means and standard deviations. This is what Keras does automatically when you use the `BatchNormalization` layer. To sum up, four parameter vectors are learned in each batch-normalized layer: $\\boldsymbol\\gamma$ (the output scale vector) and $\\boldsymbol\\beta$ (the output offset vector) are learned through regular backpropagation, and $\\boldsymbol\\mu$ (the final input mean vector) and $\\boldsymbol\\sigma$ (the final input standard deviation vector) are estimated using an exponential moving average. Note that $\\boldsymbol\\mu$ and $\\boldsymbol\\sigma$ are estimated during training, but they are used only after training (to replace the batch input means and standard deviations in Equation 11-3).  \n",
    "\n",
    "Batch Normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data, as we discussed earlier). Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it’s often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty. This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. For example, if the previous layer computes $\\mathbf X\\mathbf W + \\mathbf b$, then the BN layer will compute $\\boldsymbol\\gamma\\otimes(\\mathbf X\\mathbf W + \\mathbf b - \\boldsymbol\\mu)/\\boldsymbol\\sigma + \\boldsymbol\\beta$ (ignoring the smoothing term $\\epsilon$ in the denominator). If we define $\\mathbf W' = \\boldsymbol\\gamma\\otimes\\mathbf W/\\boldsymbol\\sigma$ and $\\mathbf b' = \\boldsymbol\\gamma\\otimes(\\mathbf b - \\boldsymbol\\mu)/\\boldsymbol\\sigma + \\boldsymbol\\beta$, the equation simplifies to $\\mathbf X\\mathbf W' + \\mathbf b'$. So if we replace the previous layer’s weights and biases ($\\mathbf W$ and $\\mathbf b$) with the updated weights and biases ($\\mathbf W'$ and $\\mathbf b'$), we can get rid of the BN layer (TFLite’s optimizer does this automatically; see Chapter 19).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement batch normalization with keras\n",
    "\n",
    "As with most things with Keras, implementing Batch Normalization is simple and intuitive. Just add a `BatchNormalization` layer before or after each hidden layer’s activation function, and optionally add a BN layer as well as the first layer in your model. For example, this model applies BN after every hidden layer and as the first layer in the model (after flattening the input images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 3136)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28, 28*28*4, # each input has 𝝁 , 𝝈 ,   𝜸 and 𝜷 4 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235500, 1200, 30100, 400, 1010)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(784+1)*300, 300*4, (300+1)*100, 100*4, (100+1)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each BN layer adds four parameters per input: $\\boldsymbol\\gamma$, $\\boldsymbol\\beta$, $\\boldsymbol\\mu$, and $\\boldsymbol\\sigma$ (for example, the first BN layer adds 3,136 parameters, which is 4 × 784). The last two parameters, $\\boldsymbol\\mu$ and $\\boldsymbol\\sigma$, are the moving averages; they are not affected by backpropagation, so Keras calls them “non-trainable” (if you count the total number of BN parameters, 3,136 + 1,200 + 400, and divide by 2, you get 2,368, which is the total number of non-trainable parameters in this model).\n",
    "\n",
    "Let’s look at the parameters of the first BN layer. Two are trainable (by backpropagation), and two are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you create a BN layer in Keras, it also creates two operations that will be called by Keras at each iteration during training. These operations will update the moving averages. Since we are using the TensorFlow backend, these operations are TensorFlow operations (we will discuss TF operations in Chapter 12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bn1.updates #deprecated\n",
    "#model.layers[1].updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 2ms/step - loss: 1.2287 - accuracy: 0.5993 - val_loss: 0.5526 - val_accuracy: 0.8230\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5996 - accuracy: 0.7960 - val_loss: 0.4725 - val_accuracy: 0.8470\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5312 - accuracy: 0.8172 - val_loss: 0.4375 - val_accuracy: 0.8556\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4885 - accuracy: 0.8295 - val_loss: 0.4151 - val_accuracy: 0.8602\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4717 - accuracy: 0.8345 - val_loss: 0.3997 - val_accuracy: 0.8632\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4420 - accuracy: 0.8462 - val_loss: 0.3867 - val_accuracy: 0.8696\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4286 - accuracy: 0.8497 - val_loss: 0.3763 - val_accuracy: 0.8706\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4087 - accuracy: 0.8553 - val_loss: 0.3711 - val_accuracy: 0.8742\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4080 - accuracy: 0.8566 - val_loss: 0.3630 - val_accuracy: 0.8750\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3903 - accuracy: 0.8616 - val_loss: 0.3572 - val_accuracy: 0.8758\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes applying BN before the activation function works better (there's a debate on this topic). Moreover, the layer before a `BatchNormalization` layer does not need to have bias terms, since the `BatchNormalization` layer some as well, it would be a waste of parameters, so you can set `use_bias=False` when creating those layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.3677 - accuracy: 0.5604 - val_loss: 0.6767 - val_accuracy: 0.7812\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7136 - accuracy: 0.7702 - val_loss: 0.5566 - val_accuracy: 0.8180\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.6123 - accuracy: 0.7991 - val_loss: 0.5007 - val_accuracy: 0.8360\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5547 - accuracy: 0.8149 - val_loss: 0.4666 - val_accuracy: 0.8448\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5255 - accuracy: 0.8229 - val_loss: 0.4434 - val_accuracy: 0.8536\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4947 - accuracy: 0.8327 - val_loss: 0.4263 - val_accuracy: 0.8548\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4736 - accuracy: 0.8387 - val_loss: 0.4131 - val_accuracy: 0.8570\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4550 - accuracy: 0.8443 - val_loss: 0.4035 - val_accuracy: 0.8612\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4495 - accuracy: 0.8438 - val_loss: 0.3943 - val_accuracy: 0.8638\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4333 - accuracy: 0.8494 - val_loss: 0.3875 - val_accuracy: 0.8656\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BatchNormalization` class has quite a few hyperparameters you can tweak. The defaults will usually be fine, but you may occasionally need to tweak the `momentum`. This hyperparameter is used by the `BatchNormalization` layer when it updates the exponential moving averages; given a new value $\\mathbf v$(i.e., a new vector of input means or standard deviations computed over the current batch), the layer updates the running average $\\hat{\\mathbf v}$ using the following equation:\n",
    "\n",
    "$$\\hat{\\mathbf v}\\leftarrow\\hat{\\mathbf v}\\times\\text{momentum}+\\mathbf v\\times(1-\\text{momentum})$$\n",
    "A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.999 (you want more 9s for larger datasets and smaller mini-batches).  \n",
    "\n",
    "Another important hyperparameter is axis: it determines which axis should be normalized. It defaults to $–1$, meaning that by default it will normalize the last axis (using the means and standard deviations computed across the other axes). When the input batch is 2D (i.e., the batch shape is [batch size, features]), this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch. For example, the first BN layer in the previous code example will independently normalize (and rescale and shift) each of the 784 input features. If we move the first BN layer before the Flatten layer, then the input batches will be 3D, with shape [batch size, height, width]; therefore, the BN layer will compute 28 means and 28 standard deviations (1 per column of pixels, computed across all instances in the batch and across all rows in the column), and it will normalize all pixels in a given column using the same mean and standard deviation. There will also be just 28 scale parameters and 28 shift parameters. If instead you still want to treat each of the 784 pixels independently, then you should set `axis=[1, 2]`.\n",
    "\n",
    "Notice that the BN layer does not perform the same computation during training and after training: it uses batch statistics during training and the “final” statistics after training (i.e., the final values of the moving averages). Let’s take a peek at the source code of this class to see how this is handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(keras.layers.Layer):\n",
    "    [...]\n",
    "    def call(self, inputs, training=None):\n",
    "        [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `call()` method is the one that performs the computations; as you can see, it has an extra training argument, which is set to None by default, but the `fit()` method sets to it to 1 during training. If you ever need to write a custom layer, and it must behave differently during training and testing, add a training argument to the `call()` method and use this argument in the method to decide what to compute (we will discuss custom layers in Chapter 12).\n",
    "\n",
    "`BatchNormalization` has become one of the most-used layers in deep neural networks, to the point that it is often omitted in the diagrams, as it is assumed that BN is added after every layer. But a recent paper by Hongyi Zhang et al. may change this assumption: by using a novel fixed-update (fixup) weight initialization technique, the authors managed to train a very deep neural network (10,000 layers!) without BN, achieving state-of-the-art performance on complex image classification tasks. As this is bleeding-edge research, however, you may want to wait for additional research to confirm this finding before you drop Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping  \n",
    "\n",
    "Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This is called Gradient Clipping. This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs, as we will see in Chapter 15. For other types of networks, BN is usually sufficient.\n",
    "\n",
    "In Keras, implementing Gradient Clipping is just a matter of setting the `clipvalue` or `clipnorm` argument when creating an optimizer, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimizer will clip every component of the gradient vector to a value between –1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyperparameter you can tune. Note that it may change the orientation of the gradient vector. For instance, if the original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. In practice, this approach works well. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue. This will clip the whole gradient if its ℓ2 norm is greater than the threshold you picked. For example, if you set clipnorm=1.0, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component. If you observe that the gradients explode during training (you can track the size of the gradients using TensorBoard), you may want to try both clipping by value and clipping by norm, with different thresholds, and see which option performs best on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the fashion MNIST training set in two:\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3, 4, 5,\n",
       "       1, 3, 4, 2, 0, 6, 7, 1], dtype=uint8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_A[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_B[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.9249 - accuracy: 0.6994 - val_loss: 0.3896 - val_accuracy: 0.8662\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3651 - accuracy: 0.8745 - val_loss: 0.3288 - val_accuracy: 0.8827\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3182 - accuracy: 0.8897 - val_loss: 0.3013 - val_accuracy: 0.8991\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3048 - accuracy: 0.8954 - val_loss: 0.2896 - val_accuracy: 0.9021\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2804 - accuracy: 0.9029 - val_loss: 0.2773 - val_accuracy: 0.9061\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2701 - accuracy: 0.9075 - val_loss: 0.2735 - val_accuracy: 0.9066\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2627 - accuracy: 0.9093 - val_loss: 0.2721 - val_accuracy: 0.9081\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2609 - accuracy: 0.9122 - val_loss: 0.2589 - val_accuracy: 0.9141\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2558 - accuracy: 0.9110 - val_loss: 0.2562 - val_accuracy: 0.9136\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2512 - accuracy: 0.9138 - val_loss: 0.2544 - val_accuracy: 0.9160\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2431 - accuracy: 0.9170 - val_loss: 0.2495 - val_accuracy: 0.9153\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2422 - accuracy: 0.9168 - val_loss: 0.2515 - val_accuracy: 0.9126\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2360 - accuracy: 0.9181 - val_loss: 0.2446 - val_accuracy: 0.9160\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2266 - accuracy: 0.9232 - val_loss: 0.2415 - val_accuracy: 0.9178\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2225 - accuracy: 0.9239 - val_loss: 0.2449 - val_accuracy: 0.9195\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2261 - accuracy: 0.9216 - val_loss: 0.2383 - val_accuracy: 0.9195\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2191 - accuracy: 0.9253 - val_loss: 0.2413 - val_accuracy: 0.9178\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2171 - accuracy: 0.9255 - val_loss: 0.2430 - val_accuracy: 0.9160\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2180 - accuracy: 0.9246 - val_loss: 0.2330 - val_accuracy: 0.9208\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2112 - accuracy: 0.9273 - val_loss: 0.2332 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.0360 - accuracy: 0.4975 - val_loss: 0.6314 - val_accuracy: 0.6004\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.5883 - accuracy: 0.6971 - val_loss: 0.4784 - val_accuracy: 0.8529\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.4380 - accuracy: 0.8854 - val_loss: 0.4102 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.4021 - accuracy: 0.8712 - val_loss: 0.3647 - val_accuracy: 0.9178\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3361 - accuracy: 0.9348 - val_loss: 0.3300 - val_accuracy: 0.9320\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3113 - accuracy: 0.9233 - val_loss: 0.3019 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2817 - accuracy: 0.9299 - val_loss: 0.2804 - val_accuracy: 0.9422\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2632 - accuracy: 0.9379 - val_loss: 0.2606 - val_accuracy: 0.9473\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2373 - accuracy: 0.9481 - val_loss: 0.2428 - val_accuracy: 0.9523\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2229 - accuracy: 0.9657 - val_loss: 0.2281 - val_accuracy: 0.9544\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2155 - accuracy: 0.9590 - val_loss: 0.2150 - val_accuracy: 0.9584\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1834 - accuracy: 0.9738 - val_loss: 0.2036 - val_accuracy: 0.9584\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1671 - accuracy: 0.9828 - val_loss: 0.1931 - val_accuracy: 0.9615\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1527 - accuracy: 0.9915 - val_loss: 0.1838 - val_accuracy: 0.9635\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1595 - accuracy: 0.9904 - val_loss: 0.1746 - val_accuracy: 0.9686\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1473 - accuracy: 0.9937 - val_loss: 0.1674 - val_accuracy: 0.9686\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1412 - accuracy: 0.9944 - val_loss: 0.1604 - val_accuracy: 0.9706\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1242 - accuracy: 0.9931 - val_loss: 0.1539 - val_accuracy: 0.9706\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1224 - accuracy: 0.9931 - val_loss: 0.1482 - val_accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1096 - accuracy: 0.9912 - val_loss: 0.1431 - val_accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at an example. Suppose the Fashion MNIST dataset only contained eight classes—for example, all the classes except for sandal and shirt. Someone built and trained a Keras model on that set and got reasonably good performance (>90% accuracy). Let’s call this model A. You now want to tackle a different task: you have images of sandals and shirts, and you want to train a binary classifier (positive=shirt, negative=sandal). Your dataset is quite small; you only have 200 labeled images. When you train a new model for this task (let’s call it model B) with the same architecture as model A, it performs reasonably well (97.2% accuracy). But since it’s a much easier task (there are just two classes), you were hoping for more. While drinking your morning coffee, you realize that your task is quite similar to task A, so perhaps transfer learning can help? Let’s find out!\n",
    "\n",
    "First, you need to load model A and create a new model based on that model’s layers. Let’s reuse all the layers except for the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `model_A` and `model_B_on_A` now share some layers. When you train `model_B_on_A`, it will also affect `model_A`. If you want to avoid that, you need to clone `model_A` before you reuse its layers. To do this, you clone model A’s architecture with `clone_model()`, then copy its weights (since `clone_model()` does not clone the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you could train `model_B_on_A` for task B, but since the new output layer was initialized randomly it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, set every layer’s trainable attribute to False and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.6125 - accuracy: 0.6233 - val_loss: 0.5824 - val_accuracy: 0.6359\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.5525 - accuracy: 0.6638 - val_loss: 0.5451 - val_accuracy: 0.6836\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.4875 - accuracy: 0.7482 - val_loss: 0.5131 - val_accuracy: 0.7099\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.4878 - accuracy: 0.7355 - val_loss: 0.4845 - val_accuracy: 0.7343\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4363 - accuracy: 0.7774 - val_loss: 0.3457 - val_accuracy: 0.8651\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2966 - accuracy: 0.9143 - val_loss: 0.2602 - val_accuracy: 0.9270\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2032 - accuracy: 0.9777 - val_loss: 0.2111 - val_accuracy: 0.9554\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1754 - accuracy: 0.9719 - val_loss: 0.1792 - val_accuracy: 0.9696\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1349 - accuracy: 0.9809 - val_loss: 0.1563 - val_accuracy: 0.9757\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1173 - accuracy: 0.9973 - val_loss: 0.1394 - val_accuracy: 0.9797\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1138 - accuracy: 0.9931 - val_loss: 0.1268 - val_accuracy: 0.9838\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1001 - accuracy: 0.9931 - val_loss: 0.1165 - val_accuracy: 0.9858\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0835 - accuracy: 1.0000 - val_loss: 0.1067 - val_accuracy: 0.9888\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0776 - accuracy: 1.0000 - val_loss: 0.1001 - val_accuracy: 0.9899\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0690 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 0.9899\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0720 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9899\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 0.9899\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9899\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9899\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9899\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what's the final verdict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1408407837152481, 0.9704999923706055]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 996us/step - loss: 0.0683 - accuracy: 0.9935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06827671080827713, 0.9934999942779541]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We got quite a bit of transfer: the error rate dropped by a factor of 4.5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.538461538461503"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 - 97.05) / (100 - 99.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Faster Optimizers  \n",
    "\n",
    "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution): applying a good initialization strategy for the connection weights, using a good activation function, using Batch Normalization, and reusing parts of a pretrained network (possibly built on an auxiliary task or using unsupervised learning). Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer. In this section we will present the most popular algorithms: momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum optimization  \n",
    "\n",
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind momentum optimization, proposed by Boris Polyak in 1964. In contrast, regular Gradient Descent will simply take small, regular steps down the slope, so the algorithm will take much more time to reach the bottom.\n",
    "\n",
    "Recall that Gradient Descent updates the weights $\\boldsymbol\\theta$ by directly subtracting the gradient of the cost function $J(\\boldsymbol\\theta)$ with regard to the weights $(\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta))$ multiplied by the learning rate $\\eta$. The equation is: $\\boldsymbol\\theta \\leftarrow \\boldsymbol\\theta - \\eta\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)$. It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
    "\n",
    "Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient (multiplied by the learning rate $\\eta$) from the *momentum vector* $\\mathbf m$, and it updates the weights by adding this momentum vector (see Equation 11-4). In other words, the gradient is used for acceleration, not for speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter $\\beta$, called the **momentum**, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9. \n",
    "\n",
    "*Equation 11-4. Momentum algorithm*  \n",
    "\n",
    "$$\\begin{align}\n",
    "&1.\\quad \\mathbf m\\leftarrow\\beta \\mathbf m-\\eta\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)\\\\\n",
    "&2.\\quad \\boldsymbol\\theta\\leftarrow\\boldsymbol\\theta+\\mathbf m\n",
    "\\end{align}$$ \n",
    "\n",
    "You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate $\\eta$ multiplied by $1/(1-\\beta)$ (ignoring the sign). For example, if $\\beta = 0.9$, then the terminal velocity is equal to 10 times the gradient times the learning rate, so momentum optimization ends up going 10 times faster than Gradient Descent! This allows momentum optimization to escape from plateaus much faster than Gradient Descent. We saw in Chapter 4 that when the inputs have very different scales, the cost function will look like an elongated bowl (see Figure 4-7). Gradient Descent goes down the steep slope quite fast, but then it takes a very long time to go down the valley. In contrast, momentum optimization will roll down the valley faster and faster until it reaches the bottom (the optimum). In deep neural networks that don’t use Batch Normalization, the upper layers will often end up having inputs with very different scales, so using momentum optimization helps a lot. It can also help roll past local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing momentum optimization in Keras is a no-brainer: just use the `SGD` optimizer and set its momentum hyperparameter, then lie back and profit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient   \n",
    "\n",
    "One small variant to momentum optimization, proposed by Yurii Nesterov in 1983, is almost always faster than vanilla momentum optimization. The *Nesterov Accelerated Gradient* (NAG) method, also known as *Nesterov momentum optimization*, measures the gradient of the cost function not at the local position $\\boldsymbol\\theta$ but slightly ahead in the direction of the momentum, at $\\boldsymbol\\theta + \\beta\\mathbf m$ (see Equation 11-5).\n",
    "\n",
    "*Equation 11-5. Nesterov Accelerated Gradient algorithm*  \n",
    "\n",
    "$$\\begin{align}\n",
    "&1.\\quad \\mathbf m\\leftarrow\\beta \\mathbf m-\\eta\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta+\\beta \\mathbf m)\\\\\n",
    "&2.\\quad \\boldsymbol\\theta\\leftarrow\\boldsymbol\\theta+\\mathbf m\n",
    "\\end{align}$$ \n",
    "\n",
    "\n",
    "This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position, as you can see in Figure 11-6 (where $\\nabla_1$ represents the gradient of the cost function measured at the starting point $\\boldsymbol\\theta$, and $\\nabla_2$ represents the gradient at the point located at $\\boldsymbol\\theta+\\beta \\mathbf m$).\n",
    "\n",
    "As you can see, the Nesterov update ends up slightly closer to the optimum. After a while, these small improvements add up and NAG ends up being significantly faster than regular momentum optimization. Moreover, note that when the momentum pushes the weights across a valley, $\\nabla_1$ continues to push farther across the valley, while $\\nabla_2$ pushes back toward the bottom of the valley. This helps reduce oscillations and thus NAG converges faster.\n",
    "\n",
    "NAG is generally faster than regular momentum optimization. To use it, simply set `nesterov=True` when creating the `SGD` optimizer:\n",
    "\n",
    "<img src=\"./chapters/11/11.6.png\" width = 600>\n",
    "<div style=\"text-align:center\"> Figure 11-6. Regular versus Nesterov momentum optimization: the former applies the gradients computed before the momentum step, while the latter applies the gradients computed after </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad  \n",
    "\n",
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The AdaGrad algorithm achieves this correction by scaling down the gradient vector along the steepest dimensions (see Equation 11-6).\n",
    "\n",
    "*Equation 11-6. AdaGrad algorithm* \n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "&1.\\quad \\mathbf s\\leftarrow\\mathbf s+\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)\\otimes\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)\\\\\n",
    "&2.\\quad \\boldsymbol\\theta\\leftarrow\\boldsymbol\\theta-\\eta\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)⊘\\sqrt{\\mathbf s+\\epsilon}\n",
    "\\end{align}$$ \n",
    "\n",
    "The first step accumulates the square of the gradients into the vector s (recall that the $\\otimes$ symbol represents the element-wise multiplication). This vectorized form is equivalent to computing $s_i \\leftarrow s_i + (\\partial J(\\boldsymbol\\theta) / \\partial \\theta_i)^2$ for each element $s_i$ of the vector $\\mathbf s$; in other words, each $s_i$ accumulates the squares of the partial derivative of the cost function with regard to parameter $\\theta_i$. If the cost function is steep along the $i^{th}$ dimension, then $s_i$ will get larger and larger at each iteration.\n",
    "\n",
    "The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector is scaled down by a factor of $\\sqrt{\\mathbf s+\\epsilon}$ (the ⊘ symbol represents the element-wise division, and $\\epsilon$ is a smoothing term to avoid division by zero, typically set to $10^{-10}$). This vectorized form is equivalent to simultaneously computing $$\\theta_i\\leftarrow \\theta_i-\\eta \\frac{\\partial J(\\boldsymbol\\theta)/\\partial \\theta_i}{\\sqrt{s_i+\\epsilon}}$$ for all parameters $\\theta_i$.\n",
    "\n",
    "In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an *adaptive learning rate*. It helps point the resulting updates more directly toward the global optimum (see Figure 11-7). One additional benefit is that it requires much less tuning of the learning rate hyperparameter $\\eta$.  \n",
    "\n",
    "<img src=\"./chapters/11/11.7.png\" width = 600>\n",
    "<div style=\"text-align:left\"> Figure 11-7. AdaGrad versus Gradient Descent: the former can correct its direction earlier to point to the optimum </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an `Adagrad` optimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression, though). Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp  \n",
    "\n",
    "As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never converging to the global optimum. The *RMSProp* algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step (see Equation 11-7).\n",
    "\n",
    "*Equation 11-7. RMSProp algorithm* \n",
    "\n",
    "$$\\begin{align}\n",
    "&1.\\quad \\mathbf s\\leftarrow\\beta\\mathbf s+(1-\\beta)\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)\\otimes\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)\\\\\n",
    "&2.\\quad \\boldsymbol\\theta\\leftarrow\\boldsymbol\\theta-\\eta\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)⊘\\sqrt{\\mathbf s+\\epsilon}\n",
    "\\end{align}$$ \n",
    "\n",
    "\n",
    "The decay rate $\\beta$ is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so you may not need to tune it at all.\n",
    "\n",
    "As you might expect, Keras has an `RMSprop` optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `rho` argument corresponds to $\\beta$ in Equation 11-7. Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers until Adam optimization came around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimization  \n",
    "\n",
    "Adam, which stands for *adaptive moment estimation*, combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients (see Equation 11-8).\n",
    "\n",
    "*Equation 11-8. Adam algorithm*  \n",
    "\n",
    "$$\\begin{align}\n",
    "&1.\\quad \\mathbf m\\leftarrow\\beta_1\\mathbf m+(1-\\beta_1)\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)\\\\\n",
    "&2.\\quad \\mathbf s\\leftarrow\\beta_2\\mathbf s+(1-\\beta_2)\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)\\otimes\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)\\\\\n",
    "&3.\\quad \\widehat{\\mathbf m}\\leftarrow \\frac{\\mathbf m}{1-\\beta_1^t}\\\\\n",
    "&4. \\quad \\widehat{\\mathbf s}\\leftarrow \\frac{\\mathbf s}{1-\\beta_2^t}\\\\\n",
    "&5.\\quad \\boldsymbol\\theta\\leftarrow\\boldsymbol\\theta+\\eta\\widehat{\\mathbf m}⊘\\sqrt{\\hat{\\mathbf s}+\\epsilon}\\\\\n",
    "\\end{align}$$ \n",
    "\n",
    "In this equation, t represents the iteration number (starting at 1).\n",
    "\n",
    "If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity to both momentum optimization and RMSProp. The only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but these are actually equivalent except for a constant factor (the decaying average is just 1 – β1 times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since $\\mathbf m$ and $\\mathbf s$ are initialized at 0, they will be biased toward 0 at the beginning of training, so these two steps will help boost $\\mathbf m$ and $\\mathbf s$ at the beginning of training.\n",
    "\n",
    "The momentum decay hyperparameter $\\beta_1$ is typically initialized to 0.9, while the scaling decay hyperparameter $\\beta_2$ is often initialized to 0.999. As earlier, the smoothing term $\\epsilon$ is usually initialized to a tiny number such as $10^{-7}$. These are the default values for the Adam class (to be precise, epsilon defaults to None, which tells Keras to use `keras.backend.epsilon()`, which defaults to $10^{-7}$; you can change it using `keras.backend.set_epsilon()`). Here is how to create an Adam optimizer using Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter $\\eta$. You can often use the default value $\\eta = 0.001$, making Adam even easier to use than Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adamax Optimization  \n",
    "\n",
    "Notice that in step 2 of Equation 11-8. $$2.\\quad \\mathbf s\\leftarrow\\beta_2\\mathbf s+(1-\\beta_2)\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)\\otimes\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta)$$ Adam accumulates the squares of the gradients in $\\mathbf s$ (with a greater weight for more recent gradients). In step 5, if we ignore $\\epsilon$ and steps 3 and 4 (which are technical details anyway), Adam scales down the parameter updates by the square root of $\\mathbf s$. In short, Adam scales down the parameter updates by the $\\ell_2$ norm of the time-decayed gradients (recall that the $\\ell_2$ norm is the square root of the sum of squares). AdaMax, introduced in the same paper as Adam, replaces the $\\ell_2$ norm with the $\\ell_{\\infty}$ norm (a fancy way of saying the max). Specifically, it replaces step 2 in Equation 11-8 with $$\\mathbf s \\leftarrow \\max (\\beta_2\\mathbf s,\\nabla_{\\boldsymbol\\theta} J(\\boldsymbol\\theta))$$, it drops step 4, and in step 5 it scales down the gradient updates by a factor of $\\mathbf s$, which is just the max of the time-decayed gradients. In practice, this can make AdaMax more stable than Adam, but it really depends on the dataset, and in general Adam performs better. So, this is just one more optimizer you can try if you experience problems with Adam on some task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nadam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling  \n",
    "\n",
    "Finding a good learning rate is very important. If you set it much too high, training may diverge (as we discussed in “Gradient Descent”). If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never really settling down. If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8).  \n",
    "\n",
    "\n",
    "<img src=\"./chapters/11/11.8.png\" width = 600>\n",
    "<div style=\"text-align:left\"> Figure 11-8. Learning curves for various learning rates $\\eta$ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Scheduling  \n",
    "\n",
    "Set the learning rate to a function of the iteration number t: $\\eta(t) = \\eta_0 / (1 + t/s)^c$. The initial learning rate $\\eta_0$, the power c (typically set to 1), and the steps s are hyperparameters. The learning rate drops at each step. After s steps, it is down to $\\eta_0 / 2$. After s more steps, it is down to $\\eta_0 / 3$, then it goes down to $\\eta_0 / 4$, then $\\eta_0/ 5$, and so on. As you can see, this schedule first drops quickly, then more and more slowly. Of course, power scheduling requires tuning $\\eta_0$ and s (and possibly c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lr = lr0 / (1 + steps / s)**c```\n",
    "* Keras uses `c=1` and `s = 1 / decay`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing power scheduling in Keras is the easiest option: just set the `decay` hyperparameter when creating an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decay is the inverse of s (the number of steps it takes to divide the learning rate by one more unit), and Keras assumes that c is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5980 - accuracy: 0.7933 - val_loss: 0.4031 - val_accuracy: 0.8598\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3829 - accuracy: 0.8636 - val_loss: 0.3714 - val_accuracy: 0.8720\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3491 - accuracy: 0.8772 - val_loss: 0.3749 - val_accuracy: 0.8738\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3276 - accuracy: 0.8814 - val_loss: 0.3501 - val_accuracy: 0.8800\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3172 - accuracy: 0.8856 - val_loss: 0.3452 - val_accuracy: 0.8776\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2922 - accuracy: 0.8941 - val_loss: 0.3419 - val_accuracy: 0.8828\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2870 - accuracy: 0.8973 - val_loss: 0.3362 - val_accuracy: 0.8872\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2720 - accuracy: 0.9033 - val_loss: 0.3415 - val_accuracy: 0.8840\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2729 - accuracy: 0.9002 - val_loss: 0.3299 - val_accuracy: 0.8862\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2585 - accuracy: 0.9070 - val_loss: 0.3270 - val_accuracy: 0.8888\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2528 - accuracy: 0.9099 - val_loss: 0.3281 - val_accuracy: 0.8878\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2484 - accuracy: 0.9103 - val_loss: 0.3342 - val_accuracy: 0.8824\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2420 - accuracy: 0.9144 - val_loss: 0.3268 - val_accuracy: 0.8892\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2372 - accuracy: 0.9146 - val_loss: 0.3300 - val_accuracy: 0.8890\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2364 - accuracy: 0.9155 - val_loss: 0.3256 - val_accuracy: 0.8874\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2309 - accuracy: 0.9180 - val_loss: 0.3217 - val_accuracy: 0.8914\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2235 - accuracy: 0.9211 - val_loss: 0.3249 - val_accuracy: 0.8916\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2247 - accuracy: 0.9194 - val_loss: 0.3203 - val_accuracy: 0.8926\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2234 - accuracy: 0.9220 - val_loss: 0.3244 - val_accuracy: 0.8910\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2226 - accuracy: 0.9222 - val_loss: 0.3224 - val_accuracy: 0.8904\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2193 - accuracy: 0.9232 - val_loss: 0.3221 - val_accuracy: 0.8910\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2163 - accuracy: 0.9226 - val_loss: 0.3195 - val_accuracy: 0.8956\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2127 - accuracy: 0.9250 - val_loss: 0.3208 - val_accuracy: 0.8910\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2076 - accuracy: 0.9276 - val_loss: 0.3226 - val_accuracy: 0.8904\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2104 - accuracy: 0.9251 - val_loss: 0.3226 - val_accuracy: 0.8924\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay = 1e-4\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = math.ceil(len(X_train) / batch_size)\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Scheduling  \n",
    "\n",
    "Set the learning rate to $\\eta(t) = \\eta_0 0.1^{t/s}$. The learning rate will gradually drop by a factor of 10 every s steps. While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every s steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lr = lr0 * 0.1**(epoch / s)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential scheduling and piecewise scheduling are quite simple too. You first need to define a function that takes the current epoch and returns the learning rate. For example, let’s implement exponential scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not want to hardcode $\\eta_0$ and s, you can create a function that returns a configured function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a `LearningRateScheduler` callback, giving it the schedule function, and pass this callback to the `fit()` method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LearningRateScheduler` will update the optimizer’s `learning_rate` attribute at the beginning of each epoch. Updating the learning rate once per epoch is usually enough, but if you want it to be updated more often, for example at every step, you can always write your own callback (see the “Exponential Scheduling” section of the notebook for an example). Updating the learning rate at every step makes sense if there are many steps per epoch. Alternatively, you can use the `keras.optimizers.schedules` approach, described shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.1262 - accuracy: 0.7299 - val_loss: 0.7363 - val_accuracy: 0.7816\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6396 - accuracy: 0.7983 - val_loss: 0.6717 - val_accuracy: 0.7776\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5940 - accuracy: 0.8152 - val_loss: 0.6683 - val_accuracy: 0.7756\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5585 - accuracy: 0.8264 - val_loss: 0.6302 - val_accuracy: 0.8280\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5902 - accuracy: 0.8251 - val_loss: 0.5702 - val_accuracy: 0.8428\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4976 - accuracy: 0.8499 - val_loss: 0.5998 - val_accuracy: 0.8502\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4467 - accuracy: 0.8681 - val_loss: 0.5539 - val_accuracy: 0.8520\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4277 - accuracy: 0.8670 - val_loss: 0.6503 - val_accuracy: 0.8336\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4053 - accuracy: 0.8744 - val_loss: 0.5171 - val_accuracy: 0.8536\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3747 - accuracy: 0.8844 - val_loss: 0.4883 - val_accuracy: 0.8690\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3414 - accuracy: 0.8931 - val_loss: 0.4463 - val_accuracy: 0.8706\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3233 - accuracy: 0.8982 - val_loss: 0.4855 - val_accuracy: 0.8662\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3064 - accuracy: 0.9027 - val_loss: 0.4499 - val_accuracy: 0.8732\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2791 - accuracy: 0.9101 - val_loss: 0.4331 - val_accuracy: 0.8740\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2739 - accuracy: 0.9133 - val_loss: 0.4641 - val_accuracy: 0.8748\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2553 - accuracy: 0.9179 - val_loss: 0.4577 - val_accuracy: 0.8784\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2293 - accuracy: 0.9257 - val_loss: 0.5127 - val_accuracy: 0.8776\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2231 - accuracy: 0.9301 - val_loss: 0.4881 - val_accuracy: 0.8762\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2115 - accuracy: 0.9329 - val_loss: 0.5101 - val_accuracy: 0.8852\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2080 - accuracy: 0.9339 - val_loss: 0.5060 - val_accuracy: 0.8786\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1922 - accuracy: 0.9373 - val_loss: 0.5161 - val_accuracy: 0.8802\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1847 - accuracy: 0.9415 - val_loss: 0.5377 - val_accuracy: 0.8836\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1793 - accuracy: 0.9444 - val_loss: 0.5305 - val_accuracy: 0.8800\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1667 - accuracy: 0.9477 - val_loss: 0.5607 - val_accuracy: 0.8874\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1618 - accuracy: 0.9491 - val_loss: 0.5672 - val_accuracy: 0.8844\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schedule function can optionally take the current learning rate as a second argument. For example, the following schedule function multiplies the previous learning rate by $0.1^{1/20}$, which results in the same exponential decay (except the decay now starts at the beginning of epoch 0 instead of 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8912509381337456"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1**(1 / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to update the learning rate at each iteration rather than at each epoch, you must write your own callback class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 1.1031 - accuracy: 0.7406 - val_loss: 0.7370 - val_accuracy: 0.7628\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6589 - accuracy: 0.7992 - val_loss: 0.5055 - val_accuracy: 0.8266\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5647 - accuracy: 0.8220 - val_loss: 0.6345 - val_accuracy: 0.8070\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5423 - accuracy: 0.8338 - val_loss: 0.5475 - val_accuracy: 0.8280\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4814 - accuracy: 0.8446 - val_loss: 0.4437 - val_accuracy: 0.8726\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4316 - accuracy: 0.8649 - val_loss: 0.4204 - val_accuracy: 0.8652\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3835 - accuracy: 0.8740 - val_loss: 0.4548 - val_accuracy: 0.8652\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3618 - accuracy: 0.8817 - val_loss: 0.4707 - val_accuracy: 0.8486\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3444 - accuracy: 0.8853 - val_loss: 0.4108 - val_accuracy: 0.8712\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3152 - accuracy: 0.8967 - val_loss: 0.3998 - val_accuracy: 0.8812\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2820 - accuracy: 0.9061 - val_loss: 0.4245 - val_accuracy: 0.8746\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2690 - accuracy: 0.9090 - val_loss: 0.4222 - val_accuracy: 0.8798\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2435 - accuracy: 0.9179 - val_loss: 0.4877 - val_accuracy: 0.8768\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2287 - accuracy: 0.9217 - val_loss: 0.4183 - val_accuracy: 0.8774\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2131 - accuracy: 0.9287 - val_loss: 0.4295 - val_accuracy: 0.8896\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1965 - accuracy: 0.9321 - val_loss: 0.4682 - val_accuracy: 0.8906\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1779 - accuracy: 0.9383 - val_loss: 0.4540 - val_accuracy: 0.8860\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1708 - accuracy: 0.9425 - val_loss: 0.4535 - val_accuracy: 0.8886\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1601 - accuracy: 0.9461 - val_loss: 0.4759 - val_accuracy: 0.8930\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1499 - accuracy: 0.9495 - val_loss: 0.4859 - val_accuracy: 0.8890\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1421 - accuracy: 0.9512 - val_loss: 0.5073 - val_accuracy: 0.8902\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1367 - accuracy: 0.9537 - val_loss: 0.5007 - val_accuracy: 0.8960\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1247 - accuracy: 0.9589 - val_loss: 0.5281 - val_accuracy: 0.8916\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1138 - accuracy: 0.9627 - val_loss: 0.5562 - val_accuracy: 0.8890\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1126 - accuracy: 0.9630 - val_loss: 0.5542 - val_accuracy: 0.8922\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Note: the `batch` argument is reset at each epoch\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, lr * 0.1**(1 / s))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "lr0 = 0.01\n",
    "optimizer = keras.optimizers.Nadam(lr=lr0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "exp_decay = ExponentialDecay(s)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[exp_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1718"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) // 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = n_epochs * len(X_train) // 32\n",
    "steps = np.arange(n_steps)\n",
    "lrs = lr0 * 0.1**(steps / s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(steps, lrs, \"-\", linewidth=2)\n",
    "plt.axis([0, n_steps - 1, 0, lr0 * 1.1])\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling (per batch)\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise Constant Scheduling  \n",
    "\n",
    "Use a constant learning rate for a number of epochs (e.g., $\\eta_0 = 0.1$ for 5 epochs), then a smaller learning rate for another number of epochs (e.g., $\\eta_1 = 0.001$ for 50 epochs), and so on. Although this solution can work very well, it requires fiddling around to figure out the right sequence of learning rates and how long to use each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For piecewise constant scheduling, you can use a schedule function like the following one (as earlier, you can define a more general function if you want), then create a `LearningRateScheduler` callback with this function and pass it to the `fit()` method, just like we did for exponential scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.1501 - accuracy: 0.7354 - val_loss: 0.9740 - val_accuracy: 0.7116\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8129 - accuracy: 0.7569 - val_loss: 0.9948 - val_accuracy: 0.7610\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.0185 - accuracy: 0.7000 - val_loss: 1.3618 - val_accuracy: 0.6616\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.9635 - accuracy: 0.6890 - val_loss: 0.7775 - val_accuracy: 0.7446\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8876 - accuracy: 0.7321 - val_loss: 0.9283 - val_accuracy: 0.7594\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6258 - accuracy: 0.8127 - val_loss: 0.6493 - val_accuracy: 0.8222\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5382 - accuracy: 0.8410 - val_loss: 0.6165 - val_accuracy: 0.8298\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5149 - accuracy: 0.8480 - val_loss: 0.8097 - val_accuracy: 0.7980\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5011 - accuracy: 0.8502 - val_loss: 0.6219 - val_accuracy: 0.8350\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4711 - accuracy: 0.8610 - val_loss: 0.6382 - val_accuracy: 0.8338\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4699 - accuracy: 0.8598 - val_loss: 0.6676 - val_accuracy: 0.8474\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4486 - accuracy: 0.8668 - val_loss: 0.6715 - val_accuracy: 0.8394\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4510 - accuracy: 0.8668 - val_loss: 0.7093 - val_accuracy: 0.8490\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4872 - accuracy: 0.8682 - val_loss: 0.6182 - val_accuracy: 0.8588\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4255 - accuracy: 0.8761 - val_loss: 0.6824 - val_accuracy: 0.8524\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3316 - accuracy: 0.8947 - val_loss: 0.5813 - val_accuracy: 0.8782\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2786 - accuracy: 0.9115 - val_loss: 0.5444 - val_accuracy: 0.8716\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2697 - accuracy: 0.9143 - val_loss: 0.5286 - val_accuracy: 0.8804\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2523 - accuracy: 0.9189 - val_loss: 0.5387 - val_accuracy: 0.8854\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2429 - accuracy: 0.9199 - val_loss: 0.5398 - val_accuracy: 0.8834\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2315 - accuracy: 0.9227 - val_loss: 0.5856 - val_accuracy: 0.8780\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2229 - accuracy: 0.9243 - val_loss: 0.5830 - val_accuracy: 0.8816\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2111 - accuracy: 0.9295 - val_loss: 0.5910 - val_accuracy: 0.8802\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1967 - accuracy: 0.9357 - val_loss: 0.5698 - val_accuracy: 0.8790\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1966 - accuracy: 0.9338 - val_loss: 0.5736 - val_accuracy: 0.8782\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEeCAYAAAC30gOQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtwklEQVR4nO3de5xcRZ338c8398nkRiaBQNAMCg4SgSCKq1kkKyLu6j5GcS+CCLKKl+XxhiDsiiLqIqzsIi4i2RUBRVfdBwiICy7CrKJ4AZGEAIlyCSThkgQTMsnk/nv+qNPhpNM9c2Yy3Z2Z/r5fr37N9Kmq03Uqnf5NnaquUkRgZmY20IY1ugJmZjY0OcCYmVlNOMCYmVlNOMCYmVlNOMCYmVlNOMCYmVlNOMDYLiSdKqmr0fXoiaSQ9M5G18OKkXS1pB/W4LxTsvfCnD6Uac/KvKrScxs4DjBNKPvPHtlji6RHJX1ZUmuW5XvASxpZxwL2BW6u5QtIGi/p85IelNQt6RlJnZLeJaku/3dq+eHXl3NLOkbSTyStkrRB0iOSrpM0YaDr1QBPkt5Pv2twPYacEY2ugDXM7cDJwEjgaOA/gFbgQxHRDXQ3sG69ioina3l+SZOAu4C9gE8DvwY2A38KnAfcDTxeyzrsKSQdAtwKfB34GLAeOBCYC4xuWMUGSERsA2r6fmpaEeFHkz2Aq4Eflh37d+Cp7PdTga6y9L8E7gU2Ao8BXwRG5dJHAf8ELAU2AY8CH8mlHwLcAqwDngW+C0zL0l4ORO75WNKH+X/nyr8f+H3ueQDvzD3/TO61nwauzaUJOBt4hBQ4FwLv7qWNvkb6IN2/QtoYYEz2+17ANcAfs3PfDszM5T0V6AKOBR7IznkncEAuz4uA+cBzwAbgYeBvc9eZf3Rmx18N/BhYBTxPCoavLatnAKcDP8he99H8dVc7d4Xr/RiwrMD76mDgJmBtds13A4fm33PAR4HlWXt9Exjbl3+n7LpL78P7gLdkdZ+Tpc/Jnk/JlWnPjr2q4PPSOY4FfpX9m9wDvLKsLqcBT2TpNwMfBqLR/7/3pIdvkVlJN6k3swtJxwPXAf8GzCT9x3onKaCUXAO8B/gEKWD8HbAmK78v8FPSB+xRwBuBccBNkoZFxEPAM6T/2ACzSR9Sfyqp1MueA3RWqd8JwCdJ/8EPAt5K6nGUfCGrz9+TAt2FwJWS3lLlfMOAvwWui4hl5ekRsTEiNmZPrwZeA7wtu7YNwK2SWnJFRgPnktrttcAkUm+g5GukoPpnpPb9GFnbZecEeDPpNs47sufjgW+Rep9HkW7v/EjSlLLqfoYUvA4n3fq8StKMXs5d7mlgqqQ/q5KOpP1IQS6A44BXApcDw3PZjgZeQfr3/xvg7aSAU9Ljv1N2C/cWUqB8FXAO8OVqdRoAF2av8UpgNXCdJGV1eS2p1385MIsUWD9Xw7oMTo2OcH7U/0FZD4b0QbMK+F72/FRyPRhScDiv7BxzSX+livShHsCbq7zeBcBPyo7tlZU5Knv+PeDK7PcvAleQbkG9Nju2DDgpV35HD4YU1BYDIyu8dispeB5ddvxS4EdV6rt3dv6P99KOpet+fe7YRFJwfF+uLQPoyOU5idRDG5Y9XwB8tsprtJP767qHugh4il17KBfmno8gBcB39/Hcw0m9jSD9IXBz1uZTc3m+SOpBjqpyjqtJYx0jcsf+Hbi96L8TqTe2BhiXS383tevBHJ87x+zs2P7Z8+8Ct5bVdR7uwez0cA+meb1ZUpekjaRbGT8F/m+VvEcC/5jl78pmmH2H9KEwDTgC2E669VOt/OvLyj+Zpb00+9nJCz2YOdm5/heYI+kgYDpVejCkW0BjgMckfUPSX0kqjQ0ckqXdWvb6H8q9djlVOV7u5aTrvrt0ICLWkm7tHJLLtykiFueeryD1Fidlz78CfFrS3ZK+IOnI3l5Y0t6SrpS0RNJa0q3HvYEXl2VdkKvbVmBllq+wiNgWEe8F9if1FJ8AzgIeljQzy3YEcFdEbO7hVA9mdShZkatLkX+nlwMLIiI/w/FuamdB7vcV2c9SfQ9m514ypNtpluNB/ub1U9JfhFuAFRGxpYe8w0jd/x9USFtJ7x/Iw0i3Nj5ZIe2Z7Gcn8LUsmLwqe94KvIvUu/pDRCyvdPKIeFJSB+me+RuBS4DPSnoNL8yU/EvSB2NetWteSRojeHkv19XTdeeXKd9aJW0YQER8Q9JtwF+Q6v8LSRdGxPk9nP8aYB/g46Se3ibgJ6SxsLzyawz6OXs0a/9vAd+S9GlgCSnQnEqxoNxTXYr8OxV5je0V8la89VtAvr47/Ztl5w+sR+7BNK8NEfGHiFjaS3AB+C1wcJa//LE1Sx9GGkOoVn4msLRC+XUA8cI4zD+SgsmzpF7MbNI9/c6eKhhpXOSWiPg4aSB4Zlb2QdKH74wKr720yrm2k27ZnSRp//J0SWMkjcnOPYw0rlJKmwAcmqUVFhHLImJeRPw1adzk9Cyp1CMYXlbkT4GvZte8iNSD2bcvr9nDuYvU94+kW3LjskO/JY2ZlQe4oor8Oz0IHJqbTg/wJ2XnWZn9zLfFrH7WqScP8cIYVkn586bnAGNFXACcKOkCSa+QdLCkd0q6GCAifg98H/gPSSdIOkDS0ZJOzspfThqb+J6k10h6iaQ3SponaXzudf6XdE/9zuy8j5M+MN5BDwEm+2Lo+yQdKukA4L2kvz5/nwWwLwNflnSapAMlzZL0QUmnVzsn8A+kv6R/Jem9kmZmZU8mzWKall33fNJA9NGSDgW+TZrV9Z2CbYukr0h6c9Yus0iD7qUA9SxpbOJ4SftImpgdXwK8W9Ihkl4N/CcvBIyiqp27vH4fkHSFpDdJemnWFheRAumNWbavkYLN9yW9Omurd2XX06uC/07fIfUGr8rqcBzpD5K8P5Buv54v6WWS3kSaZj7QLgPeJOksSQdJ+jvSpAXLa/QgkB/1f1BhmnJZ+qnsOk35TcDPSIPEz5OmbZ6RSx8NXEyagrqJNNU0n34Q8F+8MJ13MfBVdp7q/EF2nX58dXZsell98oP8c0n34teQpuP+BnhrLq9I40ulv5JXAv8DHNdLO00kDV4/TJoW+ywp0P0tLwzQF5qmXHbeOeQGorN2+H32GitJwWJ6Lv/7SMFuGy9MUz6cdM+/O2vrk0mz9M6v1Ea5Y48Dn+zp3BXa4YjsGkvTh1cDvwROLss3E/gRafLHOuAXwCuqveeA84EH+vLvRJqx99ss/X7SLbUdg/xZnteRZtV1Z++L0lTmvg7yV50okB07jRTMukkTH84Euhv9/3tPeihrKDMz2w2S/hV4Y0Qc2ui67Ck8yG9m1g+SziL1sLpIkzM+SLq1ahn3YMzM+kHS90i30yaSVre4EvhK+EN1BwcYMzOrCc8iMzOzmvAYTGbSpElx4IEHNroae5z169fT2trae8Ym43bZlduksqHeLvfee++qiJhaKc0BJrPPPvtwzz33NLoae5zOzk7mzJnT6Grscdwuu3KbVDbU20VSxS8sg2+RmZlZjTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTTjAmJlZTdQ1wEiaLOkGSeslLZV0YpV8knSRpNXZ42JJyqXPk7RY0nZJp1Yo/3FJT0taK+kqSaN7q9vjz29n9pfu4Mb7lhe6lhvvW87sL93BAefcMiTLmZntrnr3YC4HNgP7ACcBV0iaWSHf6cBc4HDgMOCtwAdy6fcDHwZ+W15Q0vHAOcCxQDvwEuBzRSq3fE03516/sNcP4RvvW8651y9k+ZpuYgiWMzMbCHXb0VJSK3AC8IqI6ALuknQTcDIpIOSdAlwSEcuyspcA7we+DhARl2fHN1Z4qVOAb0TEoizP54HrKrxGRd1btvEPNyzkrj+sqprnRwufonvLtkFb7p9vW8zcI6ZXLWdmNhDquWXyy4BtEbEkd+x+4JgKeWdmafl8lXo6lcwE5peV3UdSW0SszmeUdDqpt8SoaQfuOL5h8zbuXFT9r/wNm6PK8cFRbvmabjo7O6uWy+vq6iqct5m4XXblNqmsmdulngFmHLC27NhaYHyBvGuBcZIUEZU/NXsuS/Y6OwWYiJgHzAMYve9BO847fVILPz/nDVVfYPaX7mD5mu5djg+mckX3CB/q+4n3l9tlV26Typq5Xeo5BtMFTCg7NgFYVyDvBKCrQHCpVpYqr7OLlpHDOev4jh7znHV8By0jhw/ZcmZmA6GeAWYJMELSQbljhwOLKuRdlKX1lq+SSmWfKb89Vsn0SS1c+I5Dex2fmHvEdC58x6FMn9SCBkG5ttZRAEwZN6pQOTOzgVC3W2QRsV7S9cAFkt4HzALeBryuQvZrgU9I+hEQwJnAV0uJkkaRgqOAkZLGAJsjYntW9mpJ1wFPAZ8Gru6tfu0ThvV4u6nc3COm9+uDuhHlDn/RJP7sy52c8+cvd3Axs7qp9zTlDwMtwLPAd4EPRcQiSUdL6srluxK4GVgIPADckh0r+THQTQpO87LfXw8QEbcCFwN3Akuzx2dreE17vP33amH4MLF09fpGV8XMmkg9B/mJiOdI328pP/4z0uB86XkAZ2ePSueZ08vr/AvwL7tR1SFl5PBh7L9XC4+tcoAxs/rxUjFNYkZbK0tXb2h0NcysiTjANIn2trE8vno9xSbimZntPgeYJjGjrZV1G7fyxw1bGl0VM2sSDjBNor1tLACPe6DfzOrEAaZJzGhrBfBMMjOrGweYJvGiyS1I8PgqD/SbWX04wDSJ0SOGs9/EFt8iM7O6cYBpIgdMaeVxT1U2szpxgGkiM9rGegzGzOrGAaaJtLe1smbDFtZs2NzoqphZE3CAaSIzsqnK/ka/mdWDA0wTaZ+Spip7oN/M6sEBpom8eLJ7MGZWPw4wTWTMyOHsO3GMezBmVhcOME2mva2Vx71sv5nVgQNMk2mfMta3yMysLhxgmsyMtlZWr9/M8xu9qrKZ1ZYDTJMprar8hHsxZlZjDjBNprSqsgf6zazWHGCajL9saWb14gDTZMaOGsHe40d7JpmZ1ZwDTBNqb2v1LTIzqzkHmCbUPmWsl+03s5pzgGlCM9paWbluE+s3bW10VcxsCHOAaULt2UwyD/SbWS05wDShF2aSeRzGzGrHAaYJlQKMx2HMrJYcYJrQ+DEjmTJulHswZlZTDjBNaoanKptZjTnANKm0bL9vkZlZ7dQ1wEiaLOkGSeslLZV0YpV8knSRpNXZ42JJyqXPknSvpA3Zz1m5tNGSvi7pGUnPSbpZ0vQ6XN6g0t42lqef30j35m2NroqZDVH17sFcDmwG9gFOAq6QNLNCvtOBucDhwGHAW4EPAEgaBcwHvg3sBVwDzM+OA3wUeG1Wbj9gDfDVmlzNIDZjSpqq/MRz7sWYWW3ULcBIagVOAM6LiK6IuAu4CTi5QvZTgEsiYllELAcuAU7N0uYAI4BLI2JTRFwGCHhDln4AcFtEPBMRG4H/BCoFsabWvmMmmcdhzKw2RtTxtV4GbIuIJblj9wPHVMg7M0vL55uZS1sQEZFLX5AdvxX4BvAVSaXey0nAf1eqkKTTSb0lpk6dSmdnZ9+uaBBbvyU13x2/XsjolQ9XzdfV1dVU7VKU22VXbpPKmrld6hlgxgFry46tBcYXyLsWGJeNw/R2niXAE8ByYBuwEDijUoUiYh4wD6CjoyPmzJlT8FKGhk/f/WOGT5rGnDmHVs3T2dlJs7VLEW6XXblNKmvmdqnnGEwXMKHs2ARgXYG8E4CurNfS23muAMYAbUArcD1VejDNbkZbq78LY2Y1UzjASPpzST+U9KCkF2XH3ifp2IKnWAKMkHRQ7tjhwKIKeRdlaZXyLQIOy88qIw3oL8rlvToinouITaQB/qMkTSlYz6bR3jbWU5XNrGYKBRhJJwHfB35PGkQfmSUNB84uco6IWE/qTVwgqVXSbOBtwLcqZL8W+ISk6dlYypnA1VlaJ+nW10eyKcml2193ZD9/A7xH0kRJI4EPAysiYlWRejaT9imtrFjbzcYtnqpsZgOvaA/mbOD9EfFxIL/G+y+BWX14vQ8DLcCzwHeBD0XEIklHS+rK5bsSuJk0fvIAcEt2jIjYTJrC/B7SIP5pwNzsOMAngY2kYLgS+Avg7X2oY9Nob2slApb90b0YMxt4RQf5DwLurnC80nhIVRHxHCk4lB//GWnwvvQ8SEGtYu8oIu4DjqyStpo0c8x6sWPRy1UbOHDvSnMtzMz6r2gPZgVpmnG51wOPDFx1rJ5K+8L4uzBmVgtFA8w84LJs3ATgRZJOAS4mzdqyQWjS2JFMGDPCG4+ZWU0UukUWERdLmgj8D2kK8J3AJuDLEXF5DetnNSSJ9ileVdnMaqPwFy0j4h8lfRE4hNTzeTAiunopZnu4GW2t3P/kmkZXw8yGoKLTlK+SND4iNkTEPRHx64joyqYbX1XrSlrtHNA2lmV/3MDmrdsbXRUzG2KKjsGcQppeXK6FNF3YBqkZba1s91RlM6uBHm+RSZpMWqlYwF6S8t+BGQ68BXimdtWzWmufkqYqL129gZdMHddLbjOz4nobg1kFRPZ4sEJ6AJ8d6EpZ/czwVGUzq5HeAsyfkXovd5D2cnkul7YZWBoRK2pUN6uDttZRjBvtqcpmNvB6DDAR8b8Akg4AnowIjwQPMZKY0TbWPRgzG3BFvwezFCBbePLFwKiy9J8OfNWsXtrbWnnwqecbXQ0zG2IKBZgssHyHtDRMkG6b5XeUHD7wVbN6mdE2ltsWPc3WbdsZMbyeWwSZ2VBW9NPkUtIS+YcAG4Cjgb8CHgLeXJOaWd20T2ll6/Zg+ZruRlfFzIaQot/kPwZ4S0Q8LCmAlRHxc0mbgM+TlpCxQeqFRS837JhVZma2u4r2YFpIU5YhzSTbO/v9QdJukjaItbeVvgvjgX4zGzhFA8zDwMHZ778DPihpBvD3wPIa1MvqaOr40bSMHO7tk81sQBW9RfYVYFr2+wXArcC7SCsqn1KDelkdlaYquwdjZgOp6DTl63K//1ZSO6lH84T3uh8a2tta+f2z6xpdDTMbQvo1JzVbVfm3wHpJ5wxwnawBZkwZy5PPdbNte/Se2cysgF4DjKQpkt4i6U2ShmfHRkr6GPA48MnaVtHq4YC2VjZv284KT1U2swHSY4CR9Drg98DNwH8DP5d0MLAAOIM0RfnFta6k1V5perLXJDOzgdJbD+bzwG2kqchfAY4CfghcCBwUEf8WEf5EGgJKy/Z7TTIzGyi9BZjDgc9HxAPAp0nLw5wbEddGhG/WDyH7jB/D6BHDPJPMzAZMbwFmMrAS0sA+aZmY+2pdKau/YcNKqyq7Q2pmA6PINOXSTpalBS4nZDtd7hARz1UsaYPKjLZW92DMbMAUCTD5nSwF/KbseeDVlIeE9rax/HTJSrZvD4YNU6OrY2aDXJEdLa1JzGhrZdPW7Tz9/Eb2m9TS6OqY2SBXaEdLaw4HTCmtqrzeAcbMdpt3l7IdZuxYVdkD/Wa2++oaYCRNlnSDpPWSlko6sUo+SbpI0urscbEk5dJnSbpX0obs56yy8q+U9FNJXZKekfTRGl/akLDvxBZGDR/m78KY2YCodw/mcmAzsA9wEnCFpJkV8p0OzCV9D+cw4K3ABwAkjQLmA98G9gKuAeZnx5E0hbTa85VAG3Ag8OOaXdEQMnyYeNHkFpZ62X4zGwB1CzCSWoETgPMioisi7gJuAk6ukP0U4JKIWBYRy4FLgFOztDmksaNLI2JTRFxGms32hiz9E8BtEXFdlr4uIh6q2YUNMe1tre7BmNmAKLofzEB4GbAtIpbkjt1P2o653MwsLZ9vZi5tQdlKAguy47cCfwIslPQLUu/lV8DfR8QT5S8i6XRSb4mpU6fS2dnZj8saWoZ3b+LRlVu58847kURXV5fbpQK3y67cJpU1c7sUCjCSrqqSFMBG4A/A9yJiRQ+nGQesLTu2FhhfIO9aYFw2DtPbefYHXgkcBywELga+C8zepfIR84B5AB0dHTFnzpweqt8cnhj9OD9euoiZR76WvSeMobOzE7fLrtwuu3KbVNbM7VK0BzMVOBrYDjyQHXsF6dbUvcA7gAskHR0Rv6tyji5gQtmxCUClXa7K804AuiIiJPV2nm7ghoj4DYCkzwGrJE2MiPLAZGXas1WVH1u1nr0njGlwbcxsMCs6BvNz0nL9+0fE6yPi9aSewo9IA+gzgFtIYyXVLAFGSDood+xwYFGFvIuytEr5FgGH5WeVkSYClNIXkHpWJaXf/dX0Atq9bL+ZDZCiAeajwAX5pfmz378IfDwiNgMXAbOqnSAi1gPXk3o6rZJmA28DvlUh+7XAJyRNl7QfcCZwdZbWCWwDPiJptKQzsuN3ZD+/Cbw9m8o8EjgPuCsi1hS81qa236QxjBgmD/Sb2W4rGmDGAftWOD4tSwN4nt5vuX0YaAGeJY2LfCgiFkk6Orv1VXIlaZOzhaRbcrdkx8iC2VzgPcAa4DRgbnaciLgD+IeszLOkgf6K37exXY0YPowXTR7rHoyZ7baiYzA3AN+QdDZpscsgbT52MalXQvZ8SeXiSbbq8twKx3/GC4GKbIbY2dmj0nnuA47s4XWuAK7oqS5WXVq23z0YM9s9RQPMB4F/IX25sVRmK3AV8Mns+UPA+we0dtYQ7W2t3PP4H/Gecma2OwoFmGy85YOSzgReShow/0M2rlLK87ua1NDqbkbbWLo2bWX1+s2NroqZDWJ9+qJlFlAW1KgutocozSR7fJVvk5lZ/xX9ouUY0kyyY4G9KZscEBGHDXzVrFHadyzbv4EpDa6LmQ1eRXswXwPeDvwA+AU7f8/Ehpjpk1oYPkwsXb2eKaMaXRszG6yKBpi5wF9FxO01rIvtIUaNGMb0SS08vnoDR1aanG5mVkDR78FsAJ6sZUVszzKjbSxLPVXZzHZD0QBzMemb9d4Bs0m0t7Xy2Kr1nqpsZv1W9BbZcaTFLt8s6UFgSz4xIv7PQFfMGmtG21jWbdzK+i0ehDGz/ikaYFaRvs1vTaI0VfmZDdsbXBMzG6yKftHyvbWuiO1ZHlmZlob7/C83ctXDd3DW8R3MPWJ6r+VuvG85/3zbYlas6Wa/SS1DttzyNd1M/6Xbxawn9dzR0gaJG+9bzr/e/sKycsvXdHPu9QsBevywufG+5Zx7/UK6t2xzuSYqZ1aNqg3iSloAHBMRf5S0kB6++zIUvmjZ0dERixcvbnQ19gizv3QHy9d073J85HBxyH4Tq5Z7cMVatmzb9W3icoO73PRJLfz8nDdULVfSzDs39mSot4ukeyPiVZXSeurB/D9gU/b7fw14rWyPtaJCcAHYsi2Y1DKyarlKH04uN/jLVXs/mPWmaoCJiM9V+t2Gvv0mtVTswUyf1MI1px1VtVy1no/LDe5y+01qqVrGrCf+Xovt4qzjO2gZOXynYy0jh3PW8R0u53JmhRVd7HIyaXvkaotdThj4qlmjlAZ0d8yWKjibKF+uL7OQBmO5odwupYH+otdnVk3VQf6dMkk3AEcA84AVlA34R8Q1NaldHXmQv7KhPkDZX0O5Xa7ofISLbn2Y+z/zJiaOrT5mU24ot8nuGOrt0t9B/rxjgeMi4lcDVy0z2xMdPG08AIufWcdRB0xucG1sMCs6BvMs0FXLipjZnqGjFGCefr7BNbHBrmiA+UfgAknjalkZM2u8fSeOYcKYETz89LpGV8UGuaK3yD4NtAPPSlrKrotdDvovWppZIomDp01gsQOM7aaiAcZftDRrIh3TxnPjfcuJCCQ1ujo2SPUaYCSNBFqByyNiae2rZGaN1jFtPOs2bWX5mm7232tso6tjg1SvYzARsQX4EOA/Y8yaxI6ZZL5NZruh6CD/j4HeV7szsyHhZVmA8UC/7Y6iYzA/Af5J0mHAvcBOm7VHxPUDXTEza5wJY0YyfVKLezC2W4oGmH/Lfn6kQloAwyscN7NB7OBp4x1gbLcUukUWEcN6eDi4mA1BHdPG88jKLjZv9bbZ1j9eTdnMKuqYNp6t22PH9tlmfVU4wEiaLOlESedI+kz+0cdz3CBpvaSlkk6skk+SLpK0OntcrNxkfEmzJN0raUP2c1aFc4yS9LCkZUXrZ2YvOHhaWiTdt8msv4ou1/8nwC2kHS6nAsuBfbPnjwMXFHy9y4HNwD7ALOAWSfdHxKKyfKcDc4HDSWM8/wM8Cnxd0ihgPnAp8DXgA8B8SQdFxObcOc4iraHm5W3M+uElU1sZOVyeSWb9VrQH88/AdcB0YCNpyvKLgXuAi4qcQFIrcAJwXkR0RcRdwE3AyRWynwJcEhHLImI5cAlwapY2hxQYL42ITRFxGek7OjumUUs6AHg3cGHB6zOzMiOHD+OlU8d50Uvrt6KzyA4D/i4iQtI2YHREPCrpU8B3SMGnNy8DtkXEktyx+4FjKuSdmaXl883MpS2InTeyWZAdvzV7/lXgH4AeNxOXdDqpt8TUqVPp7OwscBnNpaury+1SQbO0y17ayP1Li11rs7RJXzVzuxQNMPlbT88AM4CHSEv471fwHOOAtWXH1gLjC+RdC4zLxmF6PI+ktwMjIuIGSXN6qlBEzCNtokZHR0cM5U2B+muob5bUX83SLg/xCHff+jBHHDW7183HmqVN+qqZ26XoLbLfAq/Ofu8EviDpFOAyUu+hiC6gfGvlCUClG7zleScAXVmvpep5sttwFwP/t2CdzKwH+c3HzPqqL/vBrMh+/zSwknQbai+yW0wFLAFGSDood+xwoHyAn+zY4VXyLQIOy88qI93CWwQcRNpW4GeSngauB/aV9LSk9oL1NLOMNx+z3VHoFllE3JP7fSXw5319oYhYL+l60sZl7yPNInsb8LoK2a8FPiHpR6RZZGeSAhqkHtQ24COSvg68Pzt+B7AdeFHuPK8jrULwSlJQNLM+2HfiGMZ78zHrpz590VLSqyT9TXYrCkmtkoqO4wB8GGghTR/+LvChiFgk6WhJ+W9zXQncDCwEHiBNkb4SIJuKPBd4D7AGOA2YGxGbI2JrRDxdegDPAduz59v6cq1mVtp8zEvGWP8U/R7MPqQpxa8m9SgOIn0v5V9I05Y/WuQ8EfEcKTiUH/8Zue+rZGMtZ2ePSue5DziywOt1AvsXqZuZVXbwtAnc+DtvPmZ9V7QH86/A00AbsCF3/AfAmwa6Uma25+iYNp51G7eyYu3GRlfFBpmit7eOBY6NiD+W/QXzCOkLl2Y2RJVmkj381PNMn9TS4NrYYFK0B9PCzt+FKZlKukVmZkOUNx+z/ioaYH7KC0u1AISk4cCnSJuRmdkQ5c3HrL+K3iI7G/hfSa8GRpPWBpsJTARm16huZraH6PBMMuuHohuOPQgcCvwC+DEwhjTAf0REPFK76pnZnuBgbz5m/VD4OyzZ90o+mz8maYak70fEXw94zcxsj1HafOzRVV079okx683u7mg5ibQEv5kNYaWg8vBTvk1mxXnLZDPrlTcfs/5wgDGzXnnzMesPBxgzK8Qzyayvehzkl3RTL+U92mfWJA6eNoH5v1vB2u4tTGzpefMxM+h9FtnqAumPDVBdzGwPVloyZskz63h1++QG18YGgx4DTES8t14VMbM9W0duTTIHGCvCYzBmVog3H7O+coAxs0K8+Zj1lQOMmRXWMW08i59ZR9oT0KxnDjBmVtjB0yZ48zErzAHGzAorzSTzFy6tCAcYMyustPnYQ16TzApwgDGzwrz5mPWFA4yZ9YmXjLGiHGDMrE86vPmYFeQAY2Z9cnBu8zGznjjAmFmflDYf820y640DjJn1SWnzMc8ks944wJhZn3jzMSvKAcbM+swzyawIBxgz67OOaeNZsXYja7u3NLoqtgera4CRNFnSDZLWS1oq6cQq+STpIkmrs8fFkpRLnyXpXkkbsp+zcmlnSXpA0jpJj0k6qw6XZtZUXp4N9C95xr0Yq67ePZjLgc3APsBJwBWSZlbIdzowFzgcOAx4K/ABAEmjgPnAt4G9gGuA+dlxAAHvydLeDJwh6W9rdD1mTWnH5mO+TWY9qFuAkdQKnACcFxFdEXEXcBNwcoXspwCXRMSyiFgOXAKcmqXNIe3EeWlEbIqIy0hB5Q0AEXFxRPw2IrZGxGJSMJpdw0szazo7Nh97ygP9Vl2PWyYPsJcB2yJiSe7Y/cAxFfLOzNLy+Wbm0hbEzhtSLMiO35o/SXZb7WjgykoVknQ6qbfE1KlT6ezsLHotTaOrq8vtUoHbBfZt2c6vFy+js3M14DapppnbpZ4BZhywtuzYWmB8gbxrgXFZwOjLec4n9dK+WalCETEPmAfQ0dERc+bM6fECmlFnZydul125XeD2NQuZ/7sVHHPMMUhym1TRzO1SzzGYLmBC2bEJQKWbuOV5JwBdWa+l0HkknUEai3lLRGzajXqbWQUd3nzMelHPALMEGCHpoNyxw4FFFfIuytIq5VsEHJafVUaaCLDjPJJOA84Bjo2IZQNQdzMr483HrDd1CzARsR64HrhAUquk2cDbgG9VyH4t8AlJ0yXtB5wJXJ2ldQLbgI9IGp31VADuAJB0EvBPwHER8Witrses2XkmmfWm3tOUPwy0AM8C3wU+FBGLJB0tKb8065XAzcBC4AHgluwYEbGZNIX5PcAa4DRgbnYc4AtAG/AbSV3Z4+u1vjCzZlPafOxhr0lmVdRzkJ+IeI4UHMqP/4w0eF96HsDZ2aPSee4DjqySdsBA1NXMeuclY6wnXirGzPrNm49ZTxxgzKzfvPmY9cQBxsz6rWPHTDLfJrNdOcCYWb+9ZMo4Rg6XZ5JZRQ4wZtZvo0aUNh9zgLFdOcCY2W7pmDbei15aRQ4wZrZbSpuPrd8SvWe2puIAY2a7pbRkzPIuT1W2nTnAmNlu6ch2t1y2zgHGduYAY2a7Zb9s8zEHGCtX16VizGzomf+7FWzasp07ntzO7C/dwVnHdzD3iOm9lrvxvuX8822LWbGmm/0mtQzZcsvXdDP9l0O3XUZNO7Disl3gAGNmu+HG+5Zz7vUL2bwt9V6Wr+nm3OsXAvT4IVUq171lm8sNgXLVaOedh5tXR0dHLF68uNHV2OM08258PXG7JLO/dAfL13Tvcnz0iGG85iVtVcv96tHVbKqwfpnLDb5yT13zMTY99XtVyucejJn124oKwQVg09btPN+9pWq5Sh9qLjf4y5VzgDGzfttvUkvFHsz0SS3c+Pezq5ar1vNxucFdrpxnkZlZv511fActI4fvdKxl5HDOOr7D5ZqsXCXuwZhZv5UGgnfMlio4Cylfri+zlwZjuaHeLk/1kM+D/BkP8lfmwezK3C67cptUNtTbRdK9EfGqSmm+RWZmZjXhAGNmZjXhAGNmZjXhAGNmZjXhAGNmZjXhAGNmZjXhAGNmZjXhAGNmZjXhAGNmZjXhAGNmZjXhAGNmZjXhAGNmZjVR1wAjabKkGyStl7RU0olV8knSRZJWZ4+LJSmXPkvSvZI2ZD9nFS1rZmb1Ue8ezOXAZmAf4CTgCkkzK+Q7HZgLHA4cBrwV+ACApFHAfODbwF7ANcD87HiPZc3MrH7qFmAktQInAOdFRFdE3AXcBJxcIfspwCURsSwilgOXAKdmaXNI+9hcGhGbIuIyQMAbCpQ1M7M6qeeGYy8DtkXEktyx+4FjKuSdmaXl883MpS2InTeyWZAdv7WXsjuRdDqpxwOwSdIDxS6lqUwBVjW6Ensgt8uu3CaVDfV2mVEtoZ4BZhywtuzYWmB8gbxrgXHZWEpv56latiwoERHzgHkAku6ptmlOM3O7VOZ22ZXbpLJmbpd6jsF0ARPKjk0A1hXIOwHoygJEb+fpqayZmdVJPQPMEmCEpINyxw4HFlXIuyhLq5RvEXBY2cyww8rSq5U1M7M6qVuAiYj1wPXABZJaJc0G3gZ8q0L2a4FPSJouaT/gTODqLK0T2AZ8RNJoSWdkx+8oULYn8/p+VU3B7VKZ22VXbpPKmrZdVM87R5ImA1cBxwGrgXMi4juSjgb+OyLGZfkEXAS8Lyv6H8CnSre5JB2RHTsEeAj4u4i4r0hZMzOrj7oGGDMzax5eKsbMzGrCAcbMzGqi6QNM0fXRmo2kTkkbJXVlj8WNrlO9STpD0j2SNkm6uiztWEkPZ+vh3Smp6pfNhppq7SKpXVLk3jNdks5rYFXrKpt09I3sc2SdpPsk/XkuveneM00fYCi+PlozOiMixmWPjkZXpgFWAF8gTUzZQdIU0ozI84DJwD3A9+peu8ap2C45k3Lvm8/XsV6NNgJ4krQ6yUTS++P7WeBtyvdMPb/Jv8fJrY/2iojoAu6SVFof7ZyGVs4aLiKuB5D0KmD/XNI7gEUR8YMs/XxglaSDI+Lhule0znpol6aWfRXj/NyhH0p6DDgSaKMJ3zPN3oOptj6aezDJhZJWSfq5pDmNrsweZKf17rIPlkfw+6ZkqaRlkr6Z/eXelCTtQ/qMWUSTvmeaPcD0ZX20ZvMp4CXAdNIXxW6W9NLGVmmP4fdNZauAV5MWPzyS1B7XNbRGDSJpJOnar8l6KE35nmn2ANOX9dGaSkT8KiLWZVsiXAP8HPiLRtdrD+H3TQXZNhz3RMTWiHgGOAN4k6TythrSJA0jrVCymdQG0KTvmWYPMH1ZH63ZBWnfHStb7y4by3spft+UK32Lu2neN9lKIt8gTRo6ISK2ZElN+Z5p6gDTx/XRmoakSZKOlzRG0ghJJwGvB25rdN3qKbv2McBwYHipPYAbgFdIOiFL/wxpj6IhO1ibV61dJL1GUoekYZLagMuAzogovzU0lF0BvBz4y4jozh1vzvdMRDT1gzRl8EZgPfAEcGKj69ToBzAV+A2p+74G+CVwXKPr1YB2OJ/0V3j+cX6W9kbgYaCbtABre6Pr2+h2Ad4FPJb9X3qKtPDstEbXt47tMiNri42kW2Klx0nN+p7xWmRmZlYTTX2LzMzMascBxszMasIBxszMasIBxszMasIBxszMasIBxszMasIBxmyIyvZmeWej62HNywHGrAYkXZ19wJc/ftnoupnVS1PvB2NWY7eT9hbK29yIipg1gnswZrWzKSKeLns8BztuX50h6ZZsC92lkt6dLyzpUEm3S+qW9FzWK5pYlucUSQuz7YufKd/aGZgs6QfZluCPlr+GWS05wJg1zueAm4BZpD13rs12iUTSWOBW0lpWRwFvB15HbptiSR8ArgS+CRxG2k6hfHXezwDzSSv5fg+4qhn2grc9g9ciM6uBrCfxbtLCh3mXR8SnJAXwHxHx/lyZ24GnI+Ldkt4PfBnYPyLWZelzgDuBgyLiD5KWAd+OiIrbe2ev8aWIODd7PgJ4Hjg9Ir49cFdrVpnHYMxq56fA6WXH1uR+v7ss7W7gLdnvLyct557fkOoXwHbgEEnPk3Yb/UkvdVhQ+iUitkpaCexdqPZmu8kBxqx2NkTEH/pZVrywYVe5vmz+tqXseeBb41YnfqOZNc6fVHj+UPb7g8DhkvJ7tr+O9H/2oUhbEi8Hjq15Lc36yT0Ys9oZLWla2bFtEbEy+/0dkn5D2nzqnaRg8Zos7TrSJIBrJX0G2Is0oH99rlf0ReBfJT0D3AKMBY6NiEtqdUFmfeEAY1Y7byTt7Ji3HNg/+/184ATS1sIrgfdGxG8AImKDpOOBS4FfkyYLzAc+WjpRRFwhaTNwJnAR8Bzwoxpdi1mfeRaZWQNkM7z+KiL+q9F1MasVj8GYmVlNOMCYmVlN+BaZmZnVhHswZmZWEw4wZmZWEw4wZmZWEw4wZmZWEw4wZmZWE/8f9J0+YSoRm8cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Scheduling  \n",
    "\n",
    "Measure the validation error every N steps (just like for early stopping), and reduce the learning rate by a factor of $\\lambda$ when the error stops dropping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performance scheduling, use the `ReduceLROnPlateau` callback. For example, if you pass the following callback to the `fit()` method, it will multiply the learning rate by 0.5 whenever the best validation loss does not improve for five consecutive epochs (other options are available; please check the documentation for more details):  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7106 - accuracy: 0.7771 - val_loss: 0.5125 - val_accuracy: 0.8486\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4918 - accuracy: 0.8382 - val_loss: 0.6228 - val_accuracy: 0.8306\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5085 - accuracy: 0.8422 - val_loss: 0.4834 - val_accuracy: 0.8558\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5015 - accuracy: 0.8487 - val_loss: 0.5006 - val_accuracy: 0.8514\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5169 - accuracy: 0.8465 - val_loss: 0.6444 - val_accuracy: 0.8280\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4875 - accuracy: 0.8577 - val_loss: 0.5703 - val_accuracy: 0.8554\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5200 - accuracy: 0.8564 - val_loss: 0.6182 - val_accuracy: 0.8408\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5094 - accuracy: 0.8570 - val_loss: 0.6273 - val_accuracy: 0.8302\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3206 - accuracy: 0.8900 - val_loss: 0.3824 - val_accuracy: 0.8822\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2464 - accuracy: 0.9134 - val_loss: 0.3941 - val_accuracy: 0.8870\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2153 - accuracy: 0.9206 - val_loss: 0.4201 - val_accuracy: 0.8808\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2053 - accuracy: 0.9252 - val_loss: 0.4491 - val_accuracy: 0.8792\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1906 - accuracy: 0.9283 - val_loss: 0.4708 - val_accuracy: 0.8858\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1784 - accuracy: 0.9321 - val_loss: 0.4560 - val_accuracy: 0.8758\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1416 - accuracy: 0.9447 - val_loss: 0.4131 - val_accuracy: 0.8884\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1189 - accuracy: 0.9533 - val_loss: 0.4473 - val_accuracy: 0.8898\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1060 - accuracy: 0.9583 - val_loss: 0.4570 - val_accuracy: 0.8928\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1012 - accuracy: 0.9602 - val_loss: 0.4706 - val_accuracy: 0.8914\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0951 - accuracy: 0.9640 - val_loss: 0.4905 - val_accuracy: 0.8916\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0802 - accuracy: 0.9706 - val_loss: 0.4804 - val_accuracy: 0.8924\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0706 - accuracy: 0.9737 - val_loss: 0.4920 - val_accuracy: 0.8926\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0687 - accuracy: 0.9745 - val_loss: 0.5083 - val_accuracy: 0.8926\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0645 - accuracy: 0.9773 - val_loss: 0.5073 - val_accuracy: 0.8922\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0598 - accuracy: 0.9787 - val_loss: 0.5262 - val_accuracy: 0.8928\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0548 - accuracy: 0.9811 - val_loss: 0.5355 - val_accuracy: 0.8920\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAEeCAYAAADRiP/HAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABgj0lEQVR4nO2dd5hU5fX4PweWpaMCNkRAFCxYUNH4VRFUYslPI5ZEY8WGJQZj7yUitsQkGiuKnRB7N8agrIKiiUpE14JlAYFFaQJLW9g9vz/OHfbu7Mzsnd3ZmdnZ83me+8zct993Z+fM+76niKriOI7jOE7DaZXrATiO4zhOc8eFqeM4juM0EhemjuM4jtNIXJg6juM4TiNxYeo4juM4jcSFqeM4juM0EhemTotDRCpEZESux1FoiMhMEbk41+NwnFzgwtTJS0TkERHR4FonIrNF5F4R2SjXY8sEIjI0eLbuSfKvDz1/tYjME5HxIrJltscajGdEaDwqIuUi8pSIbNXINisyOU7HyRUuTJ18ZiKwOdAHOAM4HLgnlwPKMl9hz98TOBbYCXgqh+NZGYynB3A8MBB4SURa53BMjpMXuDB18pk1qjpfVeeo6hvAk8BB4QIicqqIfC4iq0VkhohcICKtQvnbiEhJkP+ViBwWV79PsNIaFJeuInJM6L5HsDJcJCIrReR/IrJ/KP9wEfko6KdMRMaISHEjn39d8PzzVHUy8ACwl4h0SVVJRI4SkU9FZI2IfC8iV4mIhPJnisjVInK/iCwTkTkickmE8WgwnnJVnQT8AdgR2CbJOC4UkekiskJE5orIgyKyYZA3FHgY6Bha7V4f5BWLyK3BuFaIyH9F5OBQu61FZFwwz6tE5GsRuTTu7/6IiLwSN57rReSzCM/pOGlTlOsBOE4URKQvcAiwNpR2JnAD8DvgI+yL/YGgzF3Bl+vzwBLg/4AOwB1A2zT77gi8DfwIHAnMBXYJ5R8MjAfOB94BegH3Bf1k5AxRRDYDjgKqgitZud2Bp4EbgzHtAdwPLAP+Fip6AXAd8EfgUOBOEZmiqlPTGNaq4LVNkvxq4PfAd0DvoP+/AScB7wV5NwFbB+VjW74PB2nHA3OAXwAvi8geqvoJtgiYC/waWADsCYwFFgHj0hi/42QOVfXLr7y7gEeAddgX7CpAg+uCUJnZwElx9X4PfB68PwgTPL1C+fsG7YwI7vsE94Pi2lHgmOD9mcByoHuSsb4DXBOXNjwYuySpMzToI1mb1wdjr8C2V2PPf0c98zYeeCtBW3NC9zOBCXFlvgauTtHuCKAidN8TmAp8DxSH2r04RRuHAGuAVonaDNK2xoRwr7j0F4B7UrR9CzAx7vPzSoJ5+CzXn22/CvPylamTz7wDjATaYwJta+BOABHZGNgSuF9E7g3VKQJiW5rbA3NVdXYo/wPsyzoddgWmq+rCJPm7A3uKyGWhtFbBuDcDytPsL8a32KqsLXAEcDRwZT11tgdejUubAlwnIl1UdVmQNj2uzDxgk3ra7hgoDAm2yv8YOEpVKxMVFpEDgCuCMW0AtAaKsTmZl6SP3YL2Pw/tTIPNwVuhts/GztF7Y/PcBphVz/gdp8lwYerkMytV9Zvg/SgRmQRcg60wYudjZ2NbhomQJOlhYoI1fKYYv21ZXzutsPPDpxPkLYgwhmRUhp6/VET6AXdjK7pkCLaCTUQ4fW2CvPp0KFZiSkfVwA+quiLpIER6Y0L9AeBabAt2N2ACJlCT0SoYyx4JxrgqaPtY4K/YFvp72Bb2b7Et+BjV1P27JduOdpxG48LUaU78AfiniIxV1XkiMhfYWlUfS1L+c2ALEdlSVb8P0vakttCICbvNQ2kD49r5GDhRRLonWZ1+DGwXEnxNxWjgKxH5m6p+lKTM59hWdph9sW3e5Y3sX9N4xkGY0LxAVasA4pW/gEpstRpmGiYEN1NTckrEvsAHqnpXLEFEto4rs4C6f8f4e8fJGK7N6zQbVLUEKAWuDpKuBy4NNHi3FZEdReRkEbkiyJ8IfAk8JiIDReT/gL9gZ7GxNlcB7wOXicgAEdkb+FNc13/HlI9eEJHBIrKViPwypM17A3C8iNwQjGE7ETlGRG6L8Fg7BmMLXwn/L1X1O+AlTKgm43ZgSKC52l9ETgAuAqKMJZN8jX2//D6Yr99g59lhZgLtROTnItJdRDqo6gzs3PeRYA77isggEblYRI4K6s0AdhORQ0Wkn4hcAwyJa/stYFcROU1Mo/tSYJ8melbHcQUkv/LzIoECSZB+PKbE0ju4/w22MlyNae1OAY4Lle+PaeKuwb7gf4kp9YwIldkeeBfbxvwUGExIASko0xMzzfkpKDcNGBrKPwiYHOQtAz4EzkvxfEOpUSqKvzqRRFkG2Dsos3eKto8KnqMSUxC6ipAiFAkUhYAS4K4UbY4gTlkoQZla7QKjMK3bVcCbmPatAn1CZe4FFgbp1wdpbYLn/y54hvnYj4jdg/xiTGt3SfD3GIdtJc+MG8/12Hn1Usw++aZEc+qXX5m4RDXZ8YrjOI7jOFHwbV7HcRzHaSQuTB3HcRynkbgwdRzHcZxG4sLUcRzHcRqJ25lGpFWrVtq+fftcDyOvqK6uplUr/z0Wj89LYnxeElPI89K+fXsWLVq0UFU3zvVYmhoXphEpLi5mxYqkDl9aJCUlJQwdOjTXw8g7fF4S4/OSmEKfFxHpkOsxZIPC/DnkOI7jOFnEhanjOI6TO0S6IvI8IisQmYXI8SnK7obIO4hUIPIDIueH8mYisirIq0DkjWwMP4Zv8zqO4zi55G7M09WmmP/kVxH5BNXSWqVEugOvY7F4n8E8YfWMa+twVCc29YAT4StTx3EcJzeIdMRCC16DagWqUzDXkSclKH0h8C9Ux6O6BtXlqH6RzeGmwoWp4ziO02R0hyJEPgxdI0PZ/YEqLMBBjE+AAQma2gtYjMh7iPyIyMuI9IorMx6RBYi8gcguGX6UlGRVmIrQVYTnRVghwiwREu6NiyAi3CrCouC6TcRiE4rQX4QXRVggwmIR/iXCtnH1LxBhvghLRXhIhLahvD4iTBJhpQhfijAsytjXrGlNnz4wfny0Zx0/Hvr0gVatyPt6e/Qs520Zwh5bzo9UL9bXAQcMyetncxwn9yyEdagOCl1jQ9mdsEAEYZYCnRM01RM4BTgf6AWUYfFxY5wA9MECxk8C/oXIhhl5iChk06s+6ATQJ0E7ge4LuhR0QIJyZ4F+BdoTdAvQz0HPDvL2BD0dtCtoG9DRoF+G6h4M+gPoANCNQEtAbwnlTwX9M2h70KNBfwLduP6xd1BQ7dBB9YknNCVPPGHloObK53p3c46uo5Xexbn11msuz5ZLJk2alOsh5CU+L4kp9HkBVmiy71bYVWFlXNpFCi8nKPuJwsOh+27Bl8IGSdr+UuHwpH0316gxInTEQibtqMqMIO1xYK4ql8eVfQ94RJWxwf3pwJmq7JWg3a7AIqC7KotE+DswU5Urg/wDgfGqbCZCfyw0VXdVlgf5k4P8+1KPv6OC2ZlusAGMGpW87J13wtL431p5Wq/T0u/5jq0pZi0raU9fvmP1BpslrZcvz9a7N8ycmbxeLil0u8GG4vOSmEKfFxFZqaodk2TG5MIAVL8O0h4D5qF6eVzZx4G1qJ4W3Me++zdC9acEbX8BXIbqSxl6lJRkU5juCrynSvtQ2sXAEFUOjyu7FDhIlQ+C+0HAJNW6S38RhgP3qrJ5cP8JcJMqTwb33YEFQHdgvyBv+1D9u7AF+u8StD0SCPb3O+4eE6agiCR/VpvSRAXyr97LHMZhvAbAaooZxxmcx11J6+XLs4kob731dvKKOaSiooJOnTrlehh5h89LYgp9Xvbff//kwhRA5B9YPNszMG3e14C9E2jzHgA8C+wPlGIB7wehOjg4O90S+C92fPk74FJgO1QXZfiREpOtJTDoYND5cWlngpYkKFsFul3ovl+wxSdx5XqCzgX9TSjtW9BDQvdtgrp9QE8CfT+ujTGgj9Q//g7rtxl79064m7Ge3r211rZkvtYbtMU8XUObWpVW0F4H9SzPmzE2tF4uKfRtu4bi85KYQp8XUm3zqqLQVeEFhRUKsxWOD9IHa3xAejhHYa7CEoWXFbYM0gcoTA/aWKTwpsKglP1m+MqmAlIF0CUurQvYdms9ZbsAFTZnhggbA28A96jWOoROVJegn3TGkJAOHWDMmNRlxoyxcvleb/x2o2lFda20VlTxxLaj82aMDa3nOE4zQXUxqsNR7YhqL1T/HqRPRrVTXNl7Ud0C1Y1QPRzV74P0UlR3DtrohuqBqH6Y5efI2sq0I2glaL9Q2mNh5aBQ+nugZ4buTwuvKAPFomlJ6v4ddEzo/oDYihi0P+hq0M6h/HcIlJtSj7+D9u4dXfHliSds9SSi+Vtv4MDEy76BAyP2VZ21Z+vVy4bWpUt+Kx+pFv5Ko6H4vCSm0OeF+lamBXJltzP0H5hGb0fQfUiuzXs26BeYJm8P0FJqtHm7gP4H9K4kfRwCOh90h0DovkVtbd73Qf8E2g70SCJq87Zt2zb5p6U5s+uuqptuah+Fp55Kq2q2vwT691c95pisdtkgCv3LsaH4vCSm0OelpQjTbDttOBdoD/yI2Qedo0qpCINFqAiVux94GdO8/Qx4NUgDOBLYAzhVhIrQ1QtAldexg+lJwKzgui7U9nHAIEyD7BbgGFUWNMnTNgfKymD4cOjUCUpKcj2alPTtC999l+tROI7j1CWrvnlVWQwMT5A+GTPejd0rpol1aYKyjwKP1tPPn4E/J8mbCQyNPuoC5qef7OrXD/bdt1kI0/ffz/UoHMdx6uLuBFsyZWX2utVWMHQofP45/PhjToeUir59TfYvWZLrkTiO49TGhWlLJl6YArydn7abYMIUfKvXcZz8w4VpSyYsTHfbDTp2zOut3pgwjQ3bcRwnX3Bh2pIpKzN/fhtuCG3a5Pe5aXk5A84dwqbM95Wp4zh5hwvTlkxZma1KY+Tzueno0RS9P4UxbUe7MHUcJ+9wYdqSmTmzrjAFeOedXIwmOfPmwbhxUF3NCZUPs+SL+bkekeM4Ti1cmLZUVOsK0913z69zU1V4+WUYOBAqKwFozToO/19yd4eO4zi5wIVpU1JeDkOGwPw8XEn9+COsXFlbmObLuakqvPCCCfdf/hIWLlyf1UbXctSyh1k3Jw/n1HGcFosL06bkyithyhQYnYcrqbAmb5ihQ6G0NDfnptXV8NxzsOuucOSRsGwZ7L+/CfkQrahi5RV5OKeO47RYXJg2FZ99Bo88YgLi4Yfzb3WaSphCds9Nq6vhmWdMiB59tK2YH30UvvzSPDQEW7wx2lGJTH0ve+NzHMepBxemTcUVV9S8r6rKv9VpTJj26VM7vanPTcNb31VV8OSTsPPO8KtfwZo18PjjplF88slQVATTptm2b7DVu+jaOxCUp66Y1jTjcxzHaQAuTJuC8nJ4/fWa+8rK/FudlpXBppvWDRba1Oemo0fb1vdJJ8FOO8Fxx5lQHT/etpdPPNGEaDwbbggibFS1kNat3QuS4zj5hQvTpmD0aNu6DJNvq9N4G9MwQ4Y0zblpeTk89JDNzcSJNif/+IdtiR9/PLRunbxu69bQtSutliyid28Xpo7j5BcuTJuCqVPrCtPKSngvj875ysrqbvHGaKpz09GjYe1ae19UBAceCMcem1qIhunWDRYt8lBsjuPkHS5Mm4KPP4YuXWq2K5cssXO/aXlyzldVBbNnJ1+ZDhpk27+ZdHpfXm5b3bEfGevWmYJWOlvf3brBwoUuTB3HyTtcmDYFc+eaWcd++9l9vnlmnzPHhFkyYdoU56ajR1ufYdLd+u7eff3KdOFCm2LHcZx8IKvCVISuIjwvwgoRZolwfJJyIsKtIiwKrttEkFD+WBG+EqFahBFxde8ToSJ0rRFheSi/RITVofyvMv6gpaX2evjh9ppvy6hkZjFhhg61s8wFCzLT59SpdYVpulvfoW1eyL/fKI7jtFyyvTK9G6gENgVOAO4VYUCCciOB4cAuwM7AYcBZofxPgHOBj+MrqnK2Kp1iFzABeDqu2HmhMts28pnqku/CdOZMe61PmELmzk2fecZeb7zRtrxjVzpb36FtXsi/aXUcp+WSNWEqQkfgaOAaVSpUmQK8BJyUoPgpwO2qzFFlLnA71KxAVblblTeB1RH7fDQzTxGR0lLYZBPYemvo2jX/vvXLyqBVK+jVK3mZ2LlpprZ6H3rI+hwxouFtdO8Oq1bRd/NVgK9MHcfJH7K5Mu0PVKkyI5T2CSRcmQ4I8uorVx9HAwuA+OXVzSIsFOFdEYY2oN3UlJbCgGC4+agtU1YGPXvWcdNXi0yem8aUjQ45BLbYouHtdOsGwEbVi9hgg/ybVsdxWi4JrOObjE7A0ri0pUDnCGWXAp1EEFU0jT5PAR6Lq3MZ8Dm23Xwc8LIIA1X5Nr6yCCOxLWeKioSSKIJFlX0//ZQfDjqIr0tK2KFTJzqVlvKfXDuPDzHwf/+DjTbif/WMqVevXvR94w3efeEF1m64YZ38ioqKSHPSdepUdp43j8/OOouFjZiH7uXl7Ah8+PrrbLLJsXz4YSUlJZ82uL2mIuq8tDR8XhLj81IgqGpWLtBdQVfGpV0E+nKCsktB9wzd7w66PEG5KaAjkvS3Jeg60L71jOt10N/VN/62bdtqJGbPttPAe+6x+8svVy0qUl23Llr9bLDFFqojRtRf7r337FmeeSZh9qRJk6L1d9RRqhtvrLpmTfQxJuLtt208Eyfq0Uerbrtt45prKiLPSwvD5yUxhT4vwArNkpzJ5ZXNbd4ZQJEI/UJpuwClCcqWBnn1lUvFycB7qtS3GahQoyncaGLKR+Ft3nXrzBwlH1izxoJtp1I+ipGJc9Mff4SXXjL3gcXFDW8H1m/zxjR6y8rq+sZwHMfJBVkTpqqsAJ4DbhChowj7AEcAjyco/hhwoQhbiNADuAh4JJYpQrEI7TAh2EaEdiJ1nuXkcJ2g3oYiHByULxLhBGA/4F+ZeUoSC1PInwO+WbNMizaKMG3TBvbZp3HC9PHH7cfE6ac3vI0YMWEaaPRWVtrvAsdxnFyTbdOYc4H2wI+Yyco5qpSKMFiEilC5+4GXgU+Bz4BXg7QYbwCrgL2BscH7/WKZIvwf0JO6JjFtgBsxpaSFwO+A4aoZtDUtLTUH8rEv/nwTpsmixSQjZm8aCtAdGVUYNw722gt22CH9+vHErUwhf6bVcZwGItIVkecRWYHILEQS+h8Iyu6GyDuIVCDyAyLnh/L6IDIJkZWIfInIsGwMP0Y2FZBQZTFmPxqfPhlTOordK3BpcCVqZ2g9/UwFOiZIXwDskc6Y0yasyQuw5ZbkVZiTKA4bwoTtTY86Kr2+3n8fvvgCHnggvXrJaNPG3DTGCdP99ktdzXGcvCbsf2Ag8Coin6Ba+2hPpDvwOnAB8AxQjC2aYkwApgK/CK5nEOmHaoY8z6TG3QlmElWLxRkWpkVF5FWYk7IyO7vs0SNa+cacm44bZ7FRjz02/brJCBw39OplZqtua+o4zRiR9f4HUK1ANZX/gQuBf6E6HtU1qC5H9Yugnf7AbsB1qK5C9VlsZ/PorDwHLkwzy+zZUFFRW5hCftmalpWZcG8V8U9fXNywc9OKCgv8/etfQ+dE1k8NJPDPW1xsprL5Mq2O4ySmOxQh8mHoGhnK7g9UoRrF/8BewGJE3kPkR0ReRiTmeWYA8B2qy0PlG+qfoEG4MM0k8cpHMfJJmM6cGX2LN8bQofDpp+mdmz71lAnUTCgehQn880J+TavjOIlZCOtQHRS6xoay0/E/0BPzHXA+0Asow7Z2022nSXBhmkliwjRe2SafwpykCgqejIb46R03DrbdFvbeO72+6iPY5gUXpo5TAFQAXeLSugDLE5RdBTyP6n9RXQ38AdgbkQ3SbKdJcGGaSUpLYbPNzB9vmHwJc1JRYYIoXWGa7rnpF19YNJjTTwfJnAkvUGdlOn8+rFyZ2S4cx8kaM7Bt4Cj+B6ZDLW92sfcSlO+LSHgl2hD/BA3GhWkmiVc+ipEvdhzpavLGKC62FWZUYfrQQ6Z4dfLJ6fUThe7dbYW/dm3e/EZxHKeBqK73P4BIR0RS+R94GDgSkYGItAGuAaag+lNw5vo/4DpE2iFyJBZx7NmsPAcuTDNHdXXhClOIfm66di089hgcdpjZ22YatzV1nEKjjv8BVEsRGYxIjf8B1beAKzG/Az8C20CtmNjHAYOAJcAtwDHZMouBLNuZFjSzZ8OKFYmF6UYbwYYb5v5bv7HCFOq3N331VXMhmGnFoxi1hOlmQO6n1XGcRqCa0P8AqrX8DwRp9wL3JmlnJjRBFLCI+Mo0UyTT5I0RcyabS8rKzO4zJpDSYY89oH17ePvt1OXGjYPNN7dwa01B9+72umgR3btDp065n1bHcRwXppkiijDN9RIqpsnbEKWgKPam8+bBa69ZAPCiJtr0CPnnFbHHyfW0Oo7juDDNFKWl5lUoQdxPwL71cx3mpCFmMWGGDoXp09dr09bh0Uft+U47reF91Edomxfy4zeK4ziOC9NMEe+TN55chzlRzYwwhcT2pqqmxTtkCGyzTcP7qI8kwlTTCRnvOI6TYVyYZoLqarOtrE+YQu6WUYsWmZ1pY4Rp7Nw00VbvO+/AN980neJRjA4dbAwhYbpqFfzwQ9N26ziOkwoXpplg5kzzHJDPwrQxmrwxUp2bjhtnEV2OzoJf6TgvSOBbvY7j5BYXppkgmRvBMLEwJ7n61p85014bI0wh8bnp0qXwzDPwm9/YyrGpCZzdgwtTx3HyAxemmeDzz+01lTAtLrbYps15ZQp2Jgq1z00nTLC91qbe4o0RcikYi3HuwtRxnIxg3pXSxoVpJigthS22SK7JGyOXqqdlZSaEGhsOLdG56UMPwU47mQ/fbBDa5m3XzqbebU0dx0kbkVGIHB26HwesQuQrRLZNp6msClMRuorwvAgrRJglUssVVLiciHCrCIuC6zYRJJQ/VoSvRKgWYURc3REiVIlQEbqGhvL7iDBJhJUifCnCsEY/WH2avDFyLUwbuyoFaNu2tp/eTz+F//63aZzaJyO0zQtua+o4ToMZBZjLQZH9gF9jLgr/B9yeTkPZXpneDVQCmwInAPeKJAzeOhJzL7UL5qz4MOCsUP4nmD/Hj5P0M1WVTqGrJJQ3AZgGdAOuAp4RYeMGP1EUTd4Yffua2umKFQ3ursFkSpjC+nPToqVLTfGouBhOPDEzbUehWzdYsgSqqgC3NXUcp8FsAcwM3h8OPI3qU8D1WDDyyGRNmIrQETgauEaVClWmAC8BJyUofgpwuypzVJmL/UIYEctU5W5V3gRWpzmG/sBuwHWqrFLlWeDTYFwNo6zMzgujCtNYnWxSXW0KSLEDxsYS2Jtu9NFH8PjjMHx4w1wUNpRu3eyZfvoJsGmdOxdWp/VpcBzHYRmsX0z9HHgzeL8WaJdOQ5F9volwKPBboC9wsCrfi3AGUBYItvroD1SpMiOU9gkwJEHZAUFeuFwEabWeXUVYCCzGQvncrMq6oI3vVGsFjE3atggjsVUyRUVCSQKTkG7vvstOwMdr1rCsnhBlnZcsYXfg0xdfZFF90VcySPGCBexdWcmMtWuZFzWMWgqkspJ9i4vZ7pZbYO1aPhk0iCUZaDcqm/z4IzsAH7z2Gqu23JI1azZFdXueeuoDevValbVxJKOioiLhZ6Wl4/OSGJ+XnPIG8AAi07AoNP8M0gcAaa16IglTEU4A7gMeBA4EYtpOrYFLIZIw7QQsjUtbCiTSiIkvuxToJIKoUp+vm3eAHYFZ2IQ8CawDbk4xhi0SNaTKWGAsQLt2qkNjHoDCTJ0KwG4nnmh2lqnYcUc491x26tixxptQNpgyBYD+Bx9M/0z1u/HGthzs1IldLrrIzH6yRbAE/dk228D//R9t2sDNN8PGG/8sq9OajJKSEhJ+Vlo4Pi+J8XnJKb8FxgC9sJBti4P03bAjwchE/Qa8FDhTlQswwRTjfWBgxDYqgHhp0wVqrRKTle0CVEQQpKjynSplqlSr8ilwA3BMA8YQjdJSM3mpT5BCjTZttg/4MmUWE6O8vMbl0OrVFnItm4Sc3YPbmjqO00BUl6H6O1SPQPX1UPp1qN6UTlNRhWk/YGqC9ETCKRkzgCIR+oXSdgFKE5QtDfLqKxcFhfWawKVAX5Faq+HGtB1dkxdM2zUX2jIxYdq7d2baGz26RnO3VSu7zyahMGwAm21mJjJuHuM4TlqI7FDLBEbk54g8gcgViLROp6mownQeduYZz37At1EaUGUF8BxwgwgdRdgHOAI704znMeBCEbYQoQdwEfBILFOEYhHaYUKyjQjtROxZRDhUhE2D99sB1wAvBmOYgak8XxfUORLTFn42yjPUoaoKvvwytbOGeHIlTHv0MInTWMrL4eGHYe1au6+stPv58xvfdlTinN3n6jeK4zjNnnHArgCI9MRkRVds+/fGdBqKKkzHAncGAhBgSxFOAW4jWdTzxJwLtAd+xPajz1GlVITBIlSEyt0PvIxp2n4GvBqkxXgDWAXsHYxtFSbYwc50p4uwAngNE+Dh5fpxwCBgCXALcIxqYGeULt99Z9ucUVemUBMkPJuh2DJpFjN6dN2xV1Vld3XaubPFSw0pcbmtqeM4DWB7akwsfwV8gOovMCuT36TTUCQFJFVuE2ED4N+YuvAkYA3wJ1XujtqZKosx+9H49MmYclDsXrFz2kuTtDM0RR8XAxenyJ8JyeunRcyNYLrCdPVqW8n16JGRYdTLzJkweHBm2po61VajYSor4b33MtN+FETqOG7o29c8HKpmz3eE4zjNntaY7wOwhdhrwftvMX8IkYmsgqnKVUB3YE/MmHVjVa5Jp7OCI4qD+3iyrS2zdi18/33mVqbTppnEUqVk0qT175k2LTPtRyXknxdsWpcvTx633HEcJwGfAecgMhgTpjElpC2AtOwXIwlTER4SobMqK1X5UJX/qFIRnH0+lNbQC4nSUosGk46/22wL0++/t23ZTAnTfCHknxdco9dxnAZxGXAmUAJMQPXTIP2XwH/SaSjqyvQU7KwznvbAyel0WFCko8kbo3dv24fMlupprJ9MeT/KFxJs84ILU8dx0kD1HcwDUndUTwvl3A+ck05TKc9MReiKacwKsJFILRvT1sD/A35Ip8OCIabJ+/Ofp1evbVvo2TN73/qZtjHNF+K2eWOP58LUcZy0UK1CZBUiO2KmlN+iOjPdZupTQFoYNK7A54mGAVyXbqcFwbffwpo16a9MIbt2HGVl0Lq1CfBCIrbNG2gcdewIm27qtqaO46SBSBHmHe88oBhbOK5B5G/AVaiujdpUfcJ0/6DxtzBn8ItDeZXALFXmpTH0wiGmfNQQYbrVVvDGG5kdTzLKyuxctyiyG+bmQffusG6daR0F3qfc1tRxnDS5DTOBORuYEqQNxgRsK1JYhsST8htWlbcBRNgK+F6VLBpH5jkxYbr99unX7dsX5s2zaDPtEx1FZ5BM2pjmE2HHDYEw3Wqr7FroOI7T7DkeOA3V10Jp3yKyAPNFH1mYRlJAUmWWKtUi9BBhLxH2C1/pjb1AKC01ZaJOneovG09MW2bmzIwOKSGFLkzjNHpnz65xzuQ4jlMPG5DYi9+3wIbpNBTVNKaHCCXAHOBdTI14UuhqeTREkzdGtlRPV640h/SFLEzjNHqrq02gOo7TTBDpisjziKxAZBYixycpdz0iaxGpCF19Q/katBHLezBC758AoxKkn4+5no1M1IO0vwJVwA7Af4FDMO8QNwAXpNNhQbBuHXz1FRxySMPqZ0uYxla+hShM45zdQ+1p3XrrHIzJcZyGcDemg7MpFoXsVUQ+QTVRAJInUT0xRVu7oPpNGn1fCryGyM+xYC4K/B/QAzg0jXYi25kOAS5T5cugswWqPIcZvGY5ZEge8O235kKvoSvTTTaBDh1cmDaGJNu84EpIjtNsEOmIKbdeg2oFqlOAlzDfuE2P2Zn2B57GXNp2Cd5vG4wlMlGFaXtqXCstBjYJ3n+ORV1pWTRGkxeyF+akUG1MATbc0OYxtDLt0QOKi12YOk4+0R2KEPkwdI0MZfcHqlCdEUr7BEj25Xo4IosRKUUkkVOFdxCZj8hziPSJNEDVeaheherRqB6F6tVAG0SeilQ/IOo275fAdsBMbB/5bBG+x8LUzE2nw4KgMZq8MbIlTNu1MwPMQqN1a+jatZYwbd3aHD25ranj5A8LYR2qg5JkdwKWxqUtBRL5aH0KixL2A/Az4FlEfkJ1QpA/BHgf6ICFT3sFkYGorkvQVn1siK2YIxN1ZXoHsFnw/gbgIOA7LKTalel0WBCUltpqr2PHhrcRE6aqmRtXPGVlJl0KNYxKnH9ecFtTx2lmVGBbq2G6AMvrlFT9PFhFVqH6HiaXjgnlv4NqJao/YQpEW2Eh1rJC1BBs40PvPxahD7ZSna2anmf9gqAxmrwx+vY1bdsff2y6lWOhmsXEiHMpCPa4H3yQo/E4jpMuM7Bt4H6ofh2k7QIkUj6KRzGnQg3NzyiRQ7CFCaLHfAysEOHyDI8pv1m71jR5MyFMoWmXUYUuTOOc3YNN65IldjmOk+eorgCeA25ApCMi+wBHAI/XKStyBCIbISKI7ImZtLwY5A1AZCAirRHpBNyOHUF+kaUnqV+YitBdhP8nwkEitA7S2ojwe+wMNbKHCBG6ivC8CCtEmCVCQnsiEUSEW0VYFFy3idT8whBhrAhfiVAtwoi4uqeI8JEIy0SYE9QtCuWXiLBahIrg+irq+AH45hsTqPkuTH/6ya5CFqZJtnnBz00dpxlxLqbk+iMwATgH1VJEBiNSESp3HPANtgX8GHArqo8GeZsCTwLLsCPIPsBhSX3riryU8rIt5LSoL2rM3sCrmJcIBf4bCK/ngTaYWUw68Uzr2BOJ8IlqnSX9SGA4ttxX4N/YBN0X5H+CTdytCfroAPwe+AALrfMSJvBvCZU5T5UoBr11aUhA8ETEQqI1lTAtZE3eGAm2ecO/UXbbLQdjchwnPVQXY9/38emTMQWl2P1vUrTxFrBtGr0uipCf1k/y+s5MRwP/wjSjTsOE1CuYEtLjqkTWnhEhZk+0oyoVwBSR9fZE8VvFpwC3qzInqHs7FsD1PgBV7g7SV8f3o8q9odu5IozHHPZnhtJSU+hpjCYvmE/eHj1cmDaG7t3Nv/HKlWa3i4dicxwnAqqnZrrJ+oTpLsAQVUpFuBrTkLpClacb0Fd/oEqVeHuiIQnKDgjywuUauq+6H3UPs28W4RbgK+AqVUoSVRRhJLZKpqhIKCkpYYdJk+i8+eZ88J+0grAnZGC3bvDxx/yvJGH3jaLnxIlsA0yZO5d1y5ZlvH2AiooKSppg7FHZfOFCtgWmvvIKazbZZH16ly77MGXKAvbcc0byyk1IruclX/F5SUyhzkvxokXscMMNFGVRCSinqGrSC7QadJPQ/XLQbVLVSdHWYND5cWlngpYkKFsFul3ovp/ZkKjElZsCOiJFn6eCzgHtHkr7GWhn0LagpwTPtHV942/btq2qquoOO6gefrhmhJNPVu3ZMzNtxfPb36pusEHTtB0wadKkJm2/Xp59VhVUp02rlbzHHqoHHZSbIanmwbzkKT4viSnYeTnnHNVWrbQXrNUGyIzmdkXR5t0oUBzqhp1fdgnu118R5XZ0e6K6ZbsAFWluKw/HzkkPDZvvqPKBKstVWaPKo5jj/l9EanTtWpgxo/HKRzH69oW5cy3IeKYpdE1eSOifF+yxfZvXcXJIeTk8/DBUV9MtunOgZk0UYfo5sADTtOqEObpfEFwLg9cozACKROgXSktmT1Qa5NVXLiEiHAI8AByuyqf1FI9ui/T11+bkPpPCVBVmzcpMe2Fmzix8YZrAPy/YtM6cCVVV2R+S47R4Vq+GX/3KXmkpe7z1/2LImOKOKitEzJ5IhDMwbd4jgL0TFH8MuFCE1zBhdxHwt1imCMXYDwEB2ojQDqhUi7l6ADAeOFKVWgebImyIuaF6G1gHHIudqf4+0kM01idvPGHV0/79M9MmmICeORMOPjhzbeYjCcKwgU3runUwZ46FnHUcJwusXg0PPABjxljoxwAXpoAqb2e4v3MxU5ofMdXjcwLlpsHAP1XXq0HfD/SF9avKB4O0GG9Qo7i0N+avcX8szuo1mCnPayEvepNVORQz57kR895UhfkcHq4a0da0tBRatYLttov+xKloKlvTH380DdeWsjJNYR7jwtRxmpjVq+HBB+Hmm2HePNh8cygqsl+0zQGRDtjibhPid2tVn4vaTFb3slVJaE+kSi17ouBs9NLgStTO0BR9JF1Nq7IA2CPygOMpLbVv6vbtG9xELTbbzBzRZ1qYtgSzGIA2baBLl6SOG777DvbPnFGU4zhhVq+GceNMiM6dC4MHwxNPwIUX2plpc0BkGOYooluCXAVzVBSFBrkTbLFkwidvmKYKxdZShCkkdNyw5ZYWQcaVkBwnQ5SXw5AhMH++CdG774ZttoHzzrPvmTffhLfftl+v06YFxhd2fQQrcz38FNyBOSbqiWqruCuyIIUWomWVCQRMAWn48Mw23BSqpzFhGvOyVMgk8M9bVGTbuy5MHSdDjB4NU6bAr39t/1hz58I++8Cjj8IBBzTnyFR9gF+iOq+xDfnKNCJtVTOryRujKUKxlZXBJps0LkRccyHByhRsWt0/r+NkgLlz7Uy0uhomTzbPbRMn2vsDD2zOghTMNDIdN4RJ8ZVpRNrGhF1TCNPly00gxOwmG0tLsDGN0a2bRfGJY6ut4IUXsj8cxykoVOGgg8zGHmzbZ9AgE6KFwX3AnxDpgSm81naMr/px1IYiCVORpM7sFViNefJ/UpVGL5XzlbbV1ZnV5I0R1pbJpDDdc8/MtJXvJNjmBZvWBQvsd0rnzjkYl+M0d1ThzDPh889r0tatg0cegWuvNQXK5s8zwevYBHlNooC0MXAUpom7TXAND9K2xbRuvxJhYNSOmxvtVGHrrU37NpNk2jymqgpmz25ZK9Nly2p+OQd4KDbHaQSqcMklpq3bOk6eVFXZGWphsFWKq286DUXd5n0Xc/F3uqppZonQAfMy9Anmju8xLCBrwaz/w7RVzfwWL2Q+zMmcOfbrsSUJU7DVaeiXcvg3ys4752BcjtNcUYUrr4Tbb7ednzjTMyor4b33cjO2TKOaMfdzUVem5wM3xASpjYGVwBjgAlUqsdiiAzM1sHyjuKmEaceOsOmmmROmM2faa0sRpkn88zZ17HXHKVj+8Ae45RY46yxzABMyc1l/TZuW61FmDpGdEXkMkQ8R+S8ijyKyU7rNRBWmnYDNE6RvRo2zhWUUsEKTQNMIU8isrWlLMouBpF6QNtoINtjAhanjpMWYMSZMTzsN7rmnuWvq1o/IL4GPgS2BfwKvA72AjxE5PJ2mogq/54FxIlyKObpXYE/gNiDmbmlPIDcBJLNFUwrTKVMy01ZZmf0D9OqVmfbynSTO7pvKH4bjFCy33QZXXw0nnQRjx5rCZeFzIzAG1etqpYrcEOS9HLWhqLN1NvAv4AngW+C74P3rmL9dgC+AM6N23CzZNiPmSHXp2xe+/97OIhpLWRn07AnFxY1vqzmQZGUKbmvqOJH561/hssvguOMsdFq80lHh0h94PEH646RpfxpJmKqyUpWzga7ArsBuQFdVzlFlRVDmf6r8L53OmxPVAEuWNE3jffuaQfTs2Y1vqyXZmEJKYbrVVjYd1dVZHpPjNCfuvhsuuACOPhoef7wlCVKwoCu7J0jfHfghQXpS0lrHq7JClemqfBIToi2FVtB06uCZ1JZpacK0QwcLPBCvcYhN65o1zcfntuNknbFjzb/uEUfAhAnmlKFl8QBwPyJXIbI/IkMRuRpz5pDI9jQpUZ02tMM0eg8kQZgaVVqG8cHDD8M112TeWDlTwnTNGguB1JKEKaR0KQg2rVtskeUxOU6+8/DDprH7i1/Ak09aFKaWx42Y2edFQGy1NA+4DrgznYairkzvAS4HZgIvAM/GXS2DpjJW7tHDzjgbK0xnzTK19ZYmTFN4QQJXQnIKkHAUl4bwxBNw+unmKvDZZ6Ft28yOr7mgqqj+BdWeWBzsDVDtieodaHoO06Ou6YcDv1JlYppDLSwqK5tmddqqVWaix7Sk0GthunVLuM3bu7dp9bowdQqOWBSX0aPtzDMq5eUW5eWrryxc2gsvZN6rW3NFdXljqkcVpiuB7xvTUcEQW52m8wGOQiZUT1uyMP2+7sezuNhim7owdQqK//4X7r/fNOvuu89Wp5tsYk6o67vOOgu+/BI23xxeesn0DVoaItOBIaguQeRTzNQzMaqRjzCjbvPeBlwo0riQbSJ0FeF5EVaIMEuE45OUExFuFWFRcN0mgoTyx4rwlQjVIoxIUP8CEeaLsFSEh0RoG8rrI8IkEVaK8KUIw9J6iKZypZUJo8iyMjv36NEjM2NqLiTZ5h0/Hn74wXa0+vSx+yiMH2/lW7VqWL0DDhiS1f6yVc/JA6ZNs+3dmIp6dTX8+9+2Vfu3v5kD+gsugDPOgGOPtfPQwYNh4EDzLT4x2FxcssSiQLRMngXWhN6nuiITdWX6c2AwcIgInxMXpkaVX0Zs526gEtgUcz34qgifqFIaV24ktrW8C/ar4d+Ybet9Qf4nwJOYC8NaiHAwdr57AHaQ/DzwhyANYAIwFfMn/AvgGRH6qbIg1cA/a9vWIsw3FX37wk8/2Yd8o40a1kZZme1ttgxj6xq6dYPFi23XIFDrHz8eRo40nSyw4+Qzz7Tvj6OPTt7Us8/ChRfCqlWNqSdZ7q9x9UaOtPcnnJC8npMH/OtfcNRRNX+8GOvWwfTpdvS0di1UVNgHIf666y545x37P6mubpodtuaA6h9C76/PYLta7wX6cKorYhsdQStB+4fSHge9JUHZ90BHhu5PB30/QbkpoCPi0v4OelPo/kDQ+cH7/qBrQDuH8ieDnl3f+Nu2batNyvPPm9fLDz9seBt77KH6859nbEj1MWnSpKz1lZK//tXmbuHC9Um9eydyKOpXsqt376b/M+XN5yXPiDQv48aptm6t2q2banFx7T9ecbHqueemrj9vnmq7drXrtW+vWl6ekWdIBbBCI8iInFzwlsKGCdK7KLyVTluRVqaqnJoBud0fqFKt5XLwE2BIgrIDgrxwuai+/AYAL8bV3VSEbkHed6osj8tP2LYII7FVMkVFQklJScQhpE/HRYvYAyh9+WUWNHD7Ze8vv0TbtOGj556jsmvXzA4wARUVFU06J1HZ5Mcf2QH44LXXWLXllgDMnj0Eak4GQiijRn2dtK077+zXIuvNnq2UlLydtF4myJfPS76Rcl5U6fPoo/R59FEWDxpE8eLFdIo/0qisZPkbb/BRirnt95e/sPm6dbXO6arXrqX87LP5+ve/b+QTNBKRrsA44CBgIXAFqn9PUO564CpqtmgBdkb1uyB/YNDO9phHvtNR/V89vQ8FErmLa4ftxkYnW78AQAfHVoihtDNBSxKUrQLdLnTfL/gxJXHlEq1MvwU9JHTfJqjbB/Sk+BUu6BjQR+obf5OvTJcts1+Lt9zSsPrLl1t9kfp/pWaIvFlp/POf9uzvvrs+KdnKtL4VmNdrOvLm85JnJJ2XykrVU0+1P9CIEXbfUAYOTPyHHziw4W1GhPpWpjBB4UmFTgr7KixVGJCg3PUKTyRpo1hhlsIFCm0VRgX3xUnK7xZc1QrDQve7KeyhcLXCzJTjjruSHq6JMF2EjYL3nwb3Ca+IcrsC6BKX1gVItAyLL9sFqLA5SLuf2PvlaY4hu3TuDBtv3HAlpMcD95KqZr7TUPuz5kgCl4JjxphzpDAdOlh6KryekxcsXw6HH27/y9deCw891DinCtOmJd7hz3UoNZGOwNHANahWoDoFeAk4Kc2WhmI6QH9FdQ2qd2JbMAckKf8hNUFb3gjuY9cHwBXADekMIJWmSljj6Rkar/E0AygSoV8obReoo3xEkLZLhHKJSFT3B1UWBXl9RejcwLablobamj7+OJx7bk24pKZyLpGvJIhpesIJ5iktZmvau7fd16dkk5l6muX+GlYPzHtclHpOFok5ZJg4ER580EKiNeNQaN2hKIgVGrtGhrL7A1Woxh//JTvWOxyRxYiUInJOKH0AMD3YbowxPUU7WwFbYwJ3z+A+dm0BdEH1oajPCGRvm9eeUf8BOgFTRtoHdCloneU86NmgX4BuAdoDtJSQkhBoMWg70HeDreJ2oK2CvENA54PuALoR6FuElJxA3wf9U1DnSNCfQDeub+xNvs2rqnrccap9+6ZX58477Tdmq1a1f3NmQbkgb7btli61Z/7jH3M9ElXNo3mphz/9yaZtzpzs9Ndc5iXb1JqX0lLVXr1UO3ZUfe21nI0pk5BqmxcGK8yPSztToSRB2R0Ueii0VthboVzhN0HeNQr/iCs/XuH6pH1n+Mq2DcW5QHvMU/8E4BxVSkUYLEJFqNz9WBy5T4HPgFeDtBhvAKuAvTFnxKuA/QBUeR2zi50EzAqu60J1jwMGAUuAW4BjtB6zmKzRt6/ZKqxbV39ZVfvFOmqUGQvGO6huSavTzp3t+RPYmjrJGRZYWL/5Zm7H4QS88w7ss4/Zc739Nhx6aK5HlA2iH72pfo7qPFSrUH0PuAM4Ju124hEpQmRvRI5D5ORaVxpEdXTfFRhDckf38Q+REFUWY/aj8emTgU6hewUuDa5E7Qytp58/A39OkjcTUtfPGX37mhD8/vvUXoyqq80w+847YcQIO/eYObN2maZyLpGPiCR13OAkZ6edbNomToST0/racDJGeTkDzz/fjmlGjbLvgH/+034gtwxmYNvA/VCNqaFHPXpTalTTS4GLEJHQVu/OmG+D5Ihshy3ctgraqsLk4lrsmPOxqA8S1WnDOCyO6VjMEUIURSAnXcKe2ZMJ03XrzEH1Y4/B738Pt9/e8pw0JCKJf14nOa1awYEH2spUtVkfy2WO8nILkP3kk5mPDpWIG25gg+nT4eyzYd994cUXIQtmbXmD6gpEngNuQOQMzJnPEdiuY21EjgDeAX4C9gBGAVcGuSWYIByFyH3AmUH6W/WM4K/AR0G/84PXDYB7gavTeZSo38IHAseqcpMqj6jyaPhKp0MnBfWFOVm92tzbPPaYbeH++c8uSGMkCcPmpGbYMIva9+WXuR5JnhB2IN/UlJXBAw/Y0qp1a1MkbEmCtIY6x3+oliIyGJHw8d9xwDfY1u1jwK2omvxRrcR2PU/GhO1pwPAgPRV7ADeiugKoBopQ/RjbFb09nYeI+k38I9Q603Sagp497ewvkTBdtsz8bL70krkFu/pqX0qE8W3eBhE7N53YsuNBGeXlMG6cHaM0tXnZm2+av9yqKrtv3Rr++Mem6y+fUV2M6nBUO6Lai5jDBtXJqHYKlfsNqt1Q7YTqdoH5S7idaajujmp7VHdDNYrdj2CBXAAWYJq8AHOAbdJ5jKjC9CrgBpGac02nCWjd2s5K4oXpwoW2H/fOO+a1/be/zcnw8hrf5m0QffrYhogLU+Ckk0zXAOz1qqsy38eCBXDKKfYrJuzpLBbesSXZh+cHn1FjSvkf4DJEhmD+3L9Jp6GowvRqzNXTjyJ80UCnDU4U4qPHzJljUR8++8xiD7pBYGJi27zqx/npMmwYlJREUyIvWB59tLZac1WVOUq4+uq6juUbgio88ghsvz1MmAC77VbXCUNL0sDPH8ZQo8R0NbAlZglyEHYmG5mowvQZ4E9YlJZ/0IgwNU49hIXp11+bqvy8eRYx4rDDcju2fKZ7d5MGLTesVIMZNsxOET78MNcjyRFvv21KffHHJiLmGqp/fxO2sS3ZdIkF4j71VNhuO9O+r66uWQXHaEka+PmC6r9QfS54/x2qOwDdgU1RLUmnqXq1eUVoA3QE7lZlVvqjddKib18LJ7bLLjB3rikYTZpkv2Sd5MRcCi5cCF0iWWo5Afvvb3Jj4kTYa69cjybLfPSRue0rKqqJ1xdDFbbZxkIijhhhCn+33QYHHRRNX2HNGrj5Zrs6dIAHHoDTTrP/6ZAbv5KSEoYOHZrRx3IagerihlSrd2WqylrgHBKHqHAyTUyjd/p0096dPNkFaRQS+Od1otG9u+nCtLhz0y+/hEMOMQ3ab75J7Lv266/hgw/MVKaiwsofdFD9Pm1LSmDnnc2xyjHHWF9nnOHa9/mAyCRE3op0pUHUv+wbJHcY7GSS8Kqqqgo22CB3Y2lOJPDP60Rn2DCYOhVWrMj1SLLErFnw85+b0t/EiaZJnwwR+PWv4Ysv4I47TJDuthuceGKNs5SYP93SUtvO3X9/C9T9+usWqX7TTbPyWE4kPsOcPJQCXwK7Y1q8c4KrR5D2RTqNRnXa8CZwkwg7Ywautf7lVHkunU6dFDz3nP2DV1XZucro0XB3aiceDrW3eZ20GTbMLDOmTIGDD871aJqYH380Qbp8uZ2XbhPRAqK42LwUnXIK3Hor/OUv8PTT8Lvf2dHM5Mmw++72v3vFFaa8FB+ix8k9qr9b/17kL8CjwPkhz0kg8lfS3I2NKkzvCl4TaTcp0DqdTp0klJebxl9M0SGmLn/NNdnxxtKc8W3eRrHvviYrJk4scGH600/2gHPmwL//bboJ6bLBBnDTTeYC8LrrzAtZjLVrTSvYz0CbCycD/1dLkBr3AO8D50dtKNI2ryqtUlwuSDPF6NG2Gg3j6vLR2HBDO49yYdogOnSAvfcu8HPTlStN2ai0FJ5/3jTlG0PPnubk4de/rjkLLSqy1arTXBBgpwTpidJS4qfh+cTUqa4u31BatzatS9/mbTDDhsH//legU1hZaYpA775rjk8ytfwuLzevZLEfwe58obnxEPAgIpcjMjS4LgceAB5Op6Go27yxyDGHAL2A4nCeanoRyZ0k5DrqfXPH/fM2imHD7JjvrbdssVUwVFVZWJx//tMioWfy4VLtJrmuQ3PgUsxd7vnATUFaORaeMy3fvFFDsO2FxRRdA2wMzAU2D+5nggtTJw9wYdoodt/dlMknTiwgYaoK551npi233gpnnll/nXTw3aTmjWo1Fv/6NkS6BGnLGtJU1G3ePwLjMfXh1ZiZTC/gQ8wrkuPknu7dC3SPMgUxk4wMbCsWFZlFR0Gcm8bm5fzz4b774LLL4NKE4ZEbx7Rpie1TfZep+aG6rKGCFKIL052Bu4Kg3VVAW1V+AC4Dro/amQhdRXhehBUizBLh+CTlRIRbRVgUXLeJ1KgpizBQhI9EWBm8Dgzl/VOEitBVKcKnofyZIqwK5b8RdfxOntMSV6YZDhk2bJhFBksWBbDZMHq0mar87W8wcqR5IXIcAJHpiGwUvP80uE98pUHUM9PwPsYPQG/MoLUCM3CNyt1BW5tiQVhfFeET1TpR1Udisel2wUxv/g18B9wnQjHwIhbU9R7gLOBFEfqpUqnKoeGGRCihboDYw1UphN/fTpiWJkzLy80ZeyxkWAZMqGIh2d58s8YZV7OjvBwefNBWiK1bw7XXerhCJ8yz2BElmN/5jBBVmH6MBVGdgUU0v1GETYETIVrUGBE6AkcDO6pSAUwR4SXgJODyuOKnALerMieoezsWOf0+YGgw7r8GK+U7RbgY23p+Pa7PPsBg4NSIz+k0Z7p3twgfK1e2DGP53/++xp/smjVm83j//Y1qctttoUcP2+rN9PFiVli1yn4RrF1r961bm02oKwM5MVT/kPB9I4kqTK8COgfvr8ainP8NE65RBVV/oEqVGaG0T4AhCcoOCPLC5QaE8qYHgjTG9CC9ljDFDHInq1IWlz5ehFbANOAS1Vp9rUeEkdgqmaIioaSkJMmjtUwqKiryak42X7iQbYGpr7zCmk02ydk4sjEvHb/9lkFPPVVz9lFdjT7wAB8OGsSKfv0a1faOO27H669346233s2oK9mmnpcOM2cy4Npr6fj99zWJlZVUjRvHBwceSGXXrk3Wd2PIt/8jp4GoalYu0MGg8+PSzgQtSVC2CnS70H2/4GRfQK8B/Udc+fGg1ydo5xvQEXFp+4C2B+0AegXofNAN6xt/27Zt1anNpEmTcj2E2jz7rKl/TJuW02E0+bysXKm6ySaJ1F5U27dXfeedRjX/2GNNM41NNi/V1arjxtmzt2unWlRUe06Ki1XPPbdp+s4Aefd/lGGAFZolORPpgk8Vpke60mg3sp0pgAiDgK2BV1RZEWzdrlElSljhCiA+NlYXIFEAyviyXYAKVVQkWjsi7AtsRtyeuCrvhm5vFuEUbCv45QjP4OQzLcHZfXW1+Yb98cfk+QccYA7ZzzmnQWeFBx5orxMnWjSZvGbZMjj7bAu4fcAB8MMP5uEojJuqOLXJ2DlpmKh2ppsCL2Hnpgr0wxSC/oyZykTxXzgDKAoUhb4O0naBOspHBGm7AP9JUK4UuEgECW317owpN4U5BXguOJ9NheLh5QqDluDs/pprzF3dn/4EF11UN/+nnyyayW9/a9G+77kH2rVLq4sePWD77U2YXnxxZobdJHz4IRx3nEVuufFGuPxyOyN1nFRk8Jw0TNQTkb8A84FuwMpQ+tPAQVEaUGUF8BxwgwgdRdgHOAJ4PEHxx4ALRdhChB7ARcAjQV4JZp4zSoS2IpwXpK/X2BWhPfCrUJ1Yei8R9hGhWIR2IlyCRVUPr1ad5kqhO7t/5BFTphk5Ei68MHGZDTc093bXXGMavvvtZ07d02TYMHjnnbrxsvOC6moL1L333rbqLCmBq65yQerklKjC9EDgKlWWxKV/izlviMq5QHvMfdME4BxVSkUYHGzfxrgf23b9FIs992qQhiqVmNnMycBPwGnA8CA9xnBgKTAprv/OwL3AEsyL0yHAoaoU6LdvC6OQhWlJiQnRYcPgrrtSb9+2agU33GDO3L/4wlwbTZ6cVnfDhpli7PvvN27YGWfBAnNWf9FF8P/+nzkT3nffXI/Kac6InIrIG4h8ich3ta40iCpM21Pb1jTGxtg2byRUWazKcFU6qtJLlb8H6ZNV6RQqp6pcqkrX4Lo0tKWLKtNU2V2V9qrspsq0uH4mqNI7XCdIL1Vl56D/bqocqMqHUcfv5Dlt2pg/vELb5p0xA446yuJuPv20PWcUhg+H//zHVqsHHGDmIRofaSoxQ4bUxM3OG0pK7BB34kRzxvDcc5CnGrpOM0HkEswH70dAH+AFbAHXFXOCH5mowvQdYEToXkVojXlAejOdDh2nSSk0xw2LFtkKrHVreOUVE4zpsP32JlAPOcR81J52GqxeXa8bwg02gD32yANhWl5uW9UXXWQ/CDp1gg8+sGdxRwxO4zkTGInqFcBa4C5Uf4kJ2N7pNBRVmF4KnCnCv4G2QUefA/sAV6TToeM0Kd27F44wXbMGjjwSvv8eXnyx4S6JNtjA6l97rZ27Dh5sfmrrcUM4bJjJ4aVLG9ZtRrjsMtui/vOfLfLLRx81AxVjpxnRkxpF11XUWIpMwJwMRSZqcPDPsWCp7wFvAO0w5aNdVfk2nQ4dp0np1q0wtnlV7Yx08mRTJNp778a116oV/OEPNeeoTzxR44Ywyep02DAr8vbbjeu6QSxbZkpWjwf6iW3awC232MrUcTLHfEwJFWAW8H/B+22AaGciAZH9m6gyX5XrVDlMlV+ocjVQLMJT6XToOE1KoWzz3nwzPPaYCcDf/CZz7Q4fbgo8MWKxNxOw117mlTGrW72rV8Ptt9sq/C9/Yb0LJpGMOfN3nBBvAb8M3o8D/ozIJOBJzPokMo11FrYhaS6FHadJKYRt3qeeMlOPE080E5dMUl4OL7xQc19ZmXR12rat7QhnRZiuW2fO6fv1M+PWHXe0AcQCb6cYp+OkjUjgmoSRwI0AqN6H6QZ9irnQPTedJjPoedNx8oBu3WyLMD5gc3Ph/fftbHDffU24ZFrJZvToGgEVI8XqdNgw2xWeOzezw1hPdbX9eBgwwDzr9+wJkybBDjvU1TxOMU6nGSPSFZHnEVmByCxEEobmDJUvDsxY5sSla9BGRXA9mKKVfwemL1cANY68VZ9EdRSqd6G6Np3HcGHqFBYxW9PFi3M7jnQpL4ef/QwOOwy22MLONtu2zXw/U6fW/aGRwt1eOCRbRlGF11+HQYPg2GPtTPSFF2wcQ4emPU6nWRMOzXkCcC8iA1KUvwTzVZCIXVDtFFxnpGhjALaN+ztgFiKvIjIckQZ7/nBh6hQWzdU/79VXm+rs8uXw6qs1z5Fppk2rcQG/bBl07gzHH2/pCdh5ZxtKo4VpeTkDzz/ftmmnToX994dDD4UlS+xs+JNP4Igjalbi4XGGryTjdJopIrHQnNegWoHqFFgfmjNR+a2w0J+Ni/au+gWqF2PavMdiykZPA3MRuRWRbdNtMqVv3iDeaCriHc47Tm5pjv55y8vNZCVGurakDaVzZ7M7vftu+OMfzSlvHK1amXnnxIkmyxq86zx6NBt8+qlpNc2aBZtuap6czjwTiosb9xxOXtMdihAJO8cZi+rY4H1/oArVKKE5wUJ/XomZsSTiHURaYZYnF6I6M+XgVNdhK9TnEOmBnZmeClyMyLuo7peyfoj6VqaL6rnKMD+6jpMfNEeXgqedVvscM5vnguedZ2eRKYKKDxsG8+bBl182sI/ycnjgAUTVBOkVV8C335ozfhekBc9CWIfqoNA1NpTdCXP9GmYpNfGzaxA5EihC9fkkXQ3BvBhtB8wDXkEkemQ01XnAPcCdmKvafSLXpZ6VqWrkwN+Okx80t23eGTPs7DBGTGv1mmtgs82avv9ttjEPS/fdB1demfCcNnZuOnGiOVRKm1GjTFsXTHguXQodOzZ8zE4hES00p20H3wb8ImlLqu8E7yoROR9YBmyPaeemRmQYgZ93zEXuBCCVAlMd/MzUKSya2zbv0Qksy7KttTpqlMVHffLJhNlbbWVXg85N580zH7ox3MTFqc0MbBu4XygtUWjOftiqczIi87Gt2c0RmY9InyRtpw6vKdILkesQKcOcEfXATGV6oPpbVNM6oHdh6hQW7dvb1RxWpu+9B599Vjc921qrw4bZkvOOO5I6wh82zCxWYgvMyJx1VlqmOE4LQ3V9aE5EOiKSLDTnZ8CWwMDgOgP4IXj/PSIDEBmISGtEOmEub+cCXyTsV+TfWEzus4B/AP1RHYrqE6hGDt4SxoWpU3g0B8cNa9bAGWdAr16mwZtLrVURW51+/HFKE5llyywed2SqqxN7fHATF6c2dUJzolqKyGBELDSn6jpU56+/YDFQHdxXYWY1T2Jbu99hq9jDUtiKrgKOArZE9QpUv2nsQ7gwdQqP5uCf95ZbzBvCffflh7/Zk04yLeI770yYfcAB9pqWN6S//93cA44fD6qUTJrkJi5OXVQXozoc1Y6o9kL170H6ZFQT/3OolqDaM3T/FqrbBm1sErT3dYo+f4nqS4EgzgguTJ3CI9/9837+OYwZY/adhx6a69EYHTvaSvnZZy1KTRzdu1uwlsjnpmvWmO3sbrvBccdldKiOk49kVZiK0FWE50VYIcIsERK6jRJBRLhVhEXBdZtIzUGyCANF+EiElcHrwFDe9SKsFaEidPWNUtcpEPJ5m7e62mwrO3c2R+75xG9/a6vGe+9NmN2jh8XnbtUK+vSxBWdS7rnHzGBuvZXxE1rRpw8ccMCQ+uuFGD/e+onUXw7rOQ4Aqpq1C3QC6JOgnUD3BV0KOiBBubNAvwLtCboF6OegZwd5xaCzQC8AbQs6KrgvDvKvB30iSf8p66a62rZtq05tJk2alOshJObcc1W7ds1Z9ynn5Z57bKPz0UezNp60OPJI1W7dVFeurJX8xBOqbdvWPtjt0MHS67Bkic3/z3+uTzxh5SLVi+uvOdTLBHn7f5QhgBWaRTmTqyu6QWsjESHmNmpHVSqAKYGHpZOAy+OKnwLcrsqcoO7tWET0+4ChmH3sX1VR4E4RLgYOAF4nNY2p6zQXunUzN3VVVdC6wa42M8+cORbs+uc/tzPKfGTUKPML/Pe/w+mnr0++6irbuQ2zciWcf75Nc5hdn76NnRYv5pXBt3L++VYuSr0wF16YH/WuugpOOCF5PceJkTVhSuA2SpUobqMGBHnhcgNCedMDYRhjepAeE4iHi7AYKAfuUuXeNOquR4SRmN0RRUVCSUlJfc/YoqioqMjLOdli8WL6qTLllVdYt8EGWe8/4byosuPVV7NRZSX/HTGC1TmJuB0BVQb17Qs33cSHffuu9x84e/YQEpnsLVoEp5xSc9+DuXzNX3mCEzjp2l2TdhNfLyrZrjd7tlJS0rR/q3z9P3LSI5vCNLrbqLpllwKdgnPT+tp5ChiL2SD9DHhWhJ9UmZDmGFBlbNAW7dqpDh06NNmztUhKSkrIyzmZOxfuvpt9t9sOtk3bX3WjSTgvzzxj5iB//CN7HZ86wlTOufJKOOMMhopYBBfMgmfWrLpFe/SAyZNr7rtfcR3tX6hivzdG8+2WFg913rz668WTL/V69ZIm/4zn7f+RkxbZVECK5jYqcdkuQEWwokzZjiqfqzJPlSpV3gPuAI5pwBic5kq++eddsgR+9zvTbP3973M9mvo5/nibw5CZzJgx0KFD7WIdOsBtt0HfvsG1+nO6PPMwcu659BqyFX37Wn699RJc+VJvzJhGzqXTYsimMJ0BFIlQn9sogrRdkpQrBXYOa/cCOydpB2q7lEq3rtMcyTdheumlsGCBBfsuyuZmUANp3x5GjoQXX4SZMwE7Nxw7Fnr3tp3f3r3tvtZ54hVXmM3sVVetT6pdTxPXS0Ck/pqoXoxzzvHzUicNsqntBPqPQKO3I+g+KbR5zwb9ItDk7QFamkCb9/xAI/e8OG3eI0A3AhXQPUHngp4SpW6qy7V565K3WojffWfqmA89lJPua83LpEk2lksvzclYGszs2aqtW6tefHG08pMn23OOGZO0SN5+XuKorFTdZhvVnXZSrapq+v6ay7w0FFqINm+2nTbUcRulSqkIg0WoCJW7H3gZ8/b/GfBqkIYqlZhn/5OxMDmnAcODdIDjgG+wrdvHgFtVeTRiXacQyJeV6apVtsLr2xeuuy63Y0mXLbc0J/wPPggrVqQuq2qr7x49msc2dj20aWOugz/9FCZMyPVonOZCVvecVFmMCbP49MmYclDsXoFLgytRO9OA3ZPk/aaeMSSt6xQInTvbdmquhemNN8LXX5sPvvgDuebAqFHw1FPw+ONw9tnJy73wAkydCg880DyfMwG//jXceitcey386lcedtWpH3cn6BQeIuYFKZf+eadPN62WESPgwANzN47GsPfesPvupoikiaPJsG6dnZVut509a4HQqhXcdBN8950tzh2nPlyYOoVJrvzzlpczcNQoM2rs2hX+9KfsjyFTxKLJfPFFcg/3Dz0EX31ljvubg3JVGhxyiJnM3HBD/TvdjuPC1ClMciVMR49mg08/hf/9z+KDxs5vmyvHHgubbJI4msyKFXYWvM8+8MtfZn9sTYwI3Hwz/PBD0mA6jrMeF6ZOYZKLbd7ycnjoIbO7atUKhiRy7tXMaNvWzktffRW+iQv5+Ne/wvz5tp0tdb0jFQL77AOHHWbnp4sX53o0Tj7jwtQpTHKxMr3hBgt8DbbleeON2e2/qTj7bHueu+6qSVuwwCTM8OF2tlrAjBljgdFvuy3XI3HyGRemTmESE6bJFGcyTXm5aarE+qushIcftpVbc2fzzU299aGHYHngLOzGG22b9+abczu2LLDzzuYU6o47ErscdBxwYeoUKt27m6bpsmXZ6W/UKOsvTFWVGSwWAqNGmSB95BFTcb33Xosqs912uR5ZVrjhBvvzFsqf08k8LkydwiSbjhtWrICXX66bXllpzu0LgT33hL32soDme+1loe2uvz7Xo8oaffua/40HH6x7dOw44MLUKVSyKUwvusgE51tvgSolkybVxJieNq3p+88Wo0ZBWZmdlw4YYB6PWhBXX23OG669NtcjcfIRF6ZOYdK9u702tTB98UW4/3645BLYf/+m7SvXhBWNSksL4zw4DTbf3IKMT5hglk+OE8aFqVOYxFamTWkeU15u54a77dYyDtNuvdUc1wJUV7eMZ47j0ktho41qBcZxHMCFqVOoNPU2b3W1uc9buRLGjy98563l5aadvHat3ReStnIabLghXHYZvPZa6mDjTsvDhalTmGy4oTlOaCphescd8MYbppDTEjRaR4+2HxBhCklbOQ1+9zvb8r3iiuxZXjn5jwtTpzBp3dr245pim/eTT+Dyy82F3siRmW8/H5k6tcYhRYxC0lZOgw4dTAnp3XfNMZTjgAtTp5BpCi9Iq1aZBX/XrmYnUaBu9OowbVqNhnL4KiRt5TQ4/XTYemu48sq6C3anZeLC1ClcunfPvDC99FL4/HN49FHYeOPMtu00GzyAuBOPC1OncOnWLbPbvK+9Zv5pL7gADjooc+06zZJjj4VddrEt3/gdcCcNRLoi8jwiKxCZhcjx9ZQvRuRLRObEpQ9E5CNEVgavA5tw1HXIqjAVoasIz4uwQoRZIiScNBFEhFtFWBRct4kgofyBInwkwsrgdWAo7xIRPhNhuQhlIlwS1/ZMEVaJUBFcbzTZAzu5JZPbvD/8AKeeao5ab7opM206zZpwAPHNNrP7Pn1MuTsK48db+QMOGNKgeg3tL1v10uBuoBLYFDgBuBeRASnKXwL8WCtFpBh4EXgC2Ah4FHgxSM8Oqpq1C3QC6JOgnUD3BV0KOiBBubNAvwLtCboF6OegZwd5xaCzQC8AbQs6KrgvDvIvBd0NtAh02yDvuFDbM0GHpTv2tm3bqlObSZMm5XoIqbn4YtX27RvfTnW16qGHqrZrp/rZZ/UWz/t5yRGFOC9PPKHaqlXtg+QOHVQfe0x13brk12OPWbnmWu+JJ6LPEbBCk323QkeFSoX+obTHFW5JUn4rhS8UDlWYE0o/SGGugoTSZisckrTvTMu3rHWEdgStBO0fSnsctM6kgb4HOjJ0fzro+8H7g0DngkoofzZowkkDvRP0b6F7F6YZIu+/HG++2T7iK1Y0rp2//c3a+dvfIhXP+3nJEYU4L7171xY0LeXq3Tv6HNUjTHdVWBWXdrHCy0nKv6JwpMLQOGF6gcI/E5S9KGnfGb6KsrYEhv5AlSozQmmfAEMSlB0Q5IXLDQjlTbe5Ws/0IP31cCPB1vBg4P649seL0AqYBlyiWquvcP2RwEiAoiKhpKQk6cO1RCoqKvJ6TjZfuJBtgamvvMKaTTZpUBsdysoYdOGFLPnZz/h0wACI8Lz5Pi+5ohDnZfbsIUAijW7l1FNnJq338MN9mnW92bOVkpK3k9YL0x2KEPkwlDQW1bHB+07A0rgqS4HOdRoSORIoQvV5RIbG5UZvp6nIltQGHQw6Py7tTNCSBGWrQLcL3fcLfhEJ6DWg/4grPx70+gTt/AH0E9C2obR9QNuDdgC9AnQ+6Ib1jd9XpnXJ+5XGs8/az+hp09KvO2+e6r77qm6/veomm6jOnx+5at7PS44oxHlJtjKtb+XW3Ov16pW6XhjqX5mujEu7qM7K1LaDv1boF9wnWpm+Flfn5WyuTLOpgFQBdIlL6wIsj1C2C1ARrEYjtSPCecDJwP9TZU0sXZV3VVmlykpVbgZ+wlavTqERc3bfEI3e0aNhyhT44gtzm7fpppkdm1MQjBljThzCdOhg6YVaD2D77TNmXzsDW7n2C6XtApTGlesH9AEmIzIfeA7YHJH5iPQJyu+M1DL83jlBO01GNoXpDKBIhPomjSBtlyTlSoGdw9q9xE2aCKcBlwMHqlJbfbouSuL9D6e501D/vPPmmUMGME9Ku+2W2XE5BcMJJ8DYsdC7t/nv6N3b7k84IZ162sB6De2v4fV69YKDD4Z//QvOPNM8SjYK1RWYYLwBkY6I7AMcATweV/IzYEtgYHCdAfwQvP8eKAGqgFGItEXkvKDeW40cYXSytQS21b7+A9Po7RhstybT5j0b9AtMk7cHaCl1tXnPx7R5z6O2Nu8Jwdbt9gna7RX0WwzaDvQS0AWg3eobu2/z1iXvt+3Ky21P6u67o9cpK1Pdcsua/aziYtVzz02r27yflxzh85KY5jYv1dWq115r/x6/+pXqmjWpy5Nqm9e2G7sqvKCwQk0D9/ggfbBCRZI6tbd5a7aMP1JYpfCxwq4p+23G27wA5wLtMRuhCcA5qpSKMFiEilC5+4GXgU+xXySvBmmoUgkMx7ZwfwJOA4YH6QA3At2A/4ZsSe8L8joD9wJLgLnAIcChqmQhgrSTddIJw7ZuHfz5z7DDDvD99zXpLTQ6iuMkQwT+8Af44x/h6adh+HDzstlgVBejOhzVjqj2QvXvQfpkVDslqVOCas+4tGmo7o5qe1R3Q3VaI0aVNtnU5kWVxZggjE+fjGljxe4VuDS4ErUzDdg9Sd5WKfq3fXWnZdCmDXTpUv8277Rptmf10Ue2j1VeXhNqDGqio9x9d9OO13GaERdfbP9eZ58Nhx4KL71k9y0VdyfoFDapvCCtWAGXXAJ77AFz5sCTT5oD+7AghRYbHcVx6mPkSPOI9O67MGxY00U8bA64MHUKm+7dE2/z/utfsNNO8Kc/mZvAL76AX//ao6M4Tpr85jfw3HMwfToMHWobOy0RF6ZOYRO/Ml2wAE48EQ45xLaBS0rggQcs9qnjOA3i8MMtDkRZGey3H8yalesRZR8Xpk5h06GDxckqL7ewadttB089BddcY0G+hyRywOU4TroccABMnGgbQbvuCltsYc7xoX37XI8tG7gwdQqbr7+GNWvsXHTECBOm06bBDTdAu3a5Hp3jFBR77WUhf5csMXNtVTD938LHhalTuJSX21kowNy5cPPNMHkyDEgV3clxnMZwf7wn9BaCC1OncBk9uuZHcXGx2Y+28o+84zQls2fnegS5wb9ZnMKkvNycLcTMXNz5guNkhV69cj2C3ODC1ClMRo+u64k75nzBcZwmI5lz/ELHhalTmEydaqvRMO58wXGanHjn+DE1pELHhalTmLjzBcfJGSecADNnxjaHGuW5t9ngwtRxHMdxGokLU8dxHMdpJC5MHcdxHKeRuDB1HMdxnEbiwtRxHMdxGklWg4M3Z9asWaMi0iK00tKgCFiX60HkIT4vifF5SUwhz8vKXA8gW7gwjc7Hqjoo14PIJ0TkQ5+Tuvi8JMbnJTE+L4WBb/M6juM4TiNxYeo4juM4jcSFaXTG5noAeYjPSWJ8XhLj85IYn5cCQLRluE10HMdxnCbDV6aO4ziO00hcmDqO4zhOI3Fh6jiO4ziNxIVpPYhIVxF5XkRWiMgsETk+12PKB0SkRERWi0hFcH2V6zFlGxE5T0Q+FJE1IvJIXN6BIvKliKwUkUki0jtHw8w6yeZFRPqIiIY+MxUick0Oh5o1RKStiIwLvkOWi8g0ETk0lN9iPy+FggvT+rkbqAQ2BU4A7hWRAbkdUt5wnqp2Cq5tcz2YHDAPuBF4KJwoIt2B54BrgK7Ah8CTWR9d7kg4LyE2DH1uRmdxXLmkCPgeGAJsgH02ngp+YLT0z0tB4B6QUiAiHYGjgR1VtQKYIiIvAScBl+d0cE7OUdXnAERkENAzlHUUUKqqTwf51wMLRWQ7Vf0y6wPNMinmpcWiqiuA60NJr4hIGbA70I0W/HkpFHxlmpr+QJWqzgilfQL4ytS4WUQWisi7IjI014PJIwZgnxNg/Rfpt/jnJsYsEZkjIg8Hq7IWh4hsin2/lOKfl4LAhWlqOgFL49KWAp1zMJZ84zKgL7AFZnT+sohsndsh5Q3+uUnMQmAPoDe2IusMjM/piHKAiLTBnvvRYOXpn5cCwIVpaiqALnFpXYDlORhLXqGqH6jqclVdo6qPAu8Cv8j1uPIE/9wkQFUrVPVDVV2nqj8A5wEHiUj8XBUsItIKeBzTwzgvSPbPSwHgwjQ1M4AiEekXStsF25pxaqOA5HoQeUIp9jkB1p+9b41/buKJuV9rEZ8bERFgHKbMeLSqrg2y/PNSALgwTUFwdvEccIOIdBSRfYAjsF+WLRYR2VBEDhaRdiJSJCInAPsB/8r12LJJ8OztgNZA69h8AM8DO4rI0UH+tcD0lqJMkmxeRORnIrKtiLQSkW7AnUCJqsZvcRYq9wLbA4erajg2cov+vBQKLkzr51ygPfAjMAE4R1Vb+i/GNpjpwwLsHOx3wHBVbWm2plcDqzDN7hOD91er6gJMC3wMsAT4GXBcrgaZAxLOC3bG/jq2ffkZsAb4TY7GmFUCu9GzgIHA/JCd7Qn+eSkM3NG94ziO4zQSX5k6juM4TiNxYeo4juM4jcSFqeM4juM0EhemjuM4jtNIXJg6juM4TiNxYeo4juM4jcSFqeO0QIK4osfkehyOUyi4MHWcLCMijwTCLP56P9djcxynYXg8U8fJDROxuLhhKnMxEMdxGo+vTB0nN6xR1flx12JYvwV7noi8KiIrRWSWiJwYriwiO4nIRBFZJSKLg9XuBnFlThGRT0VkjYj8ICKPxI2hq4g8LSIrROS7+D4cx4mOC1PHyU/+ALyE+XIdCzwmIoMARKQD5uO2AtgTOBLYG3goVllEzgLuBx4GdsbC48X7lL4WeBGLWPIk8FDgQ9ZxnDRx37yOk2WCFeKJwOq4rLtV9TIRUeBBVT0zVGciMF9VTxSRM4E/AT1VdXmQPxSYBPRT1W9EZA7whKpenmQMCtyiqlcE90XAMmCkqj6Ruad1nJaBn5k6Tm54BxgZl/ZT6P3UuLypwP8L3m+PhegKB49+D6gGdhCRZcAWwJv1jGF67I2qrhORBcAmkUbvOE4tXJg6Tm5YqarfNLCuUBNYO550grSvjbtX/OjHcRqE/+M4Tn6yV4L7L4L3nwO7iEjnUP7e2P/zF6r6AzAXOLDJR+k4DuArU8fJFW1FZLO4tKogUDTAUSLyX6AEOAYTjD8L8sZjCkqPici1wEaYstFzodXuGOAvIvID8CrQAThQVW9vqgdynJaMC1PHyQ3DgPK4tLlAz+D99cDRwJ3AAuBUVf0vgKquFJGDgb8C/8EUmV4Ezo81pKr3ikglcBFwK7AYeK2JnsVxWjyuzes4eUagafsrVX0m12NxHCcafmbqOI7jOI3EhanjOI7jNBLf5nUcx3GcRuIrU8dxHMdpJC5MHcdxHKeRuDB1HMdxnEbiwtRxHMdxGokLU8dxHMdpJP8f01vMgwL0+SMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\", color='b')\n",
    "plt.tick_params('y', colors='b')\n",
    "plt.gca().set_xlim(0, n_epochs - 1)\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\n",
    "ax2.set_ylabel('Validation Loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras schedulers   \n",
    "\n",
    "Lastly, tf.keras offers an alternative way to implement learning rate scheduling: define the learning rate using one of the schedules available in `keras.optimizers.schedules`, then pass this learning rate to any optimizer. This approach updates the learning rate at each step rather than at each epoch. For example, here is how to implement the same exponential schedule as the `exponential_decay_fn()` function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5995 - accuracy: 0.7924 - val_loss: 0.4094 - val_accuracy: 0.8598\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3889 - accuracy: 0.8613 - val_loss: 0.3739 - val_accuracy: 0.8694\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3530 - accuracy: 0.8775 - val_loss: 0.3728 - val_accuracy: 0.8694\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3296 - accuracy: 0.8814 - val_loss: 0.3492 - val_accuracy: 0.8810\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3177 - accuracy: 0.8867 - val_loss: 0.3429 - val_accuracy: 0.8796\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2929 - accuracy: 0.8957 - val_loss: 0.3411 - val_accuracy: 0.8826\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2853 - accuracy: 0.8989 - val_loss: 0.3352 - val_accuracy: 0.8798\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2711 - accuracy: 0.9043 - val_loss: 0.3364 - val_accuracy: 0.8810\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2713 - accuracy: 0.9047 - val_loss: 0.3261 - val_accuracy: 0.8854\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2569 - accuracy: 0.9086 - val_loss: 0.3235 - val_accuracy: 0.8854\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2500 - accuracy: 0.9111 - val_loss: 0.3246 - val_accuracy: 0.8866\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2452 - accuracy: 0.9145 - val_loss: 0.3297 - val_accuracy: 0.8816\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2407 - accuracy: 0.9153 - val_loss: 0.3216 - val_accuracy: 0.8868\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2377 - accuracy: 0.9163 - val_loss: 0.3218 - val_accuracy: 0.8860\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2375 - accuracy: 0.9167 - val_loss: 0.3205 - val_accuracy: 0.8884\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2315 - accuracy: 0.9192 - val_loss: 0.3180 - val_accuracy: 0.8894\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2264 - accuracy: 0.9213 - val_loss: 0.3195 - val_accuracy: 0.8898\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2284 - accuracy: 0.9192 - val_loss: 0.3165 - val_accuracy: 0.8904\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2284 - accuracy: 0.9211 - val_loss: 0.3194 - val_accuracy: 0.8890\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2285 - accuracy: 0.9217 - val_loss: 0.3166 - val_accuracy: 0.8910\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2264 - accuracy: 0.9214 - val_loss: 0.3176 - val_accuracy: 0.8910\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2255 - accuracy: 0.9205 - val_loss: 0.3160 - val_accuracy: 0.8912\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2221 - accuracy: 0.9231 - val_loss: 0.3167 - val_accuracy: 0.8896\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2180 - accuracy: 0.9246 - val_loss: 0.3163 - val_accuracy: 0.8912\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2221 - accuracy: 0.9224 - val_loss: 0.3162 - val_accuracy: 0.8912\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For piecewise constant scheduling, try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],\n",
    "    values=[0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Cycle scheduling  \n",
    "\n",
    "Contrary to the other approaches, 1cycle (introduced in a 2018 paper by Leslie Smith) starts by increasing the initial learning rate $\\eta_0$, growing linearly up to $\\eta_1$ halfway through training. Then it decreases the learning rate linearly down to $\\eta_0$ again during the second half of training, finishing the last few epochs by dropping the rate down by several orders of magnitude (still linearly). The maximum learning rate $\\eta_1$ is chosen using the same approach we used to find the optimal learning rate, and the initial learning rate $\\eta_0$ is chosen to be roughly 10 times lower. When using a momentum, we start with a high momentum first (e.g., 0.95), then drop it down to a lower momentum during the first half of training (e.g., down to 0.85, linearly), and then bring it back up to the maximum value (e.g., 0.95) during the second half of training, finishing the last few epochs with that maximum value. Smith did many experiments showing that this approach was often able to speed up training considerably and reach better performance. For example, on the popular CIFAR10 image dataset, this approach reached 91.9% validation accuracy in just 100 epochs, instead of 90.3% accuracy in 800 epochs through a standard approach (with the same neural network architecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the 1cycle approach, the implementation poses no particular difficulty: just create a custom callback that modifies the learning rate at each iteration (you can update the optimizer’s learning rate by changing `self.model.optimizer.lr`).\n",
    "To sum up, exponential decay, performance scheduling, and 1cycle can considerably speed up convergence, so give them a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: In the `on_batch_end()` method, `logs[\"loss\"]` used to contain the batch loss, but in TensorFlow 2.2.0 it was replaced with the mean loss (since the start of the epoch). This explains why the graph below is much smoother than in the book (if you are using TF 2.2 or above). It also means that there is a lag between the moment the batch loss starts exploding and the moment the explosion becomes clear in the graph. So you should choose a slightly smaller learning rate than you would have chosen with the \"noisy\" graph. Alternatively, you can tweak the `ExponentialLearningRate` callback above so it computes the batch loss (based on the current mean loss and the previous mean loss):\n",
    "\n",
    "```python\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.prev_loss = 0\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n",
    "        self.prev_loss = logs[\"loss\"]\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(batch_loss)\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/430 [==============================] - 1s 3ms/step - loss: nan - accuracy: 0.3117\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAERCAYAAACO6FuTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmFElEQVR4nO3deXzV9Z3v8dcnCSSQENaQBEJAcGFVWRSsirZWKVLbsWi1Lq3VqXamTnvnTmdqH2MrbW07dWZ67+29nU6d6uBStWrRuuBSba0boggighuyg0DYkwAJST73j3OiMWbP+f5+Jyfv5+NxHpyc8zsnn29C8s53+f2+5u6IiIi0JCvuAkREJH0pJEREpFUKCRERaZVCQkREWqWQEBGRVikkRESkVTlxF5BKw4YN8zFjxsRdhmSYA4eOsHHPQY4alk9Bbkb9yGSEjburqa13jhleEHcpPdarr766y92LWnouo/7HjxkzhmXLlsVdhmSY6po6pv7oj3xpZjk3nDcp7nKkmasWvsL2A4d59Junx11Kj2VmG1t7TsNNIu3Iz83htKOH8cc1O9DJp+nJLO4KMpdCQqQDzp5YzJa9h3hre2XcpUgziu2wFBIiHXDWhOGYwR/X7Ii7FGnG3THUlQhFISHSAcMH5HHiqEEKiTSl4aZwFBIiHXTOxBJWbd3Plr0H4y5FmtBwU1gKCZEOmjelFIBHX38/5kqkKXc02BRQZCFhZrlmdouZbTSzSjNbYWZzWzl2spk9YWa7zEx/KEhaKB/anxPKBvKIQiL9aLwpmCh7EjnAZuAMYCDwPeBeMxvTwrFHgHuBqyKrTqQDPnv8CFZt3c+GXdVxlyJJ+isyrMhCwt2r3X2Bu29w9wZ3fwRYD0xv4di33f0WYHVU9Yl0xLzjk0NOq9SbSBeJ1U0SSmxzEmZWDByLgkB6kBGD+jG1fBCPvaGQSCcabQonlpAwsz7Ab4Hb3P2tbr7X1Wa2zMyWVVRUpKZAkTbMmVTCG1sPsHXfobhLEQku8pAwsyzgDqAWuLa77+fuN7v7DHefUVTU4vWpRFJqzqQSAJ5cvT3mSgS0uim0SEPCzAy4BSgG5rv7kSg/v0gqHDUsn2OLC3hCIZE2TONNwUTdk/gVMAE4z91b7atbQh7QN/lxnpnlRlSjSLvmTCrh5fV72FNdG3cpvZ5rfVNQUZ4nMRq4BjgR2G5mVcnbpWZWnrxfnjx8NHCIDye1DwFvR1WrSHvOmVhCg8NTb+oyHelA/YhwIttPwt030vb3sqDJsRvaOVYkVpNHFjJyUD+eXL2dL84YFXc5vZqu3h6WLssh0gVmxtkTi3n23V1U19TFXU6v5q4lsCEpJES6aM6kEmrrGnj2HS29jpsuFR6OQkKki04aM5jB/ftolVPMNHEdlkJCpItysrP49IRinn5rJ7V1DXGX02u5oxnMgBQSIt0wZ1IJlYfreGnd7rhL6dWUEeEoJES64bRjhtG/b7aGnGKkwaawFBIi3ZDXJ5szji3ij2t20NCgX1ex0OqmoBQSIt00Z1IJOytreG3LvrhL6bW0uikchYRIN31y/HByskxDTjHR6qawFBIi3TSwXx9OGTeUJ1fvwHX6b+R0Ml1YCgmRFJgzqYT1u6p5d2dV3KX0SgqJcBQSIilwzsRizOCJNzTkFDX13cJSSIikwPDCPKaOGsQTaxQSUUvsca2uRCgKCZEUOSe5remWvQfjLqXX0XBTOAoJkRT5cFtT7TERJQ03haWQEEmRo4blc1zxAB7XUljJIAoJkRSaO6WEVzbsYeeBw3GX0mto1XFYCgmRFJo3pRR31JuIkJPYBErCUEiIpNAxxQM4ZngBi1e9H3cpvYoiIhyFhEiKzZ1Sysvr91BRWRN3Kb2DxpuCUkiIpNi8KaU0aMgpMonhpriryFwKCZEUO7a4gHFF+Sx+XUNOUVFGhKOQEEkxM2PelFKWrt/NrioNOYWm0aawFBIiAcxNDjnp8uHhOa7VTQEpJEQCGF8ygLHD8rXKKSKKiHAUEiIBmBnnTillyXu72a0hp6A03BSWQkIkkHOTQ05PrtG1nELSpkNhKSREAplQOoAxQ/tryCkSSolQFBIigTQOOb343m72VNfGXU7G0mhTWAoJkYDOnVJKfYPzpFY5BePuGm4KSCEhEtCkEYWUD+nPYm1rGpQyIpzIQsLMcs3sFjPbaGaVZrbCzOa2cfzfm9l2M9tvZreaWW5UtYqkygdDTmt3se+ghpyk54myJ5EDbAbOAAYC3wPuNbMxzQ80sznAdcBZwBhgLPCDqAoVSaV5U0qpa3DtWBeQhpvCiSwk3L3a3Re4+wZ3b3D3R4D1wPQWDv8KcIu7r3b3vcCPgCuiqlUklSaPLGTUkH48qlVOQeg8ibBim5Mws2LgWGB1C09PAlY2+XglUGxmQ6OoTSSVzIxzJ5fywtpd7D94JO5yMo7jmGYlgoklJMysD/Bb4DZ3f6uFQwqA/U0+brw/oIX3utrMlpnZsoqKitQXK5IC5zYOOa3RBHYIGm4KJ/KQMLMs4A6gFri2lcOqgMImHzfer2x+oLvf7O4z3H1GUVFRSmsVSZXjywZSNrifTqwLQMNNYUUaEpa4VOMtQDEw391b63uvBk5o8vEJwA533x24RJEgGlc5Pb92F/sPacgplbTpUFhR9yR+BUwAznP3Q20cdztwlZlNNLPBwPXAwgjqEwlm7uQSjtQ7T+laTimnOYlwojxPYjRwDXAisN3MqpK3S82sPHm/HMDdHwduAv4MbEzeboiqVpEQThw1iJGDNOSUaq7xpqByovpE7r6Rtk+MLGh2/M+BnwctSiRCZsbcySXcvmQjBw4foTCvT9wlZQQHnXIdkC7LIRKhc48vpba+QUNOKaaMCEchIRKhE8sGUTowj8WrtBQ2ZTTaFJRCQiRCWVnG3MmlPPtuBZWHtcopFRKrm9SXCEUhIRKxeceXUFvXwNNv7oy7lIyhiAhHISESsamjBlNSmKdrOaWIVjeFpZAQiVhWljHv+FKeeXunLh+eIhptCkchIRKD86eO5Ei9qzeRAupHhKWQEInBpBGFHDO8gAdXbI27lB7PXXMSISkkRGJgZvzV1JG8smEvm/ccjLucHk+rm8JRSIjE5PMnjgBQb6KbXANOQSkkRGJSNrg/M48awgOvbdUKnW7QcFNYCgmRGJ0/dSTrKqpZtXV/+wdL65QSwSgkRGI0d0opfXOyeEBDTl2mTlhYCgmRGA3s14ezxg/n4ZXvU9+g33Zdpf0kwlFIiMTs3Cml7KqqYfmmvXGX0mNpcVM4CgmRmH1y/HD65mTxmK4M2yWa9A9LISESs4LcHGYfM4wnVm/XL7wucDRvHZJCQiQNzJlUwtZ9h3hj64G4S+mRNNwUjkJCJA18ekIxOVnGI69vi7uUHkedr7AUEiJpYHB+X848bjgPrNhKXX1D3OX0KI5rdVNACgmRNDF/2kh2Vtbwwnu74y6lx9FwUzgKCZE08akJwxnYrw+/f3VL3KX0KBpuCkshIZImcnOy+dwJI3hi9XYOaP/rTlFPIhyFhEgamT+9jJq6Bha/rs2IOkodibAUEiJp5ISygYwryud+DTl1WGK4SV2JUBQSImnEzLhg+iiWbdzLhl3VcZfTY2i4KRyFhEiaOX/qSLIMfr9cvYmO0YBTSAoJkTRTMjCPU48exqLlW2nQlWHbpU2HwlJIiKShC6aXsXXfIV5ap3MmOkLDTeEoJETS0JxJJQzIzeF+DTm1S32tsBQSImkor082nz2hlMdWbaeqpi7uctKauy7LEVKkIWFm15rZMjOrMbOFbRyXa2b/y8y2mdleM/sPM+sTYakisbtgehmHjtTz2CqdM9EeDTeFE3VPYhtwI3BrO8ddB8wAJgPHAtOA68OWJpJeppUP5qhhOmeiPRpuCivSkHD3Re7+INDebNx5wC/cfY+7VwC/AK4MXZ9IOjEz5k8bydL1e9i852Dc5aQtrW4KK13nJIyPft8NKDOzgTHVIxKL86eVYTpnol2m8aZg0jUkHgO+ZWZFZlYCfDP5eP/mB5rZ1cl5jmUVFRWRFikS2shB/fjEuKH8fvkWnTPRCm35Gla6hsSPgRXAa8CLwIPAEWBn8wPd/WZ3n+HuM4qKiqKsUSQSF0wvY/OeQ7yyYU/cpaQlRURY3Q6JEKuO3P2Qu1/r7iPdfSyJOYxX3b0+1Z9LJN3NmVRCft9sTWC3QaNN4XQqJMzsm2Y2v8nHtwCHzOxtMzuuA6/PMbM8IBvINrM8M8tp4biRZjbCEmYB3wNu6EytIpmif98c5h1fyuJV73OwVudMfIy6EkF1tifxTaACwMxmA18ELiExLPTvHXj99cAhEktcL0vev97Mys2syszKk8eNIzHMVA3cBlzn7k92slaRjHHB9FFU19bz+Bvb4y4lLelkunA+9ld8O0YCG5L3zwPuc/d7zWwV8Fx7L3b3BcCCVp4uaHLcs8CYTtYmkrFOGjOY8iH9uf/VLXxhWlnc5aQVdSTC6mxP4gDQODt8NvB08v4RIC9VRYnIRyXOmShjybrdbNmrcyaacnfNSQTU2ZB4Eviv5FzE0SSWqgJMAtansjAR+agvTBuJOzywfGvcpaQdZUQ4nQ2JbwAvAMOAC9y9cU3eNODuVBYmIh81akh/Zo0dwu+Xb9G5AU3oKxFWp0LC3Q+4+9+5++fd/fEmj9/g7j9JfXki0tQF00exYfdBXt24N+5S0oa7lsCG1NklsBObLnU1s7PN7E4z+66ZZae+PBFpau7kEvr3zdZlOprRZTnC6exw0y3AVAAzKwP+AAwhMQx1Y2pLE5Hm8nNzmDu5lEdWvs+hWp1bCuAacAqqsyExAVievH8hsNTdzwUuB76UysJEpGXzp4+ksqaOJ9fonAnQVWBD62xIZAO1yftnAYuT998DilNVlIi0btZRQxk5qJ8u09GUUiKYzobEG8DfmNnpJEKicfJ6JLArlYWJSMuysoz508t4fu0utu07FHc5sdNgU1idDYnvAF8DngHudvdVycc/B7ycwrpEpA0XTk+cdf27VzbHXEkacF2WI6TOLoF9lsQZ18PcvelOcb8G/iaVhYlI60YN6c/sY4q455VN1NU3xF1O7LS4KZxOXyo8ebnuQ2Y22cwmmVmeu29w94/t9SAi4Vw6s5wdB2p4+q3e/aOn1U1hdfY8iRwz+1dgL7ASWAXsNbObQuwrISKt+9T44ZQU5nHX0k1xlxI7dSTC6WxP4iYSl/j+OnAscAyJYabLgZ+mtjQRaUtOdhYXnTSKZ9+tYNPu3nvRP12hJKzOhsQlwFXufpu7v5e8LQT+Grg05dWJSJsuPnkUBtz9Su/tTTiakwipsyExkMQ5Ec29BwzqdjUi0imlA/vxqfHF3LdsM7V1vXcCW6ubwulsSKwksTtdc99KPiciEbt0Vjm7qmp5YnXvPANbV8QNq7M70/0TsNjMzgaWkOjpnQKMAOamuDYR6YDZxxQxakg/7nxpI+edMCLuciKn4aawunKexLHAfSS2Gy1M3p9Dyz0MEQksO8u4dOZolq7fwzs7KuMuJxbKiHC6cp7ENnf/Z3ef7+5fcPfrgWpgfurLE5GOuHB6GX2zs7jzpY1xlxI5jTaF1emQEJH0M7Qgl3nHl7Jo+Vaqa+riLid6Gm8KRiEhkiEumzWaqpo6HljR+/bAVkSEo5AQyRDTygcxobSQe3rRORNa2RReh1Y3mdlD7RxSmIJaRKQbzIyLZpSx4OE1rN62n0kjBsZdUnCNGaHRpnA62pPY3c5tPXB7iAJFpOM+f+JI+mZncd+y3rUhkU6mC6dDPQl3/2roQkSk+wbn9+WcScU8sGIr180dT16f7LhLCkqDTeFpTkIkw1x8Ujn7Dx1h8ar34y4luLqGxKVIcrLVkwhFISGSYU49eihji/K57cUNcZcSXH1Doi+RnaWQCEUhIZJhzIyvnDKGlVv2s2LT3rjLCaoxJHIUEsEoJEQy0PzpZRTk5mR8b0I9ifAUEiIZqCA3h/nTRrJ41XZ2VdXEXU4wdepJBBdpSJjZtWa2zMxqzGxhG8eZmd1oZlvNbL+ZPWNmkyIsVaTHu/yU0dTWN/C7VzbHXUowH/Yk9PduKFF/ZbcBNwK3tnPchcCVwOnAEBKXJb8jbGkimeXo4QP4xLih3LV00we/TDNN3QchEXMhGSzSL627L3L3B0mcgNeWo4Dn3X2du9cDdwITQ9cnkmm+fMpotu47xJ/e2hl3KUE0qCcRXLp+Ze8BjjazY82sD/AV4PGWDjSzq5NDWMsqKioiLVIk3X16QjElhXncvmRD3KUEoTmJ8NI1JN4HngPeBg6RGH76+5YOdPeb3X2Gu88oKiqKsESR9JeTncUlM8t57t1drKuoiruclKtPnkyn1U3hpGtI3ACcBIwC8oAfAH8ys/6xViXSA1188ihysozfLs28q8OqJxFeuobECcDv3H2Lu9e5+0JgMJqXEOm04QPymDullHuXbaYqwzYkqqtPhESWQiKYqJfA5phZHpANZJtZnpm1dJHBV4ALzazYzLLM7HKgD7A2ynpFMsVXTx1D5eE67l+WWcthG1w9idCi7klcT2KO4TrgsuT9682s3MyqzKw8edzPgJXAa8A+EvMR8919X8T1imSEaeWDmVo+iP9+cUNGLYet0xnXwUW9BHaBu1uz2wJ33+TuBe6+KXncYXf/hruXunuhu09z9xZXN4lIx1x12lFs3H2Qp97cEXcpKfPhtZvSdeS859NXVqSX+MykEkYN6cd//Hltxmz7+eGcRMyFZDB9aUV6iZzsLP72zKNZuWU/S9a1dz5rz6CeRHj6yor0IudPHcmg/n2486WNcZeSEvWuOYnQFBIivUhen2wumjGKJ1bvYMeBw3GX022NJ9NpdVM4CgmRXuaSmeU0uHP3yz3/5LrGOQn1JMJRSIj0MqOH5nPGsUXctXQTR+ob4i6nW7TpUHgKCZFe6PJZo9lZWcMf1/Ts5bC6LEd4CgmRXujM44YzclA/7ljSsyewGzRxHZxCQqQXys4yLp1VzpJ1u3l3R2Xc5XRZ45yElsCGo6+sSC910YxR5OZkccvz6+Mupcs+mJPIVk8iFIWESC81tCCXC2eUsWj5Vnb20OWwH1y7yRQSoSgkRHqxvz5tLEcaGlj44oa4S+kSbToUnkJCpBcbMyyfuZNLuOOljT1yr4l6rW4KTiEh0stdM3sclYfruKcHnlxXpzmJ4BQSIr3cCaMGMWvsEG55fn2PO7lOPYnwFBIiwjWzx/H+/sM8vHJb3KV0SmNPIksT18EoJESEM48r4rjiAfz6L+t61F4T6kmEp5AQEcyMq2eP5e0dlTzzTkXc5XSYrt0UnkJCRAA474QRlA7M4+a/rIu7lA6rb3CyswzTcFMwCgkRAaBvThZXnnoUS9btZuXmfXGX0yF1yZCQcBQSIvKBi08exYC8HG5+tmf0JuobGnS2dWAKCRH5wIC8Plw2azSPvfE+G3dXx11Ou+oaXJPWgSkkROQjvvqJMeRkZfGb59L/wn8NDa4T6QJTSIjIRwwvzOP8qSO5d9lmdlfVxF1Om9STCE8hISIf87XZY6mpa+C2NN+UqF4T18EpJETkY44eXsDZE4u5fckGDtam74X/6hpcE9eBKSREpEVXzx7LvoNHeGDF1rhLaVW95iSCU0iISItmjB7MlJEDufX59TQ0pOelOuobXFuXBqavroi0yMy48rQxvFdRzXNrd8VdTos0JxGeQkJEWjVvygiKBuRya5rug13X0KDVTYEpJESkVX1zsvjyrNH85Z0K1u6sirucj6lvcF0mPLBIQ8LMrjWzZWZWY2YL2zjuP82sqsmtxswqIyxVRJIumVlObk4Wv3j63bhL+Zi6BidHE9dBRd2T2AbcCNza1kHu/nV3L2i8AXcD90VRoIh81NCCXK6ePZaHVm5j+aa9cZfzEZqTCC/SkHD3Re7+ILC7o68xs3xgPnBbqLpEpG1fP2Mcwwfk8qNH1qTVpkT1OuM6uJ4wJzEfqACebelJM7s6OYS1rKKi52yWItKT5Ofm8O05x7Fi0z4eSqMtTnWp8PB6Qkh8BbjdW/nzxd1vdvcZ7j6jqKgo4tJEeo8LppUxaUQhP3vsLQ4fqY+7HEDDTVFI65Aws1HAGcDtcdci0ttlZRnf++xEtu0/zK/TZPe6RE8irX+N9Xjp/tX9MvCiu6fH/0iRXm7W2KHMO76UXz6zlnUV8S+JPVRbR78+6f5rrGeLeglsjpnlAdlAtpnlmVlOGy/5MrAwkuJEpENuOG8iuTlZfHfRqtgnsfdUH2FIfm6sNWS6qCP4euAQcB1wWfL+9WZWnjwforzxQDM7BShDS19F0srwAXl8d+4Elq7fw2NvbI+tDndn78FahuT3ia2G3iDqJbAL3N2a3Ra4+6bkORGbmhy7xN3z3V0n0YmkmYtOGsUxwwv4tyfepq6+IZYaDhyuo77BGdy/byyfv7fQYJ6IdFp2lvGPc45j3a5q7nt1Syw17K2uBWBIvkIiJIWEiHTJ2ROLmVY+iP/91DuxLIndczAREoMVEkEpJESkS8yM73xmPDsO1PB/Yriu0wc9CQ03BaWQEJEumzl2KBfNGMWvnnmPP7wW7Q52ezTcFAmFhIh0y4/+ajInHzWEf7z/dVZEeAHAvRpuioRCQkS6pW9OFv952XSKC3P58q0v89y70VxDbU/1EfpmZ5HfNzuSz9dbKSREpNuG5Pfl7q/NYsTAfvzd3SvYvv9w8M+5/9ARCvv1wbTpUFAKCRFJibLB/fnVZdOoOdLAt+9bSUND2LOxKw8foTCvrQs2SCooJEQkZcYWFfD98yby/Npd/PCRNUE/V1VNHQUKieD0FRaRlLr4pFG8u6OKW19YzyfGDeWcSSVBPk/V4ToGKCSCU09CRFLKzLhu7ngmlBbyjbuW85vn1gW5EGDl4ToKchUSoSkkRCTl+uZkcddfz+TM44Zz46NvcusLG1L+Oapq6ijI1cX9QlNIiEgQg/P7cvPl0zlr/HBuevwtNu85mNL3rzx8RMNNEVBIiEgwZsaN508my4yfPvZmyt7X3amq0ZxEFBQSIhJU6cB+fP2McSxetZ2X1u1OyXserK2nwdGcRAQUEiIS3NWzxzJiYB4/fHgN9Sk4f6Kqpg5AS2AjoJAQkeD69c3mO3PHs+b9A9z18qb2X9COysPJkFBPIjiFhIhE4nMnjODUo4fyL4vf7PYkduXhIwAU5ml1U2gKCRGJhJnxs/nHA3Ddote7de6Ehpuio5AQkciUDe7PdedO4IW1u1m8anuX36dKw02RUUiISKQuObmcCaWF/GTxm13e9nR3csOhwdqVLjiFhIhEKjvL+P5nJ7J13yF+8PDqLl0tdmdlDWYwrEAhEZpCQkQid8q4ofzNmeO4++XNfPOeFdTUda5HUVF5mKH5fcnJ1q+w0DSgJyKx+M5nxjOoXx9++thbHKqt5zdfmdHhDYQqKmsoGpAXuEIB9SREJEbXnDGO7392Ik+/tZN7Xtnc4dftrKxh+IDcgJVJI4WEiMTqik+M4dSjh/L9P7zR4f2xdx5QSERFISEiscrKMv7j0umMKyrg6ttf5b9fWN/mZHZDg7OrqoYihUQkFBIiEruB/fpw+1UnM2PMYH7w8BquufPVD86qbm7PwVrqGlw9iYgoJEQkLQwfkMftV57MDedN5E9v7eSy3yxtMSjWbDsAJPbTlvAUEiKSNsyMr556FDdfPp03th3ghodWf+yYpet3k51lTB89OIYKe59IQ8LMrjWzZWZWY2YL2zl2rJk9YmaVZrbLzG6KqEwRidlZE4r5xiePZtHyrSx576N7UCxdt4cpIweSr0tyRCLqnsQ24Ebg1rYOMrO+wB+BPwElQBlwZ/DqRCRt/O2Z4ygpzOOmJ9764GKAh2rrWbllHzPHDom5ut4j0pBw90Xu/iDQ3vZUVwDb3P3n7l7t7ofd/fXgBYpI2sjrk823Pn0MKzbt46k3dwKwYtNejtQ7M49SSEQlXeckZgEbzOyx5FDTM2Y2Je6iRCRaF04vY+ywfBY8tJp9B2tZun4PWQYzxigkopKug3plwCeBzwFPA98C/mBm4929tumBZnY1cDVAQek4Lvr1kqhrFZGA+vXNZv2uamb+5GmO1DfQv28OX7ttWdxl9RrWnY0/uvxJzW4Eytz9ilae/wNQ6O6fTH5swD5gtruvbON9K4G3u1neQGB/N49r6bmOPNb045buDwN2daC2tqh97R/XlfZ1pK2Z2L6mj6t97YuqfZ392Rvt7kUtfjZ3j/xGYvJ6YRvP/wj4U5OPLdmQE9p532UpqO3m7h7X0nMdeazpxy3dV/vSt30daWsmtq/ZMWpfmrSvsz97bd2iXgKbY2Z5QDaQbWZ5ZtbSkNedwCwz+7SZZQP/g0SCvxlBmQ+n4LiWnuvIYw934H53qX3tH9eV9nW0rd2Vbu1LZds6835qX9uPpexnL9LhJjNbANzQ7OEfkFgSuwaY6O6bksd+AbgJGA4sB77h7h8/s+aj77/M3Wekuu50ofb1bGpfz5bp7WtNpBPX7r4AWNDK0x85x97dFwGLOvkpbu58VT2K2tezqX09W6a3r0WxTFyLiEjPkK7nSYiISBpQSIiISKt6XUiY2Rgzq0iexf2MmbW8NriHM7MvmVnHtvnqQcys2MxeNLO/mNmfzKw07ppSycxOMbMlyfbdbWZ94q4plcxsoJm9bGZVZjY57npSwcx+bGbPmdn9ZtY/7npSrdeFRNJf3P3M5C0Tf5FmARcAHd80uOfYBZzm7mcAtwNXxVxPqm0EPpVs3zrg8zHXk2oHgXnA/XEXkgrJoBvn7qcDTwFXxlxSyvXWkDg1mfw/SZ7NnWkuIfFD2BB3Ianm7vXu3tiuAUCby6J7Gnff5u6Hkh/WkWHfQ3c/kmF/mJ0OPJa8/xhwWoy1BJHWIdHW/hNmNsTMHjCzajPbaGaXdPBt3weOBmaTOAfjC6mtuuNCtC958uEXgd8FKLlTAn3/MLMTzWwpcC2Jc2hiEap9ydcfBcwFHklhyZ0Ssn3pphttHcyHl7jYD2TclQfT9QJ/jRr3n5gD9Gv23C+BWqAYOBF41MxWuvtqMyuh5e7sBe6+HagBMLNFJK44+/sw5bcr5e1Lvte97t6QBp2kIN8/d38NmGlmXwS+C3w9UP3tCdI+MysEbgMu92YXtIxYqJ+/dNSltgJ7SVz/iOS/eyKpNkrdvRZJFDeaXesJyCfxTTu2yWN3AP/SgfcqbHL/p8CXM6x9PwOeBB4n8ZfNLzKsfblN7s8Bfp5h7csBHiUxLxFru0K0r8nxC4HJcbetu20FpgB3Je9fDfxd3G1I9S2th5vacCxQ7+7vNHlsJTCpA689w8xeNbPngJHAXSEK7KYut8/dv+Pu57j7Z4B33f2boYrshu58/6aZ2bNm9mcS1/T61wD1dVd32vclYCbw/eTqu4tCFNhN3WkfZrYYOAf4LzO7IvXlpVSbbXX3VcDG5O+TObSz62ZPlO7DTa0p4OOXyt1PYiKzTe7+MKm/aFeqdbl9TXn6XmemO9+/JSTmk9JZd9p3B4m/VNNZt/5/uvu5Ka8onHbb6u7fjbSiiPXUnkQVUNjssUKgMoZaQlD7eja1L3P0pra2qKeGxDtAjpkd0+SxE8ic5ZBqX8+m9mWO3tTWFqV1SFgr+0+4ezWJK8T+0MzyzexUEicdpXs3/SPUPrUvnWV6+5rqTW3ttLhnzttZabAA8Ga3BcnnhgAPAtXAJuCSuOtV+9Q+ta9n3npTWzt706XCRUSkVWk93CQiIvFSSIiISKsUEiIi0iqFhIiItEohISIirVJIiIhIqxQSIiLSKoWESAqZ2QIzeyPuOkRSRSfTSY+T3DlsmLt/Nu5amjOzAhJ7XuyOu5bWmJkDF7p7RuwzLWGpJyHSAWbWtyPHuXtVHAFhZlnJrWtFUkohIRnHzCaa2aNmVmlmO83s7uSWmo3Pn2RmT5rZLjM7YGbPm9kpzd7DzewbZrbIzKqBnzQOJZnZxWb2XvL9HzSzYU1e95HhJjNbaGaPmNm3zGyrme01s/82s/5Njsk3s9vNrMrMdpjZd5OvWdhGG69IHn9u8vPVAhPaa5uZbUjevS/Zxg1NnjsvuSHXYTNbb2Y/7mg4SuZSSEhGMbNS4FngDeBk4NMkNo55yMwa/78PIHEVz9OTx7wGLG76yz7pBmAxiS0qf5l8bAxwEXA+id3VpgI/bqes04HJyVoaX/utJs//O3BG8vFPkbgU9ekdaG4ecD1wDTAR2NiBtp2U/PdrQGnjx2Y2B/gt8P9I7Lp2JYk903/SgTokk8V9hUHddOvsjcT+yI+08twPgaebPTaYxFU9T27lNQa8D1zW5DEH/m+z4xYAh4GBTR77Z2Bts2PeaFbrZiCnyWP/BTyVvF9AohdwcZPn84G9NNlruYWar0jWOL2dr1Vrbbug2XHPAt9r9thfkdh0x+L+nusW3009Cck004HZyaGYKjOrIvFLGmAcgJkNN7Nfm9k7ZrafxC5jw4HyZu+1rIX33+juTbez3JZ8bVvWuHtdK68ZB/QBXm580hN7GHRkhVQdiZ7CBzrRtuamA//c7Ot2F4nAKmn7pZLJeuoe1yKtyQIeBb7dwnM7kv/eBhQDfw9sAGqAp4Hm4+/VLbzHkWYfO+0P27b1GmvyWGfVuHt9s8c62rbmsoAfAPe18FxFF2qTDKGQkEyzHPgiib/4m/9ybnQa8E13fxTAzIpJjM/HYS2JEDkZWJ+spz+JOYz3uvB+HWnbERI7sDW1HBjv7mu78DklgykkpKcqNLMTmz22j8QE89eA35nZz0j8FTyWRHD8g7tXkti3+DIzW0piOOUmEvMCkXP3KjO7FfiZme0iMX9wPYm/7LvSu+hI2zYAZ5nZX0j0RvaSmMt5xMw2AveSGMqaTGIe55+6UIdkCM1JSE91OrCi2e3f3H0bcCrQADxOYsP6X5IYdqlJvvZKEhPGrwL3ALeS+MUZl28DzwEPAX8GXicxH3K4C+/Vkbb9A/BJEnM1KwDc/QlgXvLxl5O360hs1ym9mM64FkkzZpZLYjnrv7r7v8ddj/RuGm4SiZmZTQUmkPjrfQDwneS/v4uzLhFQSIiki/8JHMeHy1pnu/uWWCsSQcNNIiLSBk1ci4hIqxQSIiLSKoWEiIi0SiEhIiKtUkiIiEirFBIiItKq/w/1Xngcc/m53wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.7740 - val_loss: 0.4872 - val_accuracy: 0.8336\n",
      "Epoch 2/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.4581 - accuracy: 0.8396 - val_loss: 0.4275 - val_accuracy: 0.8524\n",
      "Epoch 3/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.4122 - accuracy: 0.8545 - val_loss: 0.4115 - val_accuracy: 0.8582\n",
      "Epoch 4/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3837 - accuracy: 0.8642 - val_loss: 0.3869 - val_accuracy: 0.8688\n",
      "Epoch 5/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3639 - accuracy: 0.8717 - val_loss: 0.3765 - val_accuracy: 0.8684\n",
      "Epoch 6/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3457 - accuracy: 0.8775 - val_loss: 0.3743 - val_accuracy: 0.8698\n",
      "Epoch 7/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3330 - accuracy: 0.8809 - val_loss: 0.3633 - val_accuracy: 0.8706\n",
      "Epoch 8/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3184 - accuracy: 0.8860 - val_loss: 0.3947 - val_accuracy: 0.8618\n",
      "Epoch 9/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3065 - accuracy: 0.8888 - val_loss: 0.3493 - val_accuracy: 0.8762\n",
      "Epoch 10/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2945 - accuracy: 0.8922 - val_loss: 0.3402 - val_accuracy: 0.8790\n",
      "Epoch 11/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2840 - accuracy: 0.8960 - val_loss: 0.3456 - val_accuracy: 0.8810\n",
      "Epoch 12/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2710 - accuracy: 0.9020 - val_loss: 0.3657 - val_accuracy: 0.8696\n",
      "Epoch 13/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2538 - accuracy: 0.9080 - val_loss: 0.3357 - val_accuracy: 0.8832\n",
      "Epoch 14/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2406 - accuracy: 0.9133 - val_loss: 0.3461 - val_accuracy: 0.8794\n",
      "Epoch 15/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2280 - accuracy: 0.9182 - val_loss: 0.3260 - val_accuracy: 0.8844\n",
      "Epoch 16/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2160 - accuracy: 0.9231 - val_loss: 0.3298 - val_accuracy: 0.8836\n",
      "Epoch 17/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2063 - accuracy: 0.9264 - val_loss: 0.3349 - val_accuracy: 0.8872\n",
      "Epoch 18/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1979 - accuracy: 0.9302 - val_loss: 0.3247 - val_accuracy: 0.8902\n",
      "Epoch 19/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1892 - accuracy: 0.9343 - val_loss: 0.3237 - val_accuracy: 0.8902\n",
      "Epoch 20/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1821 - accuracy: 0.9371 - val_loss: 0.3228 - val_accuracy: 0.8926\n",
      "Epoch 21/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1752 - accuracy: 0.9402 - val_loss: 0.3223 - val_accuracy: 0.8916\n",
      "Epoch 22/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1700 - accuracy: 0.9419 - val_loss: 0.3184 - val_accuracy: 0.8946\n",
      "Epoch 23/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1654 - accuracy: 0.9439 - val_loss: 0.3190 - val_accuracy: 0.8940\n",
      "Epoch 24/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1626 - accuracy: 0.9457 - val_loss: 0.3181 - val_accuracy: 0.8940\n",
      "Epoch 25/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1609 - accuracy: 0.9464 - val_loss: 0.3174 - val_accuracy: 0.8944\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization  \n",
    "\n",
    "We already implemented one of the best regularization techniques in Chapter 10: early stopping. Moreover, even though Batch Normalization was designed to solve the unstable gradients problems, it also acts like a pretty good regularizer. In this section we will examine other popular regularization techniques for neural networks: $\\ell_1$ and $\\ell_2$ regularization, dropout, and max-norm regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell_1$ and $\\ell_2$ regularization  \n",
    "\n",
    "Just like you did in Chapter 4 for simple linear models, you can use $\\ell_2$ regularization to constrain a neural network’s connection weights, and/or $\\ell_1$ regularization if you want a sparse model (with many weights equal to 0). Here is how to apply $\\ell_2$ regularization to a Keras layer’s connection weights, using a regularization factor of 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor or 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 3.2189 - accuracy: 0.7967 - val_loss: 0.7169 - val_accuracy: 0.8340\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7280 - accuracy: 0.8247 - val_loss: 0.6850 - val_accuracy: 0.8376\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `l2()` function returns a regularizer that will be called at each step during training to compute the regularization loss. This is then added to the final loss. As you might expect, you can just use `keras.regularizers.l1()` if you want $\\ell_1$ regularization; if you want both $\\ell_1$ and $\\ell_2$ regularization, use `keras.regularizers.l1_l2()` (specifying both regularization factors).\n",
    "\n",
    "Since you will typically want to apply the same regularizer to all layers in your network, as well as using the same activation function and the same initialization strategy in all hidden layers, you may find yourself repeating the same arguments. This makes the code ugly and error-prone. To avoid this, you can try refactoring your code to use loops. Another option is to use Python’s `functools.partial()` function, which lets you create a thin wrapper for any callable, with some default argument values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 3.2911 - accuracy: 0.7924 - val_loss: 0.7218 - val_accuracy: 0.8310\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7282 - accuracy: 0.8245 - val_loss: 0.6826 - val_accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout  \n",
    "\n",
    "Dropout is one of the most popular regularization techniques for deep neural networks. It was proposed in a paper by Geoffrey Hinton in 2012 and further detailed in a 2014 paper by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-the-art neural networks get a 1–2% accuracy boost simply by adding dropout. This may not sound like a lot, but when a model already has 95% accuracy, getting a 2% accuracy boost means dropping the error rate by almost 40% (going from 5% error to roughly 3%).\n",
    "\n",
    "It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability $p$ of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step (see Figure 11-9). The hyperparameter $p$ is called the dropout rate, and it is typically set between 10% and 50%: closer to 20–30% in recurrent neural nets (see Chapter 15), and closer to 40–50% in convolutional neural networks (see Chapter 14). After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss momentarily).  \n",
    "\n",
    "<img src=\"./chapters/11/11.9.png\" width = 600>\n",
    "<div style=\"text-align:left\"> Figure 11-9. With dropout regularization, at each training iteration a random subset of all neurons in one or more layers—except the output layer—are “dropped out”; these neurons output 0 at this iteration (represented by the dashed arrows) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s surprising at first that this destructive technique works at all. Would a company perform better if its employees were told to toss a coin every morning to decide whether or not to go to work? Well, who knows; perhaps it would! The company would be forced to adapt its organization; it could not rely on any single person to work the coffee machine or perform any other critical tasks, so this expertise would have to be spread across several people. Employees would have to learn to cooperate with many of their coworkers, not just a handful of them. The company would become much more resilient. If one person quit, it wouldn’t make much of a difference. It’s unclear whether this idea would actually work for companies, but it certainly does for neural networks. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better.\n",
    "\n",
    "Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there are a total of $2^N$ possible networks (where N is the total number of droppable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice. Once you have run 10,000 training steps, you have essentially trained 10,000 different neural networks (each with just one training instance). These neural networks are obviously not independent because they share many of their weights, but they are nevertheless all different. The resulting neural network can be seen as an averaging ensemble of all these smaller neural networks.  \n",
    "\n",
    ">In practice, you can usually apply dropout only to the neurons in the top one to three layers (excluding the output layer).\n",
    "\n",
    "\n",
    "There is one small but important technical detail. Suppose $p = 50%$, in which case during testing a neuron would be connected to twice as many input neurons as it would be (on average) during training. To compensate for this fact, we need to multiply each neuron’s input connection weights by 0.5 after training. If we don’t, each neuron will get a total input signal roughly twice as large as what the network was trained on and will be unlikely to perform well. More generally, we need to multiply each input connection weight by the keep probability $(1 – p)$ after training. Alternatively, we can divide each neuron’s output by the keep probability during training (these alternatives are not perfectly equivalent, but they work equally well).\n",
    "\n",
    "To implement dropout using Keras, you can use the `keras.layers.Dropout` layer. During training, it randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability. After training, it does nothing at all; it just passes the inputs to the next layer. The following code applies dropout regularization before every Dense layer, using a dropout rate of 0.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7611 - accuracy: 0.7576 - val_loss: 0.3730 - val_accuracy: 0.8644\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4306 - accuracy: 0.8401 - val_loss: 0.3395 - val_accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Dropout  \n",
    "\n",
    "If you want to regularize a self-normalizing network based on the SELU activation function (as discussed earlier), you should use *alpha dropout*: this is a variant of dropout that preserves the mean and standard deviation of its inputs (it was introduced in the same paper as SELU, as regular dropout would break self-normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.8023 - accuracy: 0.7146 - val_loss: 0.5778 - val_accuracy: 0.8446\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5662 - accuracy: 0.7903 - val_loss: 0.5160 - val_accuracy: 0.8518\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5260 - accuracy: 0.8058 - val_loss: 0.4899 - val_accuracy: 0.8610\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5129 - accuracy: 0.8093 - val_loss: 0.4766 - val_accuracy: 0.8592\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5073 - accuracy: 0.8119 - val_loss: 0.4237 - val_accuracy: 0.8710\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4784 - accuracy: 0.8208 - val_loss: 0.4614 - val_accuracy: 0.8636\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4720 - accuracy: 0.8266 - val_loss: 0.4725 - val_accuracy: 0.8608\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4575 - accuracy: 0.8288 - val_loss: 0.4159 - val_accuracy: 0.8700\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4625 - accuracy: 0.8284 - val_loss: 0.4281 - val_accuracy: 0.8740\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4552 - accuracy: 0.8336 - val_loss: 0.4340 - val_accuracy: 0.8620\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4467 - accuracy: 0.8325 - val_loss: 0.4173 - val_accuracy: 0.8708\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4408 - accuracy: 0.8359 - val_loss: 0.5168 - val_accuracy: 0.8562\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4337 - accuracy: 0.8396 - val_loss: 0.4274 - val_accuracy: 0.8734\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4307 - accuracy: 0.8399 - val_loss: 0.4551 - val_accuracy: 0.8618\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4323 - accuracy: 0.8383 - val_loss: 0.4425 - val_accuracy: 0.8672\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4264 - accuracy: 0.8399 - val_loss: 0.4136 - val_accuracy: 0.8790\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4195 - accuracy: 0.8435 - val_loss: 0.5245 - val_accuracy: 0.8586\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4350 - accuracy: 0.8404 - val_loss: 0.4732 - val_accuracy: 0.8702\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4284 - accuracy: 0.8403 - val_loss: 0.4674 - val_accuracy: 0.8764\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4182 - accuracy: 0.8428 - val_loss: 0.4349 - val_accuracy: 0.8746\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 973us/step - loss: 0.4711 - accuracy: 0.8586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47112908959388733, 0.8586000204086304]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 2s 977us/step - loss: 0.3488 - accuracy: 0.8823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34875163435935974, 0.8822908997535706]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4229 - accuracy: 0.8433\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC Dropout  \n",
    "\n",
    "In 2016, a paper by Yarin Gal and Zoubin Ghahramani added a few more good reasons to use dropout:\n",
    "\n",
    "- First, the paper established a profound connection between dropout networks (i.e., neural networks containing a `Dropout` layer before every weight layer) and approximate Bayesian inference, giving dropout a solid mathematical justification.\n",
    "\n",
    "- Second, the authors introduced a powerful technique called *MC Dropout*, which can boost the performance of any trained dropout model without having to retrain it or even modify it at all, provides a much better measure of the model’s uncertainty, and is also amazingly simple to implement.\n",
    "\n",
    "If this all sounds like a “one weird trick” advertisement, then take a look at the following code. It is the full implementation of *MC Dropout*, boosting the dropout model we trained earlier without retraining it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just make 100 predictions over the test set, setting `training=True` to ensure that the `Dropout` layer is active, and stack the predictions. Since dropout is active, all the predictions will be different. Recall that `predict()` returns a matrix with one row per instance and one column per class. Because there are 10,000 instances in the test set and 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such matrices, so `y_probas` is an array of shape [100, 10000, 10]. Once we average over the first dimension (`axis=0`), we get `y_proba`, an array of shape [10000, 10], like we would get with a single prediction. That’s all! Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off. For example, let’s look at the model’s prediction for the first instance in the Fashion MNIST test set, with dropout off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems almost certain that this image belongs to class 9 (ankle boot). Should you trust it? Is there really so little room for doubt? Compare this with the predictions made when dropout is activated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.83, 0.  , 0.14]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.96, 0.  , 0.03]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.  , 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.39, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.62, 0.  , 0.38]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.39, 0.  , 0.52]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.39, 0.  , 0.51]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.62, 0.  , 0.12, 0.  , 0.26]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.22, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.39, 0.  , 0.22, 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.31, 0.  , 0.65]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.26, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.26, 0.  , 0.69]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.11, 0.  , 0.73]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.71, 0.  , 0.27]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.41, 0.  , 0.57]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.02, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.81, 0.03, 0.02, 0.  , 0.14]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.1 , 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.07, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.65, 0.  , 0.29]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.15, 0.  , 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.37, 0.  , 0.47]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.07, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.47, 0.  , 0.23]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.45, 0.  , 0.04, 0.  , 0.51]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.83, 0.  , 0.1 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.15, 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.64, 0.  , 0.32]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.37, 0.  , 0.62]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.61, 0.  , 0.06, 0.  , 0.32]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.26, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.33, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.13, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.49, 0.  , 0.5 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.27, 0.  , 0.72]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.34, 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.89, 0.  , 0.1 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.05, 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.07, 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.69]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.16, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.24, 0.  , 0.69]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.2 , 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.04, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.32, 0.  , 0.67]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.03, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.34, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.58, 0.  , 0.17]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.54, 0.  , 0.35, 0.  , 0.1 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.45, 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.28, 0.  , 0.67]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.61, 0.  , 0.26]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.2 , 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.18, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.34, 0.  , 0.63]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.28, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.02, 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.11, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.63, 0.  , 0.35]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.43, 0.  , 0.55]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.16, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.06, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.12, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.46, 0.  , 0.5 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.77, 0.  , 0.19, 0.  , 0.04]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.33, 0.  , 0.05, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.26, 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.1 , 0.  , 0.88]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.47, 0.  , 0.43]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.23, 0.  , 0.58]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.25, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.11, 0.  , 0.88]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.29, 0.  , 0.4 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.09, 0.  , 0.76]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells a very different story: apparently, when we activate dropout, the model is not sure anymore. It still seems to prefer class 9, but sometimes it hesitates with classes 5 (sandal) and 7 (sneaker), which makes sense given they’re all footwear. Once we average over the first dimension, we get the following MC Dropout predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.23, 0.  , 0.68]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still thinks this image belongs to class 9, but only with a 62% confidence, which seems much more reasonable than 99%. Plus it’s useful to know exactly which other classes it thinks are likely. And you can also take a look at the standard deviation of the probability estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.22, 0.  , 0.27]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there’s quite a lot of variance in the probability estimates: if you were building a risk-sensitive system (e.g., a medical or financial system), you should probably treat such an uncertain prediction with extreme caution. You definitely would not treat it like a 99% confident prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8665"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model contains other layers that behave in a special way during training (such as `BatchNormalization` layers), then you should not force training mode like we just did. Instead, you should replace the `Dropout` layers with the following `MCDropout` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we just subclass the `Dropout` layer and override the `call()` method to force its training argument to `True` (see Chapter 12). Similarly, you could define an `MCAlphaDropout` class by subclassing `AlphaDropout` instead. If you are creating a model from scratch, it’s just a matter of using `MCDropout` rather than `Dropout`. But if you have a model that was already trained using Dropout, you need to create a new model that’s identical to the existing model except that it replaces the `Dropout` layers with `MCDropout`, then copy the existing model’s weights to your new model.\n",
    "\n",
    "In short, MC Dropout is a fantastic technique that boosts dropout models and provides better uncertainty estimates. And of course, since it is just regular dropout during training, it also acts like a regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_18 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout (MCAlphaDro (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_1 (MCAlphaD (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_2 (MCAlphaD (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model with MC Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.26, 0.  , 0.62]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max norm  \n",
    "\n",
    "Another regularization technique that is popular for neural networks is called *max-norm regularization*: for each neuron, it constrains the weights $\\mathbf w$ of the incoming connections such that $\\lVert\\mathbf w \\rVert_2 \\le r$, where r is the max-norm hyperparameter and $\\lVert\\cdot \\rVert_2$ is the $\\ell_2$ norm.\n",
    "\n",
    "Max-norm regularization does not add a regularization loss term to the overall loss function. Instead, it is typically implemented by computing $\\lVert\\mathbf w \\rVert_2$ after each training step and rescaling $\\mathbf w$ if needed ($\\mathbf w \\leftarrow\\mathbf w r/\\lVert\\mathbf w \\rVert_2$).\n",
    "\n",
    "Reducing r increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the unstable gradients problems (if you are not using Batch Normalization).\n",
    "\n",
    "To implement max-norm regularization in Keras, set the `kernel_constraint` argument of each hidden layer to a `max_norm()` constraint with the appropriate max value, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5756 - accuracy: 0.8027 - val_loss: 0.3776 - val_accuracy: 0.8654\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3545 - accuracy: 0.8699 - val_loss: 0.3746 - val_accuracy: 0.8714\n"
     ]
    }
   ],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each training iteration, the model’s `fit()` method will call the object returned by `max_norm()`, passing it the layer’s weights and getting rescaled weights in return, which then replace the layer’s weights. As you’ll see in Chapter 12, you can define your own custom constraint function if necessary and use it as the `kernel_constraint`. You can also constrain the bias terms by setting the `bias_constraint` argument.\n",
    "\n",
    "The `max_norm()` function has an axis argument that defaults to 0. A Dense layer usually has weights of shape [number of inputs, number of neurons], so using `axis=0` means that the max-norm constraint will apply independently to each neuron’s weight vector. If you want to use max-norm with convolutional layers (see Chapter 14), make sure to set the `max_norm()` constraint’s axis argument appropriately (usually axis=[0, 1, 2])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Practical Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Table 11-3. Default DNN configuration* \n",
    "\n",
    "| Hyperparameter      | Default value |\n",
    "| ----------- | ----------- |\n",
    "| Kernel initializer      | He initialization       |\n",
    "| Activation function   | ELU        |\n",
    "| Normalization   | None if shallow; Batch Norm if deep        |\n",
    "| Regularization   | Early stopping ($+\\ell_2$ reg. if needed)        |\n",
    "| Optimizer   | Momentum optimization (or RMSProp or Nadam)        |\n",
    "| Learning rate schedule   | 1cycle       |  \n",
    "\n",
    "\n",
    "If the network is a simple stack of dense layers, then it can self-normalize, and you should use the configuration in Table 11-4 instead.\n",
    "\n",
    "*Table 11-4. DNN configuration for a self-normalizing net* \n",
    "\n",
    "| Hyperparameter      | Default value |\n",
    "| ----------- | ----------- |\n",
    "| Kernel initializer      | LeCun initialization       |\n",
    "| Activation function      | SELU       |\n",
    "| Normalization      | None (self-normalization)       |\n",
    "| Regularization      | Alpha dropout if needed       |\n",
    "| Optimizer      | Momentum optimization (or RMSProp or Nadam)       |\n",
    "| Learning rate schedule      | 1cycle       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t forget to normalize the input features! You should also try to reuse parts of a pretrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of unlabeled data, or use pretraining on an auxiliary task if you have a lot of labeled data for a similar task.\n",
    "\n",
    "While the previous guidelines should cover most cases, here are some exceptions:\n",
    "\n",
    "- If you need a sparse model, you can use $\\ell_1$ regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can use the TensorFlow Model Optimization Toolkit. This will break self-normalization, so you should use the default configuration in this case.\n",
    "\n",
    "- If you need a low-latency model (one that performs lightning-fast predictions), you may need to use fewer layers, fold the Batch Normalization layers into the previous layers, and possibly use a faster activation function such as leaky ReLU or just ReLU. Having a sparse model will also help. Finally, you may want to reduce the float precision from 32 bits to 16 or even 8 bits (see “Deploying a Model to a Mobile or Embedded Device”). Again, check out TF-MOT.\n",
    "\n",
    "- If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "> No, all weights should be sampled independently; they should not all have the same initial value. One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken (i.e., all neurons in a given layer are equivalent), and backpropagation will be unable to break it. Concretely, this means that all the neurons in any given layer will always have the same weights. It’s like having just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution.\n",
    "\n",
    "2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "> It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights, and that’s okay too; it does not make much difference.\n",
    "\n",
    "3. Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "> A few advantages of the SELU function over the ReLU function are:  \n",
    "> - It can take on negative values, so the average output of the neurons in any given layer is typically closer to zero than when using the ReLU activation function (which never outputs negative values). This helps alleviate the vanishing gradients problem.\n",
    "> - It always has a nonzero derivative, which avoids the dying units issue that can affect ReLU units.\n",
    "> - When the conditions are right (i.e., if the model is sequential, and the weights are initialized using LeCun initialization, and the inputs are standardized, and there’s no incompatible layer or regularization, such as dropout or $\\ell_1$ regularization), then the SELU activation function ensures the model is self-normalized, which solves the exploding/vanishing gradients problems.\n",
    "\n",
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "> The SELU activation function is a good default. If you need the neural network to be as fast as possible, you can use one of the leaky ReLU variants instead (e.g., a simple leaky ReLU using the default hyperparameter value). The simplicity of the ReLU activation function makes it many people’s preferred option, despite the fact that it is generally outperformed by SELU and leaky ReLU. However, the ReLU activation function’s ability to output precisely zero can be useful in some cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized implementation as well as from hardware acceleration. The hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number between –1 and 1, but nowadays it is not used much in hidden layers (except in recurrent nets). The logistic activation function is also useful in the output layer when you need to estimate a probability (e.g., for binary classification), but is rarely used in hidden layers (there are exceptions—for example, for the coding layer of variational autoencoders; see Chapter 17). Finally, the softmax activation function is useful in the output layer to output probabilities for mutually exclusive classes, but it is rarely (if ever) used in hidden layers.\n",
    "\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    ">  If you set the `momentum` hyperparameter too close to 1 (e.g., 0.99999) when using an `SGD` optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller `momentum` value.\n",
    "\n",
    "\n",
    "6. Name three ways you can produce a sparse model.\n",
    "\n",
    "> One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights. For more sparsity, you can apply $\\ell_1$ regularization during training, which pushes the optimizer toward sparsity. A third option is to use the TensorFlow Model Optimization Toolkit.\n",
    "\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "> Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor of 10 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "  a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "  b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with `keras.datasets.cifar10.load_​data()`. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "\n",
    "  c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "\n",
    "  d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "\n",
    "  e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
    "\n",
    "  f. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "*Exercise: Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "*Exercise: Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with `keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the output layer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the CIFAR10 dataset. We also want to use early stopping, so we need a validation set. Let's use the first 5,000 images of the original training set as the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the callbacks we need and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d3877adba771e5ac\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d3877adba771e5ac\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 10s 6ms/step - loss: 9.6201 - accuracy: 0.1418 - val_loss: 2.1457 - val_accuracy: 0.2250\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 2.1001 - accuracy: 0.2403 - val_loss: 2.1851 - val_accuracy: 0.2178\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.9837 - accuracy: 0.2798 - val_loss: 2.1055 - val_accuracy: 0.2610\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.8817 - accuracy: 0.3153 - val_loss: 1.9316 - val_accuracy: 0.3294\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.8096 - accuracy: 0.3414 - val_loss: 1.8083 - val_accuracy: 0.3346\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.7582 - accuracy: 0.3584 - val_loss: 1.7506 - val_accuracy: 0.3648\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.7151 - accuracy: 0.3761 - val_loss: 1.7398 - val_accuracy: 0.3522\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6731 - accuracy: 0.3922 - val_loss: 1.6687 - val_accuracy: 0.3980\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6495 - accuracy: 0.4035 - val_loss: 1.6977 - val_accuracy: 0.3802\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6136 - accuracy: 0.4143 - val_loss: 1.6884 - val_accuracy: 0.3918\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5903 - accuracy: 0.4278 - val_loss: 1.6207 - val_accuracy: 0.4162\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5727 - accuracy: 0.4340 - val_loss: 1.6774 - val_accuracy: 0.3986\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.5439 - accuracy: 0.4449 - val_loss: 1.6403 - val_accuracy: 0.4118\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5229 - accuracy: 0.4507 - val_loss: 1.5798 - val_accuracy: 0.4296\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.5172 - accuracy: 0.4534 - val_loss: 1.6300 - val_accuracy: 0.4146\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.5055 - accuracy: 0.4601 - val_loss: 1.5630 - val_accuracy: 0.4484\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4879 - accuracy: 0.4640 - val_loss: 1.5671 - val_accuracy: 0.4478\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4698 - accuracy: 0.4686 - val_loss: 1.5949 - val_accuracy: 0.4264\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.4505 - accuracy: 0.4808 - val_loss: 1.5781 - val_accuracy: 0.4372\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4393 - accuracy: 0.4807 - val_loss: 1.5414 - val_accuracy: 0.4418\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4326 - accuracy: 0.4833 - val_loss: 1.5447 - val_accuracy: 0.4514\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.4204 - accuracy: 0.4886 - val_loss: 1.5487 - val_accuracy: 0.4454\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3917 - accuracy: 0.5003 - val_loss: 1.5530 - val_accuracy: 0.4530\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3901 - accuracy: 0.5011 - val_loss: 1.5744 - val_accuracy: 0.4390\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3772 - accuracy: 0.5052 - val_loss: 1.5228 - val_accuracy: 0.4580\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3561 - accuracy: 0.5110 - val_loss: 1.5477 - val_accuracy: 0.4556\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3546 - accuracy: 0.5141 - val_loss: 1.5684 - val_accuracy: 0.4506\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3463 - accuracy: 0.5192 - val_loss: 1.5318 - val_accuracy: 0.4642\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3363 - accuracy: 0.5190 - val_loss: 1.5210 - val_accuracy: 0.4570\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3399 - accuracy: 0.5197 - val_loss: 1.6036 - val_accuracy: 0.4504\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3327 - accuracy: 0.5229 - val_loss: 1.5818 - val_accuracy: 0.4424\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3081 - accuracy: 0.5296 - val_loss: 1.5406 - val_accuracy: 0.4618\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3105 - accuracy: 0.5288 - val_loss: 1.5316 - val_accuracy: 0.4548\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2872 - accuracy: 0.5350 - val_loss: 1.5400 - val_accuracy: 0.4578\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2863 - accuracy: 0.5366 - val_loss: 1.5395 - val_accuracy: 0.4624\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2758 - accuracy: 0.5421 - val_loss: 1.5584 - val_accuracy: 0.4552\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2614 - accuracy: 0.5470 - val_loss: 1.5331 - val_accuracy: 0.4592\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2563 - accuracy: 0.5476 - val_loss: 1.5182 - val_accuracy: 0.4636\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2554 - accuracy: 0.5443 - val_loss: 1.5423 - val_accuracy: 0.4674\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2440 - accuracy: 0.5550 - val_loss: 1.5705 - val_accuracy: 0.4580\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2354 - accuracy: 0.5575 - val_loss: 1.5482 - val_accuracy: 0.4604\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2263 - accuracy: 0.5625 - val_loss: 1.5707 - val_accuracy: 0.4658\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2238 - accuracy: 0.5640 - val_loss: 1.5986 - val_accuracy: 0.4556\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2115 - accuracy: 0.5663 - val_loss: 1.5606 - val_accuracy: 0.4574\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2041 - accuracy: 0.5633 - val_loss: 1.5645 - val_accuracy: 0.4552\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1884 - accuracy: 0.5740 - val_loss: 1.5300 - val_accuracy: 0.4680\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1850 - accuracy: 0.5733 - val_loss: 1.5457 - val_accuracy: 0.4634\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.1855 - accuracy: 0.5734 - val_loss: 1.5428 - val_accuracy: 0.4728\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1797 - accuracy: 0.5746 - val_loss: 1.5761 - val_accuracy: 0.4614\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1713 - accuracy: 0.5836 - val_loss: 1.5743 - val_accuracy: 0.4698\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1561 - accuracy: 0.5829 - val_loss: 1.5756 - val_accuracy: 0.4594\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1585 - accuracy: 0.5848 - val_loss: 1.5495 - val_accuracy: 0.4768\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1457 - accuracy: 0.5872 - val_loss: 1.5429 - val_accuracy: 0.4676\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.1297 - accuracy: 0.5926 - val_loss: 1.5932 - val_accuracy: 0.4606\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1338 - accuracy: 0.5945 - val_loss: 1.6144 - val_accuracy: 0.4752\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1288 - accuracy: 0.5925 - val_loss: 1.5847 - val_accuracy: 0.4682\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.1201 - accuracy: 0.5992 - val_loss: 1.5864 - val_accuracy: 0.4710\n",
      "Epoch 58/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1081 - accuracy: 0.6017 - val_loss: 1.5750 - val_accuracy: 0.4708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83a0136160>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.5182 - accuracy: 0.4636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5181844234466553, 0.4636000096797943]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the lowest validation loss gets about 47.6% accuracy on the validation set. It took 27 epochs to reach the lowest validation loss, with roughly 8 seconds per epoch on my laptop (without a GPU). Let's see if we can improve performance using Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "*Exercise: Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is very similar to the code above, with a few changes:\n",
    "\n",
    "* I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.\n",
    "* I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.\n",
    "* I renamed the run directories to run_bn_* and the model file name to my_cifar10_bn_model.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 20s 10ms/step - loss: 1.9790 - accuracy: 0.2945 - val_loss: 1.6887 - val_accuracy: 0.3950\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6821 - accuracy: 0.4023 - val_loss: 1.5786 - val_accuracy: 0.4354\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6078 - accuracy: 0.4309 - val_loss: 1.5159 - val_accuracy: 0.4598\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5495 - accuracy: 0.4474 - val_loss: 1.5030 - val_accuracy: 0.4672\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4992 - accuracy: 0.4684 - val_loss: 1.4537 - val_accuracy: 0.4798\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4653 - accuracy: 0.4820 - val_loss: 1.4150 - val_accuracy: 0.4924\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4316 - accuracy: 0.4914 - val_loss: 1.4199 - val_accuracy: 0.4892\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4085 - accuracy: 0.5037 - val_loss: 1.3814 - val_accuracy: 0.5102\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3792 - accuracy: 0.5132 - val_loss: 1.3801 - val_accuracy: 0.5050\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3566 - accuracy: 0.5164 - val_loss: 1.3591 - val_accuracy: 0.5136\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.3242 - accuracy: 0.5334 - val_loss: 1.3368 - val_accuracy: 0.5320\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.3108 - accuracy: 0.5404 - val_loss: 1.3654 - val_accuracy: 0.5070\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2916 - accuracy: 0.5424 - val_loss: 1.4202 - val_accuracy: 0.4988\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2852 - accuracy: 0.5447 - val_loss: 1.3445 - val_accuracy: 0.5278\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2608 - accuracy: 0.5513 - val_loss: 1.3641 - val_accuracy: 0.5200\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2451 - accuracy: 0.5586 - val_loss: 1.3737 - val_accuracy: 0.5170\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2317 - accuracy: 0.5626 - val_loss: 1.3231 - val_accuracy: 0.5368\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2118 - accuracy: 0.5694 - val_loss: 1.3388 - val_accuracy: 0.5274\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1985 - accuracy: 0.5765 - val_loss: 1.3280 - val_accuracy: 0.5332\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1851 - accuracy: 0.5813 - val_loss: 1.3733 - val_accuracy: 0.5256\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1677 - accuracy: 0.5849 - val_loss: 1.3690 - val_accuracy: 0.5258\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1569 - accuracy: 0.5905 - val_loss: 1.3743 - val_accuracy: 0.5200\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1409 - accuracy: 0.5975 - val_loss: 1.3220 - val_accuracy: 0.5430\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1265 - accuracy: 0.6021 - val_loss: 1.3237 - val_accuracy: 0.5374\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1206 - accuracy: 0.6068 - val_loss: 1.3354 - val_accuracy: 0.5484\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1008 - accuracy: 0.6103 - val_loss: 1.3432 - val_accuracy: 0.5360\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0842 - accuracy: 0.6192 - val_loss: 1.3458 - val_accuracy: 0.5332\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0747 - accuracy: 0.6202 - val_loss: 1.3523 - val_accuracy: 0.5342\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0721 - accuracy: 0.6209 - val_loss: 1.3310 - val_accuracy: 0.5418\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0674 - accuracy: 0.6227 - val_loss: 1.3681 - val_accuracy: 0.5346\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0600 - accuracy: 0.6262 - val_loss: 1.3534 - val_accuracy: 0.5362\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0363 - accuracy: 0.6347 - val_loss: 1.3703 - val_accuracy: 0.5372\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0281 - accuracy: 0.6364 - val_loss: 1.3539 - val_accuracy: 0.5476\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0195 - accuracy: 0.6398 - val_loss: 1.3688 - val_accuracy: 0.5392\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0162 - accuracy: 0.6403 - val_loss: 1.3595 - val_accuracy: 0.5404\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0022 - accuracy: 0.6467 - val_loss: 1.3544 - val_accuracy: 0.5396\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 0.9883 - accuracy: 0.6558 - val_loss: 1.3639 - val_accuracy: 0.5408\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9800 - accuracy: 0.6537 - val_loss: 1.3861 - val_accuracy: 0.5404\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9727 - accuracy: 0.6557 - val_loss: 1.3921 - val_accuracy: 0.5350\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9590 - accuracy: 0.6634 - val_loss: 1.4103 - val_accuracy: 0.5374\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 0.9649 - accuracy: 0.6630 - val_loss: 1.3833 - val_accuracy: 0.5406\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9430 - accuracy: 0.6694 - val_loss: 1.3903 - val_accuracy: 0.5366\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 0.9456 - accuracy: 0.6707 - val_loss: 1.3861 - val_accuracy: 0.5398\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.3220 - accuracy: 0.5430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.322007656097412, 0.5429999828338623]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Is the model converging faster than before?* Much faster! The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs and continued to make progress until the 16th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
    "* *Does BN produce a better model?* Yes! The final model is also much better, with 54.0% accuracy instead of 47.6%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).\n",
    "* *How does BN affect training speed?* Although the model converged much faster, each epoch took about 12s instead of 8s, because of the extra computations required by the BN layers. But overall the training time (wall time) was shortened significantly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\n",
    "*Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 10s 6ms/step - loss: 2.0616 - accuracy: 0.2678 - val_loss: 1.8302 - val_accuracy: 0.3460\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.7424 - accuracy: 0.3778 - val_loss: 1.7084 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6368 - accuracy: 0.4272 - val_loss: 1.6648 - val_accuracy: 0.4070\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.5536 - accuracy: 0.4544 - val_loss: 1.6482 - val_accuracy: 0.4220\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5041 - accuracy: 0.4734 - val_loss: 1.5607 - val_accuracy: 0.4522\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4450 - accuracy: 0.4935 - val_loss: 1.5184 - val_accuracy: 0.4724\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3987 - accuracy: 0.5175 - val_loss: 1.5547 - val_accuracy: 0.4580\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3626 - accuracy: 0.5260 - val_loss: 1.4878 - val_accuracy: 0.4928\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3352 - accuracy: 0.5339 - val_loss: 1.5150 - val_accuracy: 0.4768\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.2997 - accuracy: 0.5468 - val_loss: 1.5272 - val_accuracy: 0.4872\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.2682 - accuracy: 0.5653 - val_loss: 1.4673 - val_accuracy: 0.4926\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.2354 - accuracy: 0.5767 - val_loss: 1.4922 - val_accuracy: 0.4894\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.2091 - accuracy: 0.5840 - val_loss: 1.4667 - val_accuracy: 0.5064\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1774 - accuracy: 0.5977 - val_loss: 1.4893 - val_accuracy: 0.5054\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.1718 - accuracy: 0.5981 - val_loss: 1.4996 - val_accuracy: 0.5012\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1436 - accuracy: 0.6082 - val_loss: 1.5178 - val_accuracy: 0.5136\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1240 - accuracy: 0.6166 - val_loss: 1.4862 - val_accuracy: 0.5152\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1022 - accuracy: 0.6245 - val_loss: 1.5141 - val_accuracy: 0.5168\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0707 - accuracy: 0.6360 - val_loss: 1.5581 - val_accuracy: 0.5062\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.0743 - accuracy: 0.6323 - val_loss: 1.4870 - val_accuracy: 0.5110\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0483 - accuracy: 0.6405 - val_loss: 1.5453 - val_accuracy: 0.4964\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.0301 - accuracy: 0.6472 - val_loss: 1.5682 - val_accuracy: 0.4982\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.0143 - accuracy: 0.6555 - val_loss: 1.6299 - val_accuracy: 0.4850\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0014 - accuracy: 0.6620 - val_loss: 1.5900 - val_accuracy: 0.5124\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.9898 - accuracy: 0.6675 - val_loss: 1.5580 - val_accuracy: 0.5102\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9509 - accuracy: 0.6787 - val_loss: 1.5821 - val_accuracy: 0.5134\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9482 - accuracy: 0.6793 - val_loss: 1.5332 - val_accuracy: 0.5032\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.9319 - accuracy: 0.6862 - val_loss: 1.5611 - val_accuracy: 0.5090\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9479 - accuracy: 0.6824 - val_loss: 1.6656 - val_accuracy: 0.5042\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9197 - accuracy: 0.6900 - val_loss: 1.5861 - val_accuracy: 0.5124\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.9139 - accuracy: 0.6889 - val_loss: 1.6278 - val_accuracy: 0.5158\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.8750 - accuracy: 0.7055 - val_loss: 1.6560 - val_accuracy: 0.5086\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8806 - accuracy: 0.7027 - val_loss: 1.6192 - val_accuracy: 0.5108\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4667 - accuracy: 0.5064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4666820764541626, 0.5063999891281128]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4667 - accuracy: 0.5064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4666820764541626, 0.5063999891281128]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 47.9% accuracy, which is not much better than the original model (47.6%), and not as good as the model using batch normalization (54.0%). However, convergence was almost as fast as with the BN model, plus each epoch took only 7 seconds. So it's by far the fastest model to train so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.\n",
    "*Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 11s 6ms/step - loss: 2.0537 - accuracy: 0.2808 - val_loss: 1.7096 - val_accuracy: 0.3914\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6718 - accuracy: 0.4087 - val_loss: 1.6353 - val_accuracy: 0.4188\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5808 - accuracy: 0.4463 - val_loss: 1.6741 - val_accuracy: 0.4158\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5018 - accuracy: 0.4681 - val_loss: 1.5885 - val_accuracy: 0.4582\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4421 - accuracy: 0.4940 - val_loss: 1.5328 - val_accuracy: 0.4760\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3911 - accuracy: 0.5129 - val_loss: 1.5277 - val_accuracy: 0.4816\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3489 - accuracy: 0.5275 - val_loss: 1.5603 - val_accuracy: 0.4684\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2998 - accuracy: 0.5462 - val_loss: 1.4986 - val_accuracy: 0.4912\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2733 - accuracy: 0.5526 - val_loss: 1.4833 - val_accuracy: 0.4962\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2348 - accuracy: 0.5693 - val_loss: 1.4834 - val_accuracy: 0.5048\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1984 - accuracy: 0.5859 - val_loss: 1.6173 - val_accuracy: 0.4884\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1783 - accuracy: 0.5930 - val_loss: 1.4862 - val_accuracy: 0.5028\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1492 - accuracy: 0.6019 - val_loss: 1.5518 - val_accuracy: 0.5022\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1021 - accuracy: 0.6201 - val_loss: 1.5177 - val_accuracy: 0.5042\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0972 - accuracy: 0.6190 - val_loss: 1.6022 - val_accuracy: 0.5108\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0732 - accuracy: 0.6293 - val_loss: 1.5834 - val_accuracy: 0.5138\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0523 - accuracy: 0.6348 - val_loss: 1.6553 - val_accuracy: 0.5148\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0259 - accuracy: 0.6478 - val_loss: 1.6424 - val_accuracy: 0.5038\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0068 - accuracy: 0.6538 - val_loss: 1.6738 - val_accuracy: 0.5070\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9792 - accuracy: 0.6623 - val_loss: 1.6853 - val_accuracy: 0.5028\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9619 - accuracy: 0.6714 - val_loss: 1.7348 - val_accuracy: 0.5066\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9375 - accuracy: 0.6780 - val_loss: 1.7678 - val_accuracy: 0.5074\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9200 - accuracy: 0.6902 - val_loss: 1.7528 - val_accuracy: 0.4966\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9024 - accuracy: 0.6899 - val_loss: 1.7280 - val_accuracy: 0.5020\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8904 - accuracy: 0.6973 - val_loss: 1.7279 - val_accuracy: 0.5030\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8692 - accuracy: 0.7073 - val_loss: 1.7150 - val_accuracy: 0.5090\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8453 - accuracy: 0.7119 - val_loss: 1.7321 - val_accuracy: 0.5162\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8288 - accuracy: 0.7213 - val_loss: 1.8248 - val_accuracy: 0.4946\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8338 - accuracy: 0.7199 - val_loss: 1.8727 - val_accuracy: 0.5098\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4833 - accuracy: 0.4962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.483300805091858, 0.49619999527931213]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches 48.9% accuracy on the validation set. That's very slightly better than without dropout (47.6%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. We will need the `MCAlphaDropout` class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with `MCAlphaDropout` dropout layers instead of `AlphaDropout` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.497"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get no accuracy improvement in this case (we're still at 48.9% accuracy).\n",
    "\n",
    "So the best model we got in this exercise is the Batch Normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.\n",
    "*Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 3s 6ms/step - loss: nan - accuracy: 0.1251          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.622215986251831,\n",
       " 3.939341306686402)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 3s 7ms/step - loss: 2.2398 - accuracy: 0.2309 - val_loss: 1.7721 - val_accuracy: 0.3778\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 2s 6ms/step - loss: 1.7947 - accuracy: 0.3664 - val_loss: 1.6650 - val_accuracy: 0.4178\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.6358 - accuracy: 0.4239 - val_loss: 1.6489 - val_accuracy: 0.4242\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.5402 - accuracy: 0.4557 - val_loss: 1.6302 - val_accuracy: 0.4398\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.4850 - accuracy: 0.4767 - val_loss: 1.6161 - val_accuracy: 0.4368\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 2s 6ms/step - loss: 1.4328 - accuracy: 0.4902 - val_loss: 1.5974 - val_accuracy: 0.4416\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.4013 - accuracy: 0.5013 - val_loss: 1.5453 - val_accuracy: 0.4590\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.3384 - accuracy: 0.5233 - val_loss: 1.4979 - val_accuracy: 0.4806\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.2679 - accuracy: 0.5475 - val_loss: 1.5281 - val_accuracy: 0.4862\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.1915 - accuracy: 0.5717 - val_loss: 1.5235 - val_accuracy: 0.4936\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 2s 6ms/step - loss: 1.1186 - accuracy: 0.6043 - val_loss: 1.5333 - val_accuracy: 0.4970\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 2s 6ms/step - loss: 1.0543 - accuracy: 0.6261 - val_loss: 1.4947 - val_accuracy: 0.5112\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 2s 6ms/step - loss: 0.9865 - accuracy: 0.6505 - val_loss: 1.5422 - val_accuracy: 0.5138\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 0.9128 - accuracy: 0.6747 - val_loss: 1.5649 - val_accuracy: 0.5188\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 0.8898 - accuracy: 0.6826 - val_loss: 1.5885 - val_accuracy: 0.5210\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 47.6% to 52.0%). The batch normalized model reaches a slightly better performance (54%), but it's much slower to train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
