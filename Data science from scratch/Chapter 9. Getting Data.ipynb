{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141c6b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\books\\\\python\\\\0.Data Science from Scratch- First Principles with Python',\n",
       " 'D:/books/python/0.Data Science from Scratch- First Principles with Python/data-science-from-scratch/scratch/',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\python38.zip',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\lidan\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\lib\\\\site-packages\\\\magic_impute-2.0.4-py3.8.egg',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\lib\\\\site-packages\\\\seqc-0.2.0-py3.8.egg',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\lib\\\\site-packages\\\\weasyprint-56.1-py3.8.egg',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\lib\\\\site-packages\\\\cairocffi-1.3.0-py3.8.egg',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\lidan\\\\miniconda3\\\\lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "    # caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, 'D:/books/python/0.Data Science from Scratch- First Principles with Python/data-science-from-scratch/scratch/')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6ddbe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import webbrowser\n",
    "from twython import Twython\n",
    "from twython import Twython\n",
    "import csv\n",
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "605ad204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Just stick some data there\n",
    "with open('email_addresses.txt', 'w') as f:\n",
    "    f.write(\"joelgrus@gmail.com\\n\")\n",
    "    f.write(\"joel@m.datasciencester.com\\n\")\n",
    "    f.write(\"joelgrus@m.datasciencester.com\\n\")\n",
    "\n",
    "def get_domain(email_address: str) -> str:\n",
    "    \"\"\"Split on '@' and return the last piece\"\"\"\n",
    "    return email_address.lower().split(\"@\")[-1]\n",
    "\n",
    "# a couple of tests\n",
    "assert get_domain('joelgrus@gmail.com') == 'gmail.com'\n",
    "assert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com'\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "with open('email_addresses.txt', 'r') as f:\n",
    "    domain_counts = Counter(get_domain(line.strip())\n",
    "                            for line in f\n",
    "                            if \"@\" in line)\n",
    "\n",
    "\n",
    "with open('tab_delimited_stock_prices.txt', 'w') as f:\n",
    "    f.write(\"\"\"6/20/2014\\tAAPL\\t90.91\n",
    "6/20/2014\\tMSFT\\t41.68\n",
    "6/20/2014\\tFB\\t64.5\n",
    "6/19/2014\\tAAPL\\t91.86\n",
    "6/19/2014\\tMSFT\\t41.51\n",
    "6/19/2014\\tFB\\t64.34\n",
    "\"\"\")\n",
    "\n",
    "def process(date: str, symbol: str, closing_price: float) -> None:\n",
    "    # Imaginge that this function actually does something.\n",
    "    assert closing_price > 0.0\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('tab_delimited_stock_prices.txt') as f:\n",
    "    tab_reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in tab_reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        process(date, symbol, closing_price)\n",
    "\n",
    "\n",
    "with open('colon_delimited_stock_prices.txt', 'w') as f:\n",
    "    f.write(\"\"\"date:symbol:closing_price\n",
    "6/20/2014:AAPL:90.91\n",
    "6/20/2014:MSFT:41.68\n",
    "6/20/2014:FB:64.5\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "with open('colon_delimited_stock_prices.txt') as f:\n",
    "    colon_reader = csv.DictReader(f, delimiter=':')\n",
    "    for dict_row in colon_reader:\n",
    "        date = dict_row[\"date\"]\n",
    "        symbol = dict_row[\"symbol\"]\n",
    "        closing_price = float(dict_row[\"closing_price\"])\n",
    "        process(date, symbol, closing_price)\n",
    "\n",
    "todays_prices = {'AAPL': 90.91, 'MSFT': 41.68, 'FB': 64.5 }\n",
    "\n",
    "with open('comma_delimited_stock_prices.txt', 'w') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',')\n",
    "    for stock, price in todays_prices.items():\n",
    "        csv_writer.writerow([stock, price])\n",
    "\n",
    "results = [[\"test1\", \"success\", \"Monday\"],\n",
    "           [\"test2\", \"success, kind of\", \"Tuesday\"],\n",
    "           [\"test3\", \"failure, kind of\", \"Wednesday\"],\n",
    "           [\"test4\", \"failure, utter\", \"Thursday\"]]\n",
    "\n",
    "# don't do this!\n",
    "with open('bad_csv.txt', 'w') as f:\n",
    "    for row in results:\n",
    "        f.write(\",\".join(map(str, row))) # might have too many commas in it!\n",
    "        f.write(\"\\n\")                    # row might have newlines as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed9fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test1', 'success', 'Monday']\n",
      "['test2', 'success, kind of', 'Tuesday']\n",
      "['test3', 'failure, kind of', 'Wednesday']\n",
      "['test4', 'failure, utter', 'Thursday']\n"
     ]
    }
   ],
   "source": [
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5581bebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test1', 'success', 'Monday']\n",
      "['test2', 'success, kind of', 'Tuesday']\n",
      "['test3', 'failure, kind of', 'Wednesday']\n",
      "['test4', 'failure, utter', 'Thursday']\n"
     ]
    }
   ],
   "source": [
    "for row in results:\n",
    "    print(str(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "326aeb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1,success,Monday\n",
      "test2,success, kind of,Tuesday\n",
      "test3,failure, kind of,Wednesday\n",
      "test4,failure, utter,Thursday\n"
     ]
    }
   ],
   "source": [
    "for row in results:\n",
    "    print(\",\".join(map(str, row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "267467ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "760a0dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html>\\n<html lang=\"en-US\">\\n<head>\\n    <title>Getting Data</title>\\n    <meta charset=\"utf-8\">\\n</head>\\n<body>\\n    <h1>Getting Data</h1>\\n    <div class=\"explanation\">\\n        This is an explanation.\\n    </div>\\n    <div class=\"comment\">\\n        This is a comment.\\n    </div>\\n    <div class=\"content\">\\n        <p id=\"p1\">This is the first paragraph.</p>\\n        <p class=\"important\">This is the second paragraph.</p>\\n    </div>\\n    <div class=\"signature\">\\n        <span id=\"name\">Joel</span>\\n        <span id=\"twitter\">@joelgrus</span>\\n        <span id=\"email\">joelgrus-at-gmail</span>\\n    </div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I put the relevant HTML file on GitHub. In order to fit\n",
    "# the URL in the book I had to split it across two lines.\n",
    "# Recall that whitespace-separated strings get concatenated.\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/\"\n",
    "       \"joelgrus/data/master/getting-data.html\")\n",
    "html = requests.get(url).text\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0dbdd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"p1\">This is the first paragraph.</p>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "first_paragraph = soup.find('p')        # or just soup.p\n",
    "first_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3829426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert str(soup.find('p')) == '<p id=\"p1\">This is the first paragraph.</p>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1236e135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'the', 'first', 'paragraph.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "\n",
    "\n",
    "assert first_paragraph_words == ['This', 'is', 'the', 'first', 'paragraph.']\n",
    "first_paragraph_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4aed762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the first paragraph.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_paragraph_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e894876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_paragraph_id = soup.p['id']       # raises KeyError if no 'id'\n",
    "first_paragraph_id2 = soup.p.get('id')  # returns None if no 'id'\n",
    "\n",
    "\n",
    "assert first_paragraph_id == first_paragraph_id2 == 'p1'\n",
    "first_paragraph_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22f4f266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"p1\">This is the first paragraph.</p>,\n",
       " <p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27082cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"p1\">This is the first paragraph.</p>,\n",
       " <p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_paragraphs = soup.find_all('p')  # or just soup('p')\n",
    "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
    "all_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97840ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"p1\">This is the first paragraph.</p>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_with_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b9ce8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_paragraphs) == 2\n",
    "assert len(paragraphs_with_ids) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "532428f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"p1\">This is the first paragraph.</p>,\n",
       " <p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in soup('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1227b538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_paragraphs = soup('p', {'class' : 'important'})\n",
    "important_paragraphs2 = soup('p', 'important')\n",
    "important_paragraphs3 = [p for p in soup('p')\n",
    "                         if 'important' in p.get('class', [])]\n",
    "important_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2081dcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_paragraphs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a47b849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_paragraphs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b196402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert important_paragraphs == important_paragraphs2 == important_paragraphs3\n",
    "assert len(important_paragraphs) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4925916a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"explanation\">\n",
       "         This is an explanation.\n",
       "     </div>,\n",
       " <div class=\"comment\">\n",
       "         This is a comment.\n",
       "     </div>,\n",
       " <div class=\"content\">\n",
       " <p id=\"p1\">This is the first paragraph.</p>\n",
       " <p class=\"important\">This is the second paragraph.</p>\n",
       " </div>,\n",
       " <div class=\"signature\">\n",
       " <span id=\"name\">Joel</span>\n",
       " <span id=\"twitter\">@joelgrus</span>\n",
       " <span id=\"email\">joelgrus-at-gmail</span>\n",
       " </div>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64696211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span id=\"name\">Joel</span>,\n",
       " <span id=\"twitter\">@joelgrus</span>,\n",
       " <span id=\"email\">joelgrus-at-gmail</span>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# warning, will return the same span multiple times\n",
    "# if it sits inside multiple divs\n",
    "# be more clever if that's the case\n",
    "spans_inside_divs = [span\n",
    "                     for div in soup('div')     # for each <div> on the page\n",
    "                     for span in div('span')]   # find each <span> inside it\n",
    "spans_inside_divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe9e4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(spans_inside_divs) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61dc8d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>Twitter</p>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\"\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "soup('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d1ac74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>Facebook</h1>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36a941f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<body><h1>Facebook</h1><p>Twitter</p></body>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup('body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "edcf4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_mentions(text: str, keyword: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if a <p> inside the text mentions {keyword}\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    paragraphs = [p.get_text() for p in soup('p')]\n",
    "\n",
    "    return any(keyword.lower() in paragraph.lower()\n",
    "               for paragraph in paragraphs)\n",
    "\n",
    "text = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\"\n",
    "assert paragraph_mentions(text, \"twitter\")       # is inside a <p>\n",
    "assert not paragraph_mentions(text, \"facebook\")  # not inside a <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7326f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_mentions(text, \"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47f5b5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_mentions(text, \"facebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4180aafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Data Science Book',\n",
       " 'author': 'Joel Grus',\n",
       " 'publicationYear': 2019,\n",
       " 'topics': ['data', 'science', 'data science']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ \"title\" : \"Data Science Book\",\n",
    "  \"author\" : \"Joel Grus\",\n",
    "  \"publicationYear\" : 2019,\n",
    "  \"topics\" : [ \"data\", \"science\", \"data science\"] }\n",
    "\n",
    "import json\n",
    "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                  \"author\" : \"Joel Grus\",\n",
    "                  \"publicationYear\" : 2019,\n",
    "                  \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "\n",
    "# parse the JSON to create a Python dict\n",
    "deserialized = json.loads(serialized)\n",
    "deserialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf9dfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert deserialized[\"publicationYear\"] == 2019\n",
    "assert \"data science\" in deserialized[\"topics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ec6564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "    \n",
    "url = \"https://www.house.gov/representatives\"\n",
    "text = requests.get(url).text\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "    \n",
    "all_urls = [a['href'] for a in soup('a') if a.has_attr('href')]\n",
    "    \n",
    "print(len(all_urls))  # 965 for me, way too many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0cb66d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"visually-hidden focusable skip-link\" href=\"#main-content\">\n",
       "       Skip to main content\n",
       "     </a>,\n",
       " <a class=\"logo navbar-btn pull-left\" href=\"/\" rel=\"home\" title=\"Home\">\n",
       " <img alt=\"Home\" src=\"/sites/default/themes/housegov/logo.svg\"/>\n",
       " </a>,\n",
       " <a class=\"name navbar-brand\" href=\"/\" rel=\"home\" title=\"Home\">house.gov</a>,\n",
       " <a class=\"is-active\" data-drupal-link-system-path=\"representatives\" href=\"/representatives\">Representatives</a>,\n",
       " <a data-drupal-link-system-path=\"node/3\" href=\"/leadership\">Leadership</a>,\n",
       " <a data-drupal-link-system-path=\"node/4\" href=\"/committees\">Committees</a>,\n",
       " <a data-drupal-link-system-path=\"legislative-activity\" href=\"/legislative-activity\">Legislative Activity</a>,\n",
       " <a data-drupal-link-system-path=\"node/24\" href=\"/the-house-explained\" title=\"The House Explained\">The House Explained</a>,\n",
       " <a data-drupal-link-system-path=\"node/5\" href=\"/visitors\" title=\"Visitors\">Visitors</a>,\n",
       " <a data-drupal-link-system-path=\"node/6\" href=\"/educators-and-students\">Educators and Students</a>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup('a')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec42e68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#main-content',\n",
       " '/',\n",
       " '/',\n",
       " '/representatives',\n",
       " '/leadership',\n",
       " '/committees',\n",
       " '/legislative-activity',\n",
       " '/the-house-explained',\n",
       " '/visitors',\n",
       " '/educators-and-students']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7df5377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n",
      "876\n",
      "438\n",
      "{'https://jayapal.house.gov/category/news/', 'https://jayapal.house.gov/category/press-releases/'}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    \n",
    "    url = \"https://www.house.gov/representatives\"\n",
    "    text = requests.get(url).text\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    \n",
    "    all_urls = [a['href']\n",
    "                for a in soup('a')\n",
    "                if a.has_attr('href')]\n",
    "    \n",
    "    print(len(all_urls))  # 965 for me, way too many\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Must start with http:// or https://\n",
    "    # Must end with .house.gov or .house.gov/\n",
    "    regex = r\"^https?://.*\\.house\\.gov/?$\"\n",
    "    \n",
    "    # Let's write some tests!\n",
    "    assert re.match(regex, \"http://joel.house.gov\")\n",
    "    assert re.match(regex, \"https://joel.house.gov\")\n",
    "    assert re.match(regex, \"http://joel.house.gov/\")\n",
    "    assert re.match(regex, \"https://joel.house.gov/\")\n",
    "    assert not re.match(regex, \"joel.house.gov\")\n",
    "    assert not re.match(regex, \"http://joel.house.com\")\n",
    "    assert not re.match(regex, \"https://joel.house.gov/biography\")\n",
    "    \n",
    "    # And now apply\n",
    "    good_urls = [url for url in all_urls if re.match(regex, url)]\n",
    "    \n",
    "    print(len(good_urls))  # still 862 for me\n",
    "    \n",
    "    \n",
    "    num_original_good_urls = len(good_urls)\n",
    "    \n",
    "    good_urls = list(set(good_urls))\n",
    "    \n",
    "    print(len(good_urls))  # only 431 for me\n",
    "    \n",
    "    \n",
    "    assert len(good_urls) < num_original_good_urls\n",
    "    \n",
    "    html = requests.get('https://jayapal.house.gov').text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Use a set because the links might appear multiple times.\n",
    "    links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "    \n",
    "    print(links) # {'/media/press-releases'}\n",
    "    \n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "03532269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after sampling, left with ['https://desaulnier.house.gov/', 'https://baird.house.gov/', 'https://troycarter.house.gov', 'https://jordan.house.gov/', 'https://mikethompson.house.gov/', 'https://bobbyscott.house.gov', 'https://stansbury.house.gov/', 'https://womack.house.gov/', 'https://steube.house.gov/', 'https://correa.house.gov', 'https://bera.house.gov', 'https://chuygarcia.house.gov/', 'https://panetta.house.gov', 'https://chu.house.gov/', 'https://yakym.house.gov/', 'https://cohen.house.gov/', 'https://crawford.house.gov/', 'https://fischbach.house.gov', 'https://cleaver.house.gov', 'https://bucshon.house.gov/']\n",
      "https://desaulnier.house.gov/: {'/media-center/press-releases'}\n",
      "https://baird.house.gov/: {'/news/documentquery.aspx?DocumentTypeID=27'}\n",
      "https://troycarter.house.gov: {'/media/press-releases'}\n",
      "https://jordan.house.gov/: {'/media/press-releases'}\n",
      "https://mikethompson.house.gov/: {'/newsroom/press-releases'}\n",
      "https://bobbyscott.house.gov: {'/media-center/press-releases'}\n",
      "https://stansbury.house.gov/: {'/media/press-releases'}\n",
      "https://womack.house.gov/: {'/News/DocumentQuery.aspx?DocumentTypeID=2067'}\n",
      "https://steube.house.gov/: {'/media/press-releases'}\n",
      "https://correa.house.gov: {'/press/table'}\n",
      "https://bera.house.gov: {'/media-center/press-releases'}\n",
      "https://chuygarcia.house.gov/: {'/media/press-releases'}\n",
      "https://panetta.house.gov: {'/media/press-releases'}\n",
      "https://chu.house.gov/: {'/media-center/press-releases'}\n",
      "https://yakym.house.gov/: {'/media/press-releases'}\n",
      "https://cohen.house.gov/: {'/media-center/press-releases'}\n",
      "https://crawford.house.gov/: set()\n",
      "https://fischbach.house.gov: {'/press-releases'}\n",
      "https://cleaver.house.gov: {'/media-center/press-releases'}\n",
      "https://bucshon.house.gov/: {'/news/documentquery.aspx?DocumentTypeID=27'}\n",
      "https://cohen.house.gov/\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    url = \"https://www.house.gov/representatives\"\n",
    "    text = requests.get(url).text\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    \n",
    "    all_urls = [a['href']\n",
    "                for a in soup('a')\n",
    "                if a.has_attr('href')]\n",
    "    \n",
    "    #print(len(all_urls))  # 965 for me, way too many\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Must start with http:// or https://\n",
    "    # Must end with .house.gov or .house.gov/\n",
    "    regex = r\"^https?://.*\\.house\\.gov/?$\"\n",
    "    \n",
    "    # Let's write some tests!\n",
    "    assert re.match(regex, \"http://joel.house.gov\")\n",
    "    assert re.match(regex, \"https://joel.house.gov\")\n",
    "    assert re.match(regex, \"http://joel.house.gov/\")\n",
    "    assert re.match(regex, \"https://joel.house.gov/\")\n",
    "    assert not re.match(regex, \"joel.house.gov\")\n",
    "    assert not re.match(regex, \"http://joel.house.com\")\n",
    "    assert not re.match(regex, \"https://joel.house.gov/biography\")\n",
    "    \n",
    "    # And now apply\n",
    "    good_urls = [url for url in all_urls if re.match(regex, url)]\n",
    "    \n",
    "    #print(len(good_urls))  # still 862 for me\n",
    "    \n",
    "    \n",
    "    num_original_good_urls = len(good_urls)\n",
    "    \n",
    "    good_urls = list(set(good_urls))\n",
    "    \n",
    "    #print(len(good_urls))  # only 431 for me\n",
    "    \n",
    "    \n",
    "    assert len(good_urls) < num_original_good_urls\n",
    "    \n",
    "    html = requests.get('https://jayapal.house.gov').text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Use a set because the links might appear multiple times.\n",
    "    links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "    \n",
    "    #print(links) # {'/media/press-releases'}\n",
    "    # I don't want this file to scrape all 400+ websites every time it runs.\n",
    "    # So I'm going to randomly throw out most of the urls.\n",
    "    # The code in the book doesn't do this.\n",
    "    import random\n",
    "    good_urls = random.sample(good_urls, 20)\n",
    "    print(f\"after sampling, left with {good_urls}\")\n",
    "    \n",
    "    from typing import Dict, Set\n",
    "    \n",
    "    press_releases: Dict[str, Set[str]] = {}\n",
    "    \n",
    "    for house_url in good_urls:\n",
    "        html = requests.get(house_url).text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        pr_links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "        print(f\"{house_url}: {pr_links}\")\n",
    "        press_releases[house_url] = pr_links\n",
    "    for house_url, pr_links in press_releases.items():\n",
    "        for pr_link in pr_links:\n",
    "            url = f\"{house_url}/{pr_link}\"\n",
    "            text = requests.get(url).text\n",
    "    \n",
    "            if paragraph_mentions(text, 'data'):\n",
    "                print(f\"{house_url}\")\n",
    "                break  # done with this house_url\n",
    "        \n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58fabae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 112873601,\n",
       " 'node_id': 'MDEwOlJlcG9zaXRvcnkxMTI4NzM2MDE=',\n",
       " 'name': 'advent2017',\n",
       " 'full_name': 'joelgrus/advent2017',\n",
       " 'private': False,\n",
       " 'owner': {'login': 'joelgrus',\n",
       "  'id': 1308313,\n",
       "  'node_id': 'MDQ6VXNlcjEzMDgzMTM=',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/1308313?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/joelgrus',\n",
       "  'html_url': 'https://github.com/joelgrus',\n",
       "  'followers_url': 'https://api.github.com/users/joelgrus/followers',\n",
       "  'following_url': 'https://api.github.com/users/joelgrus/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/joelgrus/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/joelgrus/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/joelgrus/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/joelgrus/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/joelgrus/repos',\n",
       "  'events_url': 'https://api.github.com/users/joelgrus/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/joelgrus/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'html_url': 'https://github.com/joelgrus/advent2017',\n",
       " 'description': 'advent of code 2017',\n",
       " 'fork': False,\n",
       " 'url': 'https://api.github.com/repos/joelgrus/advent2017',\n",
       " 'forks_url': 'https://api.github.com/repos/joelgrus/advent2017/forks',\n",
       " 'keys_url': 'https://api.github.com/repos/joelgrus/advent2017/keys{/key_id}',\n",
       " 'collaborators_url': 'https://api.github.com/repos/joelgrus/advent2017/collaborators{/collaborator}',\n",
       " 'teams_url': 'https://api.github.com/repos/joelgrus/advent2017/teams',\n",
       " 'hooks_url': 'https://api.github.com/repos/joelgrus/advent2017/hooks',\n",
       " 'issue_events_url': 'https://api.github.com/repos/joelgrus/advent2017/issues/events{/number}',\n",
       " 'events_url': 'https://api.github.com/repos/joelgrus/advent2017/events',\n",
       " 'assignees_url': 'https://api.github.com/repos/joelgrus/advent2017/assignees{/user}',\n",
       " 'branches_url': 'https://api.github.com/repos/joelgrus/advent2017/branches{/branch}',\n",
       " 'tags_url': 'https://api.github.com/repos/joelgrus/advent2017/tags',\n",
       " 'blobs_url': 'https://api.github.com/repos/joelgrus/advent2017/git/blobs{/sha}',\n",
       " 'git_tags_url': 'https://api.github.com/repos/joelgrus/advent2017/git/tags{/sha}',\n",
       " 'git_refs_url': 'https://api.github.com/repos/joelgrus/advent2017/git/refs{/sha}',\n",
       " 'trees_url': 'https://api.github.com/repos/joelgrus/advent2017/git/trees{/sha}',\n",
       " 'statuses_url': 'https://api.github.com/repos/joelgrus/advent2017/statuses/{sha}',\n",
       " 'languages_url': 'https://api.github.com/repos/joelgrus/advent2017/languages',\n",
       " 'stargazers_url': 'https://api.github.com/repos/joelgrus/advent2017/stargazers',\n",
       " 'contributors_url': 'https://api.github.com/repos/joelgrus/advent2017/contributors',\n",
       " 'subscribers_url': 'https://api.github.com/repos/joelgrus/advent2017/subscribers',\n",
       " 'subscription_url': 'https://api.github.com/repos/joelgrus/advent2017/subscription',\n",
       " 'commits_url': 'https://api.github.com/repos/joelgrus/advent2017/commits{/sha}',\n",
       " 'git_commits_url': 'https://api.github.com/repos/joelgrus/advent2017/git/commits{/sha}',\n",
       " 'comments_url': 'https://api.github.com/repos/joelgrus/advent2017/comments{/number}',\n",
       " 'issue_comment_url': 'https://api.github.com/repos/joelgrus/advent2017/issues/comments{/number}',\n",
       " 'contents_url': 'https://api.github.com/repos/joelgrus/advent2017/contents/{+path}',\n",
       " 'compare_url': 'https://api.github.com/repos/joelgrus/advent2017/compare/{base}...{head}',\n",
       " 'merges_url': 'https://api.github.com/repos/joelgrus/advent2017/merges',\n",
       " 'archive_url': 'https://api.github.com/repos/joelgrus/advent2017/{archive_format}{/ref}',\n",
       " 'downloads_url': 'https://api.github.com/repos/joelgrus/advent2017/downloads',\n",
       " 'issues_url': 'https://api.github.com/repos/joelgrus/advent2017/issues{/number}',\n",
       " 'pulls_url': 'https://api.github.com/repos/joelgrus/advent2017/pulls{/number}',\n",
       " 'milestones_url': 'https://api.github.com/repos/joelgrus/advent2017/milestones{/number}',\n",
       " 'notifications_url': 'https://api.github.com/repos/joelgrus/advent2017/notifications{?since,all,participating}',\n",
       " 'labels_url': 'https://api.github.com/repos/joelgrus/advent2017/labels{/name}',\n",
       " 'releases_url': 'https://api.github.com/repos/joelgrus/advent2017/releases{/id}',\n",
       " 'deployments_url': 'https://api.github.com/repos/joelgrus/advent2017/deployments',\n",
       " 'created_at': '2017-12-02T20:13:49Z',\n",
       " 'updated_at': '2022-05-02T13:59:29Z',\n",
       " 'pushed_at': '2017-12-26T15:02:27Z',\n",
       " 'git_url': 'git://github.com/joelgrus/advent2017.git',\n",
       " 'ssh_url': 'git@github.com:joelgrus/advent2017.git',\n",
       " 'clone_url': 'https://github.com/joelgrus/advent2017.git',\n",
       " 'svn_url': 'https://github.com/joelgrus/advent2017',\n",
       " 'homepage': None,\n",
       " 'size': 156,\n",
       " 'stargazers_count': 8,\n",
       " 'watchers_count': 8,\n",
       " 'language': 'Python',\n",
       " 'has_issues': True,\n",
       " 'has_projects': True,\n",
       " 'has_downloads': True,\n",
       " 'has_wiki': True,\n",
       " 'has_pages': False,\n",
       " 'has_discussions': False,\n",
       " 'forks_count': 2,\n",
       " 'mirror_url': None,\n",
       " 'archived': False,\n",
       " 'disabled': False,\n",
       " 'open_issues_count': 0,\n",
       " 'license': None,\n",
       " 'allow_forking': True,\n",
       " 'is_template': False,\n",
       " 'web_commit_signoff_required': False,\n",
       " 'topics': [],\n",
       " 'visibility': 'public',\n",
       " 'forks': 2,\n",
       " 'open_issues': 0,\n",
       " 'watchers': 8,\n",
       " 'default_branch': 'master'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json\n",
    "    \n",
    "github_user = \"joelgrus\"\n",
    "endpoint = f\"https://api.github.com/users/{github_user}/repos\"\n",
    "    \n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "repos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4dd0b3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'Python', 'JavaScript', 'Python', 'Python']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from dateutil.parser import parse\n",
    "    \n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "    \n",
    "last_5_repositories = sorted(repos, key=lambda r: r[\"pushed_at\"],\n",
    "                             reverse=True)[:5]\n",
    "    \n",
    "last_5_languages = [repo[\"language\"] for repo in last_5_repositories]\n",
    "last_5_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b850646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2017, 12, 2, 20, 13, 49, tzinfo=tzutc()),\n",
       " datetime.datetime(2018, 11, 30, 22, 41, 16, tzinfo=tzutc()),\n",
       " datetime.datetime(2019, 12, 1, 2, 57, 18, tzinfo=tzutc()),\n",
       " datetime.datetime(2020, 11, 21, 16, 21, 49, tzinfo=tzutc()),\n",
       " datetime.datetime(2021, 11, 24, 13, 53, 23, tzinfo=tzutc()),\n",
       " datetime.datetime(2022, 11, 22, 2, 25, 22, tzinfo=tzutc()),\n",
       " datetime.datetime(2018, 2, 23, 15, 51, 4, tzinfo=tzutc()),\n",
       " datetime.datetime(2017, 12, 19, 0, 12, 40, tzinfo=tzutc()),\n",
       " datetime.datetime(2018, 1, 31, 23, 51, 16, tzinfo=tzutc()),\n",
       " datetime.datetime(2018, 12, 19, 19, 44, 45, tzinfo=tzutc()),\n",
       " datetime.datetime(2018, 9, 5, 2, 43, 52, tzinfo=tzutc()),\n",
       " datetime.datetime(2019, 2, 1, 20, 25, 46, tzinfo=tzutc()),\n",
       " datetime.datetime(2013, 7, 5, 2, 2, 28, tzinfo=tzutc()),\n",
       " datetime.datetime(2017, 5, 10, 17, 22, 45, tzinfo=tzutc()),\n",
       " datetime.datetime(2013, 11, 15, 5, 33, 22, tzinfo=tzutc()),\n",
       " datetime.datetime(2012, 9, 18, 4, 20, 23, tzinfo=tzutc()),\n",
       " datetime.datetime(2016, 7, 19, 17, 34, 31, tzinfo=tzutc()),\n",
       " datetime.datetime(2015, 11, 11, 14, 15, 36, tzinfo=tzutc()),\n",
       " datetime.datetime(2016, 5, 31, 14, 33, 6, tzinfo=tzutc()),\n",
       " datetime.datetime(2018, 9, 22, 15, 23, 50, tzinfo=tzutc()),\n",
       " datetime.datetime(2015, 6, 30, 0, 33, 3, tzinfo=tzutc()),\n",
       " datetime.datetime(2020, 9, 26, 14, 19, 22, tzinfo=tzutc()),\n",
       " datetime.datetime(2013, 8, 21, 13, 26, 5, tzinfo=tzutc()),\n",
       " datetime.datetime(2013, 8, 18, 5, 3, 41, tzinfo=tzutc()),\n",
       " datetime.datetime(2015, 7, 30, 1, 54, 55, tzinfo=tzutc()),\n",
       " datetime.datetime(2018, 9, 22, 17, 30, 59, tzinfo=tzutc()),\n",
       " datetime.datetime(2014, 11, 9, 2, 31, 24, tzinfo=tzutc()),\n",
       " datetime.datetime(2013, 11, 10, 6, 52, 56, tzinfo=tzutc()),\n",
       " datetime.datetime(2015, 4, 8, 1, 1, 47, tzinfo=tzutc()),\n",
       " datetime.datetime(2016, 1, 8, 3, 33, 58, tzinfo=tzutc())]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b968d687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.time(20, 13, 49),\n",
       " datetime.time(22, 41, 16),\n",
       " datetime.time(2, 57, 18),\n",
       " datetime.time(16, 21, 49),\n",
       " datetime.time(13, 53, 23),\n",
       " datetime.time(2, 25, 22),\n",
       " datetime.time(15, 51, 4),\n",
       " datetime.time(0, 12, 40),\n",
       " datetime.time(23, 51, 16),\n",
       " datetime.time(19, 44, 45),\n",
       " datetime.time(2, 43, 52),\n",
       " datetime.time(20, 25, 46),\n",
       " datetime.time(2, 2, 28),\n",
       " datetime.time(17, 22, 45),\n",
       " datetime.time(5, 33, 22),\n",
       " datetime.time(4, 20, 23),\n",
       " datetime.time(17, 34, 31),\n",
       " datetime.time(14, 15, 36),\n",
       " datetime.time(14, 33, 6),\n",
       " datetime.time(15, 23, 50),\n",
       " datetime.time(0, 33, 3),\n",
       " datetime.time(14, 19, 22),\n",
       " datetime.time(13, 26, 5),\n",
       " datetime.time(5, 3, 41),\n",
       " datetime.time(1, 54, 55),\n",
       " datetime.time(17, 30, 59),\n",
       " datetime.time(2, 31, 24),\n",
       " datetime.time(6, 52, 56),\n",
       " datetime.time(1, 1, 47),\n",
       " datetime.time(3, 33, 58)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[date.time() for date in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ddb20fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 2),\n",
       " (11, 30),\n",
       " (12, 1),\n",
       " (11, 21),\n",
       " (11, 24),\n",
       " (11, 22),\n",
       " (2, 23),\n",
       " (12, 19),\n",
       " (1, 31),\n",
       " (12, 19),\n",
       " (9, 5),\n",
       " (2, 1),\n",
       " (7, 5),\n",
       " (5, 10),\n",
       " (11, 15),\n",
       " (9, 18),\n",
       " (7, 19),\n",
       " (11, 11),\n",
       " (5, 31),\n",
       " (9, 22),\n",
       " (6, 30),\n",
       " (9, 26),\n",
       " (8, 21),\n",
       " (8, 18),\n",
       " (7, 30),\n",
       " (9, 22),\n",
       " (11, 9),\n",
       " (11, 10),\n",
       " (4, 8),\n",
       " (1, 8)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(date.month, date.day) for date in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a312012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({12: 4, 11: 8, 2: 2, 1: 2, 9: 5, 7: 3, 5: 2, 6: 1, 8: 2, 4: 1})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7834d521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([5, 4, 6, 2, 1, 3])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekday_counts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebb0494c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_5_repositories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b52c19b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 569069856,\n",
       " 'node_id': 'R_kgDOIetRIA',\n",
       " 'name': 'advent2022',\n",
       " 'full_name': 'joelgrus/advent2022',\n",
       " 'private': False,\n",
       " 'owner': {'login': 'joelgrus',\n",
       "  'id': 1308313,\n",
       "  'node_id': 'MDQ6VXNlcjEzMDgzMTM=',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/1308313?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/joelgrus',\n",
       "  'html_url': 'https://github.com/joelgrus',\n",
       "  'followers_url': 'https://api.github.com/users/joelgrus/followers',\n",
       "  'following_url': 'https://api.github.com/users/joelgrus/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/joelgrus/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/joelgrus/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/joelgrus/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/joelgrus/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/joelgrus/repos',\n",
       "  'events_url': 'https://api.github.com/users/joelgrus/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/joelgrus/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'html_url': 'https://github.com/joelgrus/advent2022',\n",
       " 'description': None,\n",
       " 'fork': False,\n",
       " 'url': 'https://api.github.com/repos/joelgrus/advent2022',\n",
       " 'forks_url': 'https://api.github.com/repos/joelgrus/advent2022/forks',\n",
       " 'keys_url': 'https://api.github.com/repos/joelgrus/advent2022/keys{/key_id}',\n",
       " 'collaborators_url': 'https://api.github.com/repos/joelgrus/advent2022/collaborators{/collaborator}',\n",
       " 'teams_url': 'https://api.github.com/repos/joelgrus/advent2022/teams',\n",
       " 'hooks_url': 'https://api.github.com/repos/joelgrus/advent2022/hooks',\n",
       " 'issue_events_url': 'https://api.github.com/repos/joelgrus/advent2022/issues/events{/number}',\n",
       " 'events_url': 'https://api.github.com/repos/joelgrus/advent2022/events',\n",
       " 'assignees_url': 'https://api.github.com/repos/joelgrus/advent2022/assignees{/user}',\n",
       " 'branches_url': 'https://api.github.com/repos/joelgrus/advent2022/branches{/branch}',\n",
       " 'tags_url': 'https://api.github.com/repos/joelgrus/advent2022/tags',\n",
       " 'blobs_url': 'https://api.github.com/repos/joelgrus/advent2022/git/blobs{/sha}',\n",
       " 'git_tags_url': 'https://api.github.com/repos/joelgrus/advent2022/git/tags{/sha}',\n",
       " 'git_refs_url': 'https://api.github.com/repos/joelgrus/advent2022/git/refs{/sha}',\n",
       " 'trees_url': 'https://api.github.com/repos/joelgrus/advent2022/git/trees{/sha}',\n",
       " 'statuses_url': 'https://api.github.com/repos/joelgrus/advent2022/statuses/{sha}',\n",
       " 'languages_url': 'https://api.github.com/repos/joelgrus/advent2022/languages',\n",
       " 'stargazers_url': 'https://api.github.com/repos/joelgrus/advent2022/stargazers',\n",
       " 'contributors_url': 'https://api.github.com/repos/joelgrus/advent2022/contributors',\n",
       " 'subscribers_url': 'https://api.github.com/repos/joelgrus/advent2022/subscribers',\n",
       " 'subscription_url': 'https://api.github.com/repos/joelgrus/advent2022/subscription',\n",
       " 'commits_url': 'https://api.github.com/repos/joelgrus/advent2022/commits{/sha}',\n",
       " 'git_commits_url': 'https://api.github.com/repos/joelgrus/advent2022/git/commits{/sha}',\n",
       " 'comments_url': 'https://api.github.com/repos/joelgrus/advent2022/comments{/number}',\n",
       " 'issue_comment_url': 'https://api.github.com/repos/joelgrus/advent2022/issues/comments{/number}',\n",
       " 'contents_url': 'https://api.github.com/repos/joelgrus/advent2022/contents/{+path}',\n",
       " 'compare_url': 'https://api.github.com/repos/joelgrus/advent2022/compare/{base}...{head}',\n",
       " 'merges_url': 'https://api.github.com/repos/joelgrus/advent2022/merges',\n",
       " 'archive_url': 'https://api.github.com/repos/joelgrus/advent2022/{archive_format}{/ref}',\n",
       " 'downloads_url': 'https://api.github.com/repos/joelgrus/advent2022/downloads',\n",
       " 'issues_url': 'https://api.github.com/repos/joelgrus/advent2022/issues{/number}',\n",
       " 'pulls_url': 'https://api.github.com/repos/joelgrus/advent2022/pulls{/number}',\n",
       " 'milestones_url': 'https://api.github.com/repos/joelgrus/advent2022/milestones{/number}',\n",
       " 'notifications_url': 'https://api.github.com/repos/joelgrus/advent2022/notifications{?since,all,participating}',\n",
       " 'labels_url': 'https://api.github.com/repos/joelgrus/advent2022/labels{/name}',\n",
       " 'releases_url': 'https://api.github.com/repos/joelgrus/advent2022/releases{/id}',\n",
       " 'deployments_url': 'https://api.github.com/repos/joelgrus/advent2022/deployments',\n",
       " 'created_at': '2022-11-22T02:25:22Z',\n",
       " 'updated_at': '2022-12-02T13:55:26Z',\n",
       " 'pushed_at': '2022-12-03T13:38:45Z',\n",
       " 'git_url': 'git://github.com/joelgrus/advent2022.git',\n",
       " 'ssh_url': 'git@github.com:joelgrus/advent2022.git',\n",
       " 'clone_url': 'https://github.com/joelgrus/advent2022.git',\n",
       " 'svn_url': 'https://github.com/joelgrus/advent2022',\n",
       " 'homepage': None,\n",
       " 'size': 15,\n",
       " 'stargazers_count': 1,\n",
       " 'watchers_count': 1,\n",
       " 'language': 'Python',\n",
       " 'has_issues': True,\n",
       " 'has_projects': True,\n",
       " 'has_downloads': True,\n",
       " 'has_wiki': True,\n",
       " 'has_pages': False,\n",
       " 'has_discussions': False,\n",
       " 'forks_count': 0,\n",
       " 'mirror_url': None,\n",
       " 'archived': False,\n",
       " 'disabled': False,\n",
       " 'open_issues_count': 0,\n",
       " 'license': None,\n",
       " 'allow_forking': True,\n",
       " 'is_template': False,\n",
       " 'web_commit_signoff_required': False,\n",
       " 'topics': [],\n",
       " 'visibility': 'public',\n",
       " 'forks': 0,\n",
       " 'open_issues': 0,\n",
       " 'watchers': 1,\n",
       " 'default_branch': 'master'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_5_repositories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79a9c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2cc6c28d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TwythonError",
     "evalue": "Twitter API returned a 400 (Bad Request), b'{\"errors\":[{\"code\":215,\"message\":\"Bad Authentication data.\"}]}'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTwythonError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 89>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m     stream\u001b[38;5;241m.\u001b[39mstatuses\u001b[38;5;241m.\u001b[39mfilter(track\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# if instead we wanted to start consuming a sample of *all* public statuses\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# stream.statuses.sample()\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: main()\n",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Get a temporary client to retrieve an authentication url\u001b[39;00m\n\u001b[0;32m     34\u001b[0m temp_client \u001b[38;5;241m=\u001b[39m Twython(CONSUMER_KEY, CONSUMER_SECRET)\n\u001b[1;32m---> 35\u001b[0m temp_creds \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_authentication_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m url \u001b[38;5;241m=\u001b[39m temp_creds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauth_url\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Now visit that URL to authorize the application and get a PIN\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\twython\\api.py:339\u001b[0m, in \u001b[0;36mTwython.get_authentication_tokens\u001b[1;34m(self, callback_url, force_login, screen_name)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TwythonAuthError(response\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[0;32m    337\u001b[0m                            error_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TwythonError(response\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[0;32m    340\u001b[0m                        error_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m    342\u001b[0m request_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parse_qsl(response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m request_tokens:\n",
      "\u001b[1;31mTwythonError\u001b[0m: Twitter API returned a 400 (Bad Request), b'{\"errors\":[{\"code\":215,\"message\":\"Bad Authentication data.\"}]}'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # Feel free to plug your key and secret in directly\n",
    "    CONSUMER_KEY = os.environ.get(\"TWITTER_CONSUMER_KEY\")\n",
    "    CONSUMER_SECRET = os.environ.get(\"TWITTER_CONSUMER_SECRET\")\n",
    "    \n",
    "    import webbrowser\n",
    "    from twython import Twython\n",
    "    \n",
    "    # Get a temporary client to retrieve an authentication url\n",
    "    temp_client = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    temp_creds = temp_client.get_authentication_tokens()\n",
    "    url = temp_creds['auth_url']\n",
    "    \n",
    "    # Now visit that URL to authorize the application and get a PIN\n",
    "    print(f\"go visit {url} and get the PIN code and paste it below\")\n",
    "    webbrowser.open(url)\n",
    "    PIN_CODE = input(\"please enter the PIN code: \")\n",
    "    \n",
    "    # Now we use that PIN_CODE to get the actual tokens\n",
    "    auth_client = Twython(CONSUMER_KEY,\n",
    "                          CONSUMER_SECRET,\n",
    "                          temp_creds['oauth_token'],\n",
    "                          temp_creds['oauth_token_secret'])\n",
    "    final_step = auth_client.get_authorized_tokens(PIN_CODE)\n",
    "    ACCESS_TOKEN = final_step['oauth_token']\n",
    "    ACCESS_TOKEN_SECRET = final_step['oauth_token_secret']\n",
    "    \n",
    "    # And get a new Twython instance using them.\n",
    "    twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "    \n",
    "    from twython import TwythonStreamer\n",
    "    \n",
    "    # Appending data to a global variable is pretty poor form\n",
    "    # but it makes the example much simpler\n",
    "    tweets = []\n",
    "    \n",
    "    class MyStreamer(TwythonStreamer):\n",
    "        def on_success(self, data):\n",
    "            \"\"\"\n",
    "            What do we do when twitter sends us data?\n",
    "            Here data will be a Python dict representing a tweet\n",
    "            \"\"\"\n",
    "            # We only want to collect English-language tweets\n",
    "            if data.get('lang') == 'en':\n",
    "                tweets.append(data)\n",
    "                print(f\"received tweet #{len(tweets)}\")\n",
    "    \n",
    "            # Stop when we've collected enough\n",
    "            if len(tweets) >= 100:\n",
    "                self.disconnect()\n",
    "    \n",
    "        def on_error(self, status_code, data):\n",
    "            print(status_code, data)\n",
    "            self.disconnect()\n",
    "    \n",
    "    stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET,\n",
    "                        ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "    \n",
    "    # starts consuming public statuses that contain the keyword 'data'\n",
    "    stream.statuses.filter(track='data')\n",
    "    \n",
    "    # if instead we wanted to start consuming a sample of *all* public statuses\n",
    "    # stream.statuses.sample()\n",
    "    \n",
    "if __name__ == \"__main__\": main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
